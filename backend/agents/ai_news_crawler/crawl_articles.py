#!/usr/bin/env python3
"""
AIèµ„è®¯è¿½è¸ªå®˜ - æ–‡ç« çˆ¬è™«è„šæœ¬

çˆ¬å– bestblogs.dev çš„ AI å‰æ²¿èµ„è®¯æ–‡ç« 
æ”¯æŒå¢é‡æ›´æ–°ã€æ ¼å¼ä¿ç•™ã€æŠ¥å‘Šç”Ÿæˆ
"""

import argparse
import hashlib
import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup

# ä»£ç†é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
HTTP_PROXY = os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
HTTPS_PROXY = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy")

# é…ç½®
BASE_URL = "https://www.bestblogs.dev"
ARTICLES_URL = f"{BASE_URL}/articles"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
REQUEST_DELAY = 1.5  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

# é‡è¯•é…ç½®
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 5  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
RETRY_BACKOFF = 2  # é€€é¿å› å­ï¼ˆæ¯æ¬¡é‡è¯•é—´éš”ç¿»å€ï¼‰

# è·¯å¾„é…ç½®
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
ARTICLES_DIR = DATA_DIR / "articles"
REPORTS_DIR = SCRIPT_DIR / "reports"
INDEX_FILE = DATA_DIR / "index.json"


def get_url_hash(url: str) -> str:
    """ç”Ÿæˆ URL çš„çŸ­å“ˆå¸Œå€¼"""
    return hashlib.md5(url.encode()).hexdigest()[:12]


def slugify(text: str, max_length: int = 50) -> str:
    """å°†æ ‡é¢˜è½¬æ¢ä¸ºæ–‡ä»¶åå®‰å…¨çš„ slug"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—
    text = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', text)
    # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
    text = re.sub(r'\s+', '-', text)
    # æˆªæ–­
    return text[:max_length].strip('-')


def load_index() -> dict:
    """åŠ è½½æ–‡ç« ç´¢å¼•"""
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "last_updated": None,
        "source": "bestblogs.dev",
        "articles": {}
    }


def save_index(index: dict):
    """ä¿å­˜æ–‡ç« ç´¢å¼•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    index["last_updated"] = datetime.now().isoformat()
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def fetch_page(url: str, client: httpx.Client) -> Optional[str]:
    """è·å–é¡µé¢ HTMLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    last_error = None

    for attempt in range(MAX_RETRIES):
        try:
            response = client.get(url, follow_redirects=True)
            response.raise_for_status()
            return response.text
        except httpx.TimeoutException as e:
            last_error = e
            delay = RETRY_DELAY * (RETRY_BACKOFF ** attempt)
            print(f"â³ è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {url}", file=sys.stderr)
            print(f"   {delay}ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(delay)
        except httpx.ConnectError as e:
            last_error = e
            delay = RETRY_DELAY * (RETRY_BACKOFF ** attempt)
            print(f"ğŸ”Œ è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {url}", file=sys.stderr)
            print(f"   {delay}ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(delay)
        except httpx.HTTPStatusError as e:
            # 4xx é”™è¯¯ä¸é‡è¯•
            if 400 <= e.response.status_code < 500:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (HTTP {e.response.status_code}): {url}", file=sys.stderr)
                return None
            # 5xx é”™è¯¯é‡è¯•
            last_error = e
            delay = RETRY_DELAY * (RETRY_BACKOFF ** attempt)
            print(f"âš ï¸ æœåŠ¡å™¨é”™è¯¯ (å°è¯• {attempt + 1}/{MAX_RETRIES}): HTTP {e.response.status_code}", file=sys.stderr)
            print(f"   {delay}ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(delay)
        except httpx.HTTPError as e:
            last_error = e
            delay = RETRY_DELAY * (RETRY_BACKOFF ** attempt)
            print(f"âš ï¸ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}", file=sys.stderr)
            print(f"   {delay}ç§’åé‡è¯•...", file=sys.stderr)
            time.sleep(delay)

    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {url}", file=sys.stderr)
    print(f"   æœ€åé”™è¯¯: {last_error}", file=sys.stderr)
    return None


def parse_article_card(card) -> Optional[dict]:
    """è§£ææ–‡ç« å¡ç‰‡ï¼Œæå–å…ƒæ•°æ®"""
    try:
        # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
        title_elem = card.select_one('h2, h3, [class*="title"]')
        if not title_elem:
            return None
        
        title = title_elem.get_text(strip=True)
        
        # æŸ¥æ‰¾é“¾æ¥
        link_elem = card.select_one('a[href*="/article"]') or card.select_one('a')
        if not link_elem:
            return None
        
        url = link_elem.get('href', '')
        if not url.startswith('http'):
            url = urljoin(BASE_URL, url)
        
        # æå–å…¶ä»–å…ƒæ•°æ®
        article = {
            "title": title,
            "url": url,
            "source": "",
            "date": "",
            "word_count": 0,
            "read_time": "",
            "score": 0,
            "category": "",
            "summary": ""
        }
        
        # å°è¯•æå–æ¥æº
        source_elem = card.select_one('[class*="source"], [class*="author"]')
        if source_elem:
            article["source"] = source_elem.get_text(strip=True)
        
        # å°è¯•æå–æ—¥æœŸ
        date_elem = card.select_one('time, [class*="date"]')
        if date_elem:
            date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
            article["date"] = date_text
        
        # å°è¯•æå–è¯„åˆ†
        score_elem = card.select_one('[class*="score"], [class*="rating"]')
        if score_elem:
            score_text = score_elem.get_text(strip=True)
            score_match = re.search(r'\d+', score_text)
            if score_match:
                article["score"] = int(score_match.group())
        
        # å°è¯•æå–æ‘˜è¦
        summary_elem = card.select_one('[class*="summary"], [class*="desc"], p')
        if summary_elem:
            article["summary"] = summary_elem.get_text(strip=True)[:500]
        
        # å°è¯•æå–åˆ†ç±»
        category_elem = card.select_one('[class*="category"], [class*="tag"]')
        if category_elem:
            article["category"] = category_elem.get_text(strip=True)
        
        # å°è¯•æå–å­—æ•°å’Œé˜…è¯»æ—¶é—´
        text = card.get_text()
        word_match = re.search(r'(\d+)\s*å­—', text)
        if word_match:
            article["word_count"] = int(word_match.group(1))
        
        time_match = re.search(r'çº¦?\s*(\d+)\s*åˆ†é’Ÿ', text)
        if time_match:
            article["read_time"] = f"çº¦ {time_match.group(1)} åˆ†é’Ÿ"
        
        return article
        
    except Exception as e:
        print(f"âš ï¸ è§£ææ–‡ç« å¡ç‰‡å¤±è´¥: {e}", file=sys.stderr)
        return None


def fetch_article_list(client: httpx.Client, days: int = 7, category: Optional[str] = None) -> list[dict]:
    """è·å–æ–‡ç« åˆ—è¡¨"""
    print(f"ğŸ“¡ æ­£åœ¨è·å–æœ€è¿‘ {days} å¤©çš„æ–‡ç« åˆ—è¡¨...")
    
    articles = []
    seen_urls = set()  # ç”¨äºå»é‡
    page = 1
    max_pages = 10  # é™åˆ¶æœ€å¤§é¡µæ•°
    cutoff_date = datetime.now() - timedelta(days=days)
    
    while page <= max_pages:
        # æ„å»º URLï¼ˆæ ¹æ®ç½‘ç«™å®é™… API è°ƒæ•´ï¼‰
        url = f"{ARTICLES_URL}?page={page}"
        
        html = fetch_page(url, client)
        if not html:
            break
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« å¡ç‰‡ - ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨é¿å…é‡å¤
        # bestblogs.dev çš„æ–‡ç« å¡ç‰‡é€šå¸¸æœ‰ç‰¹å®šçš„ç»“æ„
        cards = soup.select('article[class*="card"], div[class*="article-card"], div[class*="post-card"]')
        
        if not cards:
            # å°è¯•å…¶ä»–é€‰æ‹©å™¨ï¼Œä½†æ’é™¤åµŒå¥—å…ƒç´ 
            cards = soup.select('a[href*="/article/"]')
            # è¿‡æ»¤æ‰åµŒå¥—çš„é“¾æ¥ï¼Œåªä¿ç•™æœ€å¤–å±‚
            cards = [c for c in cards if not c.find_parent('a')]
        
        if not cards:
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
            cards = soup.select('[class*="article"], [class*="post-item"]')
        
        if not cards:
            print(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« å¡ç‰‡", file=sys.stderr)
            break
        
        found_old = False
        page_articles = 0
        
        for card in cards:
            article = parse_article_card(card)
            if article:
                # å»é‡æ£€æŸ¥
                if article["url"] in seen_urls:
                    continue
                seen_urls.add(article["url"])
                
                # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨èŒƒå›´å†…
                if article["date"]:
                    try:
                        article_date = datetime.fromisoformat(article["date"].replace('Z', '+00:00'))
                        if article_date.replace(tzinfo=None) < cutoff_date:
                            found_old = True
                            continue
                    except ValueError:
                        pass  # æ—¥æœŸæ ¼å¼ä¸æ ‡å‡†ï¼Œä¿ç•™æ–‡ç« 
                
                # åˆ†ç±»ç­›é€‰
                if category and article["category"] and category not in article["category"]:
                    continue
                
                articles.append(article)
                page_articles += 1
        
        print(f"  ğŸ“„ ç¬¬ {page} é¡µ: è·å– {page_articles} ç¯‡æ–‡ç« ï¼ˆå»é‡åï¼‰")
        
        if found_old:
            print("  â° å·²åˆ°è¾¾æ—¶é—´èŒƒå›´è¾¹ç•Œ")
            break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        next_link = soup.select_one('a[class*="next"], [aria-label="ä¸‹ä¸€é¡µ"], .pagination a:last-child')
        if not next_link:
            break
        
        page += 1
        time.sleep(REQUEST_DELAY)
    
    print(f"âœ… å…±è·å– {len(articles)} ç¯‡æ–‡ç« ï¼ˆå·²å»é‡ï¼‰")
    return articles


def html_to_markdown(html_content: str, base_url: str = "") -> str:
    """å°† HTML è½¬æ¢ä¸º Markdownï¼Œä¿ç•™æ ¼å¼"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
    for tag in soup.select('script, style, nav, footer, header, aside'):
        tag.decompose()
    
    lines = []
    
    def process_element(elem, depth=0):
        if isinstance(elem, str):
            text = elem.strip()
            if text:
                lines.append(text)
            return
        
        tag_name = elem.name if hasattr(elem, 'name') else None
        
        if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            level = int(tag_name[1])
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"\n{'#' * level} {text}\n")
        
        elif tag_name == 'p':
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"\n{text}\n")
        
        elif tag_name in ['pre', 'code']:
            code = elem.get_text()
            lang = ''
            if elem.get('class'):
                for cls in elem.get('class', []):
                    if cls.startswith('language-'):
                        lang = cls.replace('language-', '')
                        break
            if tag_name == 'pre' or '\n' in code:
                lines.append(f"\n```{lang}\n{code}\n```\n")
            else:
                lines.append(f"`{code}`")
        
        elif tag_name == 'img':
            src = elem.get('src', '')
            alt = elem.get('alt', 'å›¾ç‰‡')
            if src:
                if not src.startswith('http'):
                    src = urljoin(base_url, src)
                lines.append(f"\n![{alt}]({src})\n")
        
        elif tag_name == 'a':
            href = elem.get('href', '')
            text = elem.get_text(strip=True)
            if href and text:
                if not href.startswith('http'):
                    href = urljoin(base_url, href)
                lines.append(f"[{text}]({href})")
        
        elif tag_name in ['ul', 'ol']:
            lines.append("")
            for i, li in enumerate(elem.find_all('li', recursive=False)):
                prefix = f"{i+1}. " if tag_name == 'ol' else "- "
                text = li.get_text(strip=True)
                if text:
                    lines.append(f"{prefix}{text}")
            lines.append("")
        
        elif tag_name == 'blockquote':
            text = elem.get_text(strip=True)
            if text:
                quoted = '\n'.join(f"> {line}" for line in text.split('\n'))
                lines.append(f"\n{quoted}\n")
        
        elif tag_name in ['strong', 'b']:
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"**{text}**")
        
        elif tag_name in ['em', 'i']:
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"*{text}*")
        
        elif tag_name in ['div', 'section', 'article', 'main']:
            for child in elem.children:
                process_element(child, depth + 1)
        
        elif hasattr(elem, 'children'):
            for child in elem.children:
                process_element(child, depth)
    
    # æŸ¥æ‰¾æ–‡ç« ä¸»ä½“
    main_content = soup.select_one('article, main, [class*="content"], [class*="post-body"]')
    if main_content:
        process_element(main_content)
    else:
        process_element(soup.body if soup.body else soup)
    
    # æ¸…ç†è¾“å‡º
    result = '\n'.join(lines)
    result = re.sub(r'\n{3,}', '\n\n', result)  # å‹ç¼©å¤šä½™ç©ºè¡Œ
    return result.strip()


def fetch_article_detail(url: str, client: httpx.Client) -> Optional[str]:
    """è·å–æ–‡ç« è¯¦æƒ…å¹¶è½¬æ¢ä¸º Markdown"""
    html = fetch_page(url, client)
    if not html:
        return None
    
    return html_to_markdown(html, url)


def save_article(article: dict, content: str) -> str:
    """ä¿å­˜æ–‡ç« ä¸º Markdown æ–‡ä»¶"""
    ARTICLES_DIR.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    date_str = article.get("date", datetime.now().strftime("%Y-%m-%d"))
    if isinstance(date_str, str) and len(date_str) >= 10:
        date_str = date_str[:10]
    else:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    slug = slugify(article["title"])
    url_hash = get_url_hash(article["url"])
    filename = f"{date_str}-{slug}-{url_hash}.md"
    filepath = ARTICLES_DIR / filename
    
    # æ„å»º frontmatter
    frontmatter = f"""---
title: "{article['title']}"
source: "{article.get('source', '')}"
url: "{article['url']}"
date: "{article.get('date', '')}"
category: "{article.get('category', '')}"
score: {article.get('score', 0)}
word_count: {article.get('word_count', 0)}
crawled_at: "{datetime.now().isoformat()}"
---

"""
    
    full_content = frontmatter + content
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    return f"articles/{filename}"


def generate_weekly_report(index: dict) -> str:
    """ç”Ÿæˆå‘¨æŠ¥"""
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    articles = list(index.get("articles", {}).values())
    
    # æŒ‰è¯„åˆ†æ’åº
    articles_by_score = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for article in articles:
        cat = article.get("category", "å…¶ä»–") or "å…¶ä»–"
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(article)
    
    # ç”ŸæˆæŠ¥å‘Š
    now = datetime.now()
    week_num = now.isocalendar()[1]
    
    report = f"""# AIèµ„è®¯å‘¨æŠ¥ - {now.year}å¹´ç¬¬{week_num}å‘¨

> æœ¬å‘¨æ”¶å½• **{len(articles)}** ç¯‡ AI å‰æ²¿æ–‡ç« 
> æ•°æ®æ¥æº: bestblogs.dev
> ç”Ÿæˆæ—¶é—´: {now.strftime("%Y-%m-%d %H:%M")}

## ğŸ”¥ çƒ­é—¨æ–‡ç«  TOP 5

"""
    
    for i, article in enumerate(articles_by_score[:5], 1):
        report += f"""{i}. **[{article['title']}]({article['url']})** - {article.get('source', 'æœªçŸ¥')} | â­ {article.get('score', 0)}
   > {article.get('summary', '')[:150]}...

"""
    
    report += "\n## ğŸ“‚ æŒ‰åˆ†ç±»æµè§ˆ\n\n"
    
    for cat, cat_articles in sorted(by_category.items(), key=lambda x: len(x[1]), reverse=True):
        report += f"### {cat} ({len(cat_articles)}ç¯‡)\n\n"
        for article in cat_articles[:10]:  # æ¯åˆ†ç±»æœ€å¤šæ˜¾ç¤º10ç¯‡
            report += f"- [{article['title']}]({article['url']}) - {article.get('source', '')}\n"
        if len(cat_articles) > 10:
            report += f"- *...è¿˜æœ‰ {len(cat_articles) - 10} ç¯‡*\n"
        report += "\n"
    
    report += """---
*ç”± AIèµ„è®¯è¿½è¸ªå®˜ è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    # ä¿å­˜æŠ¥å‘Š
    filename = f"weekly_{now.strftime('%Y-%m-%d')}.md"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… å‘¨æŠ¥å·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def generate_briefing_for_feed(articles: list[dict]) -> dict:
    """
    ç”Ÿæˆç®€æŠ¥æ ¼å¼ - ç”¨äºä¿¡æ¯æµå¡ç‰‡å±•ç¤º
    
    ç”Ÿæˆçš„æ•°æ®ç»“æ„å…¼å®¹ Flutter åº”ç”¨çš„ Briefing æ¨¡å‹ï¼Œ
    å¯ä»¥ç›´æ¥æ’å…¥ Supabase æ•°æ®åº“ä¾›å‰ç«¯å±•ç¤ºã€‚
    
    Returns:
        ç®€æŠ¥æ•°æ®ï¼Œå…¼å®¹ç°æœ‰å¡ç‰‡ç³»ç»Ÿ
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    if not articles:
        return {
            "error": "No articles found",
            "should_push": False
        }
    
    now = datetime.now()
    report_date = now.strftime("%Y-%m-%d")
    
    # æŒ‰è¯„åˆ†æ’åº
    articles_sorted = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for article in articles:
        cat = article.get("category", "AI") or "AI"
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(article)
    
    # åˆ¤æ–­æ˜¯å¦å€¼å¾—æ¨é€
    should_push = False
    priority = "P2"
    
    high_score_count = sum(1 for a in articles if a.get("score", 0) >= 90)
    
    if high_score_count >= 3:
        should_push = True
        priority = "P1"  # é‡è¦
    elif len(articles) >= 5 or high_score_count >= 1:
        should_push = True
        priority = "P2"  # æ™®é€š
    
    # ç”Ÿæˆæ ‡é¢˜
    if articles_sorted:
        top_article = articles_sorted[0]
        title_text = top_article["title"]
        
        if len(articles) > 1:
            title = f"ğŸ“š ä»Šæ—¥ç²¾é€‰ï¼š{title_text[:25]}ç­‰{len(articles)}ç¯‡"
        else:
            title = f"ğŸ“– æ¨èé˜…è¯»ï¼š{title_text[:30]}"
    else:
        title = "ä»Šæ—¥æš‚æ— æ–°æ–‡ç« æ¨è"
        should_push = False
    
    # ç”Ÿæˆæ‘˜è¦ï¼ˆMarkdown æ ¼å¼ï¼Œç”¨äºå¡ç‰‡å†…å®¹å±•ç¤ºï¼‰
    summary_lines = []
    for i, article in enumerate(articles_sorted[:5], 1):
        score = article.get("score", 0)
        source = article.get("source", "")
        summary_lines.append(f"**{i}. [{article['title']}]({article['url']})**")
        if source:
            summary_lines.append(f"   ğŸ“– {source} | â­ {score}")
        if article.get("summary"):
            summary_lines.append(f"   > {article['summary'][:100]}...")
        summary_lines.append("")
    
    summary = "\n".join(summary_lines)
    
    # å½±å“è¯´æ˜
    impact = None
    if high_score_count >= 2:
        impact = f"ä»Šæ—¥æœ‰ {high_score_count} ç¯‡é«˜è¯„åˆ†æ–‡ç« ï¼ˆâ‰¥90åˆ†ï¼‰ï¼Œå»ºè®®ä¼˜å…ˆé˜…è¯»"
    
    # å…³é”®æ–‡ç« åˆ—è¡¨ï¼ˆç”¨äº context_dataï¼‰
    key_articles = []
    for article in articles_sorted[:10]:
        key_articles.append({
            "title": article["title"],
            "url": article["url"],
            "source": article.get("source", ""),
            "score": article.get("score", 0),
            "category": article.get("category", ""),
            "summary": article.get("summary", "")[:200],
            "date": article.get("date", "")
        })
    
    # æ„å»ºå…¼å®¹ Briefing æ¨¡å‹çš„æ•°æ®
    briefing = {
        # ç®€æŠ¥å…ƒæ•°æ®
        "briefing_type": "summary",  # æ‘˜è¦ç±»å‹
        "priority": priority,
        "title": title,
        "summary": summary,
        "impact": impact,
        
        # æ“ä½œæŒ‰é’®
        "actions": [
            {"label": "æŸ¥çœ‹å…¨éƒ¨", "action": "view_report"},
            {"label": "è¯¦ç»†åˆ†æ", "action": "start_conversation", "prompt": "è¯·å¸®æˆ‘åˆ†æä»Šå¤©çš„AIèµ„è®¯æœ‰ä»€ä¹ˆå€¼å¾—å…³æ³¨çš„è¶‹åŠ¿"}
        ],
        
        # ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå®Œæ•´æ–‡ç« åˆ—è¡¨ï¼‰
        "context_data": {
            "source": "bestblogs.dev",
            "date": report_date,
            "total_articles": len(articles),
            "high_score_count": high_score_count,
            "categories": list(by_category.keys()),
            "articles": key_articles,
            "generated_at": now.isoformat()
        },
        
        # æ¨é€åˆ¤æ–­
        "should_push": should_push,
        "importance_score": min(0.5 + high_score_count * 0.1, 0.95)
    }
    
    # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
    filename = f"briefing_articles_{report_date}.json"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(briefing, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç®€æŠ¥å·²ç”Ÿæˆ: {filepath}")
    
    return briefing


def push_briefing_to_feed(briefing: dict, agent_name: str = "AIèµ„è®¯è¿½è¸ªå®˜") -> bool:
    """
    å°†ç®€æŠ¥æ¨é€åˆ°ä¿¡æ¯æµï¼ˆæ’å…¥ Supabase æ•°æ®åº“ï¼‰
    
    éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡:
    - SUPABASE_URL
    - SUPABASE_SERVICE_KEY
    
    Args:
        briefing: ç®€æŠ¥æ•°æ®
        agent_name: Agent åç§°ï¼Œç”¨äºæŸ¥æ‰¾ agent_id
        
    Returns:
        æ˜¯å¦æ¨é€æˆåŠŸ
    """
    import uuid
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    supabase_url = os.environ.get("SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_SERVICE_KEY")
    
    if not supabase_url or not supabase_key:
        print("âš ï¸ æœªé…ç½® Supabase ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡æ¨é€åˆ°ä¿¡æ¯æµ")
        print("   è®¾ç½® SUPABASE_URL å’Œ SUPABASE_SERVICE_KEY ä»¥å¯ç”¨æ¨é€")
        return False
    
    try:
        from supabase import create_client
        
        supabase = create_client(supabase_url, supabase_key)
        
        # 1. æŸ¥æ‰¾ Agent ID
        agents = supabase.table('agents').select('id, name').eq('name', agent_name).limit(1).execute()
        if not agents.data:
            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            agents = supabase.table('agents').select('id, name').ilike('name', f'%èµ„è®¯%').limit(1).execute()
        
        if not agents.data:
            print(f"âš ï¸ æœªæ‰¾åˆ° Agent: {agent_name}")
            return False
        
        agent_id = agents.data[0]['id']
        print(f"âœ… æ‰¾åˆ° Agent: {agents.data[0]['name']}")
        
        # 2. è·å–æ‰€æœ‰æ´»è·ƒç”¨æˆ·
        users = supabase.table('users').select('id').eq('is_active', True).execute()
        if not users.data:
            print("âš ï¸ æ²¡æœ‰æ´»è·ƒç”¨æˆ·ï¼Œè·³è¿‡æ¨é€")
            return False
        
        print(f"ğŸ“¤ å‡†å¤‡æ¨é€ç»™ {len(users.data)} ä¸ªç”¨æˆ·...")
        
        # 3. ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç®€æŠ¥
        briefings_to_insert = []
        for user in users.data:
            briefings_to_insert.append({
                'id': str(uuid.uuid4()),
                'agent_id': agent_id,
                'user_id': user['id'],
                'briefing_type': briefing.get('briefing_type', 'summary'),
                'priority': briefing.get('priority', 'P2'),
                'title': briefing['title'],
                'summary': briefing['summary'],
                'impact': briefing.get('impact'),
                'actions': briefing.get('actions', []),
                'context_data': briefing.get('context_data', {}),
                'importance_score': briefing.get('importance_score', 0.5),
                'status': 'new'
            })
        
        # 4. æ‰¹é‡æ’å…¥
        result = supabase.table('briefings').insert(briefings_to_insert).execute()
        print(f"âœ… æˆåŠŸæ¨é€ {len(result.data)} æ¡ç®€æŠ¥åˆ°ä¿¡æ¯æµï¼")
        return True
        
    except ImportError:
        print("âš ï¸ æœªå®‰è£… supabase åº“ï¼Œè¿è¡Œ: pip install supabase")
        return False
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")
        return False


def generate_html_cards_report(articles: list[dict], index: dict) -> str:
    """
    ç”Ÿæˆ HTML å¡ç‰‡å¼æŠ¥å‘Š
    - å¤–å±‚ä»¥å¡ç‰‡å½¢å¼å‘ˆç°æ–‡ç« åˆ—è¡¨
    - ç‚¹å‡»å¡ç‰‡å…¨å±æ˜¾ç¤ºå®Œæ•´å†…å®¹
    - ç°ä»£åŒ–çš„æ ·å¼è®¾è®¡
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    now = datetime.now()
    
    # æŒ‰è¯„åˆ†æ’åº
    articles_sorted = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)
    
    # åˆ†ç±»é¢œè‰²æ˜ å°„
    category_colors = {
        "AI": "#6366f1",
        "LLM": "#8b5cf6",
        "GPT": "#a855f7",
        "æœºå™¨å­¦ä¹ ": "#ec4899",
        "æ·±åº¦å­¦ä¹ ": "#f43f5e",
        "äº§ä¸š": "#f97316",
        "æŠ€æœ¯": "#0ea5e9",
        "å¼€æº": "#14b8a6",
        "é»˜è®¤": "#64748b"
    }
    
    def get_category_color(category: str) -> str:
        for key, color in category_colors.items():
            if key in (category or ""):
                return color
        return category_colors["é»˜è®¤"]
    
    # ç”Ÿæˆæ–‡ç« å¡ç‰‡ HTML
    cards_html = ""
    for i, article in enumerate(articles_sorted):
        url_hash = get_url_hash(article["url"])
        article_data = index.get("articles", {}).get(url_hash, {})
        file_path = article_data.get("file_path", "")
        
        # è¯»å–æ–‡ç« å†…å®¹ï¼ˆå¦‚æœå·²çˆ¬å–ï¼‰
        content_html = ""
        if file_path:
            full_path = SCRIPT_DIR / file_path.replace("articles/", "data/articles/")
            if full_path.exists():
                with open(full_path, 'r', encoding='utf-8') as f:
                    md_content = f.read()
                    # ç§»é™¤ frontmatter
                    if md_content.startswith("---"):
                        parts = md_content.split("---", 2)
                        if len(parts) >= 3:
                            md_content = parts[2].strip()
                    content_html = md_content
        
        if not content_html:
            content_html = article.get("summary", "æš‚æ— å†…å®¹é¢„è§ˆ")
        
        category = article.get("category", "AIèµ„è®¯") or "AIèµ„è®¯"
        category_color = get_category_color(category)
        score = article.get("score", 0)
        source = article.get("source", "æœªçŸ¥æ¥æº") or "æœªçŸ¥æ¥æº"
        
        # å¤„ç†å†…å®¹ï¼Œè½¬ä¹‰ HTML å’Œç‰¹æ®Šå­—ç¬¦ç”¨äº JavaScript
        content_escaped = content_html.replace("\\", "\\\\").replace("`", "\\`").replace("${", "\\${")
        
        cards_html += f'''
        <article class="card" onclick="openModal({i})" data-index="{i}">
            <div class="card-header">
                <span class="category" style="background: {category_color}20; color: {category_color}">{category}</span>
                <span class="score">â­ {score}</span>
            </div>
            <h2 class="card-title">{article["title"]}</h2>
            <p class="card-summary">{article.get("summary", "")[:200]}...</p>
            <div class="card-footer">
                <span class="source">{source}</span>
                <span class="read-more">ç‚¹å‡»é˜…è¯» â†’</span>
            </div>
        </article>
        <script>
            articleContents[{i}] = {{
                title: `{article["title"].replace("`", "'")}`,
                category: `{category}`,
                categoryColor: `{category_color}`,
                source: `{source}`,
                url: `{article["url"]}`,
                score: {score},
                content: `{content_escaped}`
            }};
        </script>
        '''
    
    html_template = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIèµ„è®¯é€Ÿé€’ - {now.strftime("%Yå¹´%mæœˆ%dæ—¥")}</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {{
            --bg-primary: #0f0f23;
            --bg-secondary: #1a1a2e;
            --bg-card: #16213e;
            --bg-card-hover: #1a2a4a;
            --text-primary: #e2e8f0;
            --text-secondary: #94a3b8;
            --text-muted: #64748b;
            --accent-primary: #6366f1;
            --accent-secondary: #8b5cf6;
            --border-color: #334155;
            --shadow-color: rgba(0, 0, 0, 0.3);
        }}
        
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'Noto Sans SC', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            min-height: 100vh;
        }}
        
        /* èƒŒæ™¯æ¸å˜æ•ˆæœ */
        body::before {{
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: 
                radial-gradient(ellipse at 20% 20%, rgba(99, 102, 241, 0.15) 0%, transparent 50%),
                radial-gradient(ellipse at 80% 80%, rgba(139, 92, 246, 0.1) 0%, transparent 50%),
                radial-gradient(ellipse at 40% 60%, rgba(6, 182, 212, 0.08) 0%, transparent 40%);
            pointer-events: none;
            z-index: -1;
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
        }}
        
        /* å¤´éƒ¨æ ·å¼ */
        .header {{
            text-align: center;
            margin-bottom: 3rem;
            padding: 2rem 0;
        }}
        
        .header h1 {{
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, #6366f1, #8b5cf6, #06b6d4);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 0.5rem;
        }}
        
        .header .subtitle {{
            color: var(--text-secondary);
            font-size: 1.1rem;
        }}
        
        .header .meta {{
            margin-top: 1rem;
            display: flex;
            justify-content: center;
            gap: 2rem;
            color: var(--text-muted);
            font-size: 0.9rem;
        }}
        
        .header .meta span {{
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }}
        
        /* å¡ç‰‡ç½‘æ ¼ */
        .cards-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(380px, 1fr));
            gap: 1.5rem;
        }}
        
        /* å¡ç‰‡æ ·å¼ */
        .card {{
            background: var(--bg-card);
            border-radius: 16px;
            padding: 1.5rem;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            border: 1px solid var(--border-color);
            position: relative;
            overflow: hidden;
        }}
        
        .card::before {{
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
            opacity: 0;
            transition: opacity 0.3s;
        }}
        
        .card:hover {{
            transform: translateY(-4px);
            background: var(--bg-card-hover);
            box-shadow: 0 20px 40px var(--shadow-color);
            border-color: var(--accent-primary);
        }}
        
        .card:hover::before {{
            opacity: 1;
        }}
        
        .card-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
        }}
        
        .category {{
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }}
        
        .score {{
            color: var(--text-muted);
            font-size: 0.85rem;
        }}
        
        .card-title {{
            font-size: 1.2rem;
            font-weight: 600;
            line-height: 1.4;
            margin-bottom: 0.75rem;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }}
        
        .card-summary {{
            color: var(--text-secondary);
            font-size: 0.95rem;
            line-height: 1.6;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
            margin-bottom: 1rem;
        }}
        
        .card-footer {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding-top: 1rem;
            border-top: 1px solid var(--border-color);
        }}
        
        .source {{
            color: var(--text-muted);
            font-size: 0.85rem;
        }}
        
        .read-more {{
            color: var(--accent-primary);
            font-size: 0.9rem;
            font-weight: 500;
            opacity: 0;
            transform: translateX(-10px);
            transition: all 0.3s;
        }}
        
        .card:hover .read-more {{
            opacity: 1;
            transform: translateX(0);
        }}
        
        /* æ¨¡æ€æ¡†æ ·å¼ */
        .modal {{
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: rgba(0, 0, 0, 0.85);
            z-index: 1000;
            opacity: 0;
            transition: opacity 0.3s;
            backdrop-filter: blur(8px);
        }}
        
        .modal.active {{
            display: flex;
            opacity: 1;
        }}
        
        .modal-content {{
            background: var(--bg-secondary);
            width: 100%;
            max-width: 900px;
            max-height: 90vh;
            margin: auto;
            border-radius: 20px;
            overflow: hidden;
            transform: scale(0.9);
            transition: transform 0.3s;
            display: flex;
            flex-direction: column;
        }}
        
        .modal.active .modal-content {{
            transform: scale(1);
        }}
        
        .modal-header {{
            padding: 1.5rem 2rem;
            background: var(--bg-card);
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
        }}
        
        .modal-header-info {{
            flex: 1;
        }}
        
        .modal-header .category {{
            margin-bottom: 0.75rem;
            display: inline-block;
        }}
        
        .modal-title {{
            font-size: 1.5rem;
            font-weight: 700;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }}
        
        .modal-meta {{
            display: flex;
            gap: 1.5rem;
            color: var(--text-muted);
            font-size: 0.9rem;
        }}
        
        .close-btn {{
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            font-size: 1.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
            flex-shrink: 0;
        }}
        
        .close-btn:hover {{
            background: var(--accent-primary);
            color: white;
            border-color: var(--accent-primary);
        }}
        
        .modal-body {{
            padding: 2rem;
            overflow-y: auto;
            flex: 1;
        }}
        
        .modal-body .content {{
            color: var(--text-primary);
            font-size: 1.05rem;
            line-height: 1.8;
        }}
        
        .modal-body .content h1,
        .modal-body .content h2,
        .modal-body .content h3 {{
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            font-weight: 600;
            color: var(--text-primary);
        }}
        
        .modal-body .content h1 {{ font-size: 1.5rem; }}
        .modal-body .content h2 {{ font-size: 1.3rem; }}
        .modal-body .content h3 {{ font-size: 1.1rem; }}
        
        .modal-body .content p {{
            margin-bottom: 1rem;
        }}
        
        .modal-body .content pre {{
            background: var(--bg-primary);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            border: 1px solid var(--border-color);
        }}
        
        .modal-body .content code {{
            font-family: 'JetBrains Mono', monospace;
            background: var(--bg-primary);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-size: 0.9em;
        }}
        
        .modal-body .content ul,
        .modal-body .content ol {{
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }}
        
        .modal-body .content li {{
            margin-bottom: 0.5rem;
        }}
        
        .modal-body .content blockquote {{
            border-left: 4px solid var(--accent-primary);
            padding-left: 1rem;
            margin: 1rem 0;
            color: var(--text-secondary);
            font-style: italic;
        }}
        
        .modal-body .content img {{
            max-width: 100%;
            border-radius: 8px;
            margin: 1rem 0;
        }}
        
        .modal-body .content a {{
            color: var(--accent-primary);
            text-decoration: none;
        }}
        
        .modal-body .content a:hover {{
            text-decoration: underline;
        }}
        
        .modal-footer {{
            padding: 1rem 2rem;
            background: var(--bg-card);
            border-top: 1px solid var(--border-color);
            display: flex;
            justify-content: flex-end;
            gap: 1rem;
        }}
        
        .btn {{
            padding: 0.75rem 1.5rem;
            border-radius: 8px;
            font-size: 0.95rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }}
        
        .btn-primary {{
            background: linear-gradient(135deg, var(--accent-primary), var(--accent-secondary));
            color: white;
            border: none;
        }}
        
        .btn-primary:hover {{
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(99, 102, 241, 0.4);
        }}
        
        .btn-secondary {{
            background: transparent;
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
        }}
        
        .btn-secondary:hover {{
            background: var(--bg-card);
            color: var(--text-primary);
        }}
        
        /* å“åº”å¼ */
        @media (max-width: 768px) {{
            .container {{
                padding: 1rem;
            }}
            
            .header h1 {{
                font-size: 1.8rem;
            }}
            
            .cards-grid {{
                grid-template-columns: 1fr;
            }}
            
            .modal-content {{
                margin: 0;
                border-radius: 0;
                max-height: 100vh;
            }}
            
            .modal-title {{
                font-size: 1.2rem;
            }}
        }}
        
        /* åŠ¨ç”» */
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(20px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        
        .card {{
            animation: fadeIn 0.5s ease-out forwards;
        }}
        
        .cards-grid .card:nth-child(1) {{ animation-delay: 0.05s; }}
        .cards-grid .card:nth-child(2) {{ animation-delay: 0.1s; }}
        .cards-grid .card:nth-child(3) {{ animation-delay: 0.15s; }}
        .cards-grid .card:nth-child(4) {{ animation-delay: 0.2s; }}
        .cards-grid .card:nth-child(5) {{ animation-delay: 0.25s; }}
        .cards-grid .card:nth-child(6) {{ animation-delay: 0.3s; }}
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>ğŸ¤– AIèµ„è®¯é€Ÿé€’</h1>
            <p class="subtitle">ç²¾é€‰å‰æ²¿ AI æŠ€æœ¯æ–‡ç« ï¼Œæ¯æ—¥æ›´æ–°</p>
            <div class="meta">
                <span>ğŸ“… {now.strftime("%Yå¹´%mæœˆ%dæ—¥")}</span>
                <span>ğŸ“° å…± {len(articles)} ç¯‡æ–‡ç« </span>
                <span>ğŸ”— æ¥æº: bestblogs.dev</span>
            </div>
        </header>
        
        <main class="cards-grid">
            <script>const articleContents = {{}};</script>
            {cards_html}
        </main>
    </div>
    
    <!-- æ¨¡æ€æ¡† -->
    <div class="modal" id="articleModal">
        <div class="modal-content">
            <div class="modal-header">
                <div class="modal-header-info">
                    <span class="category" id="modalCategory"></span>
                    <h2 class="modal-title" id="modalTitle"></h2>
                    <div class="modal-meta">
                        <span id="modalSource"></span>
                        <span id="modalScore"></span>
                    </div>
                </div>
                <button class="close-btn" onclick="closeModal()">&times;</button>
            </div>
            <div class="modal-body">
                <div class="content" id="modalContent"></div>
            </div>
            <div class="modal-footer">
                <button class="btn btn-secondary" onclick="closeModal()">å…³é—­</button>
                <a class="btn btn-primary" id="modalLink" href="#" target="_blank">
                    ğŸ”— é˜…è¯»åŸæ–‡
                </a>
            </div>
        </div>
    </div>
    
    <script>
        const modal = document.getElementById('articleModal');
        
        function openModal(index) {{
            const article = articleContents[index];
            if (!article) return;
            
            document.getElementById('modalCategory').textContent = article.category;
            document.getElementById('modalCategory').style.background = article.categoryColor + '20';
            document.getElementById('modalCategory').style.color = article.categoryColor;
            document.getElementById('modalTitle').textContent = article.title;
            document.getElementById('modalSource').textContent = 'ğŸ“– ' + article.source;
            document.getElementById('modalScore').textContent = 'â­ ' + article.score;
            document.getElementById('modalContent').innerHTML = renderMarkdown(article.content);
            document.getElementById('modalLink').href = article.url;
            
            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        }}
        
        function closeModal() {{
            modal.classList.remove('active');
            document.body.style.overflow = '';
        }}
        
        // ç‚¹å‡»æ¨¡æ€æ¡†å¤–éƒ¨å…³é—­
        modal.addEventListener('click', (e) => {{
            if (e.target === modal) {{
                closeModal();
            }}
        }});
        
        // ESC é”®å…³é—­
        document.addEventListener('keydown', (e) => {{
            if (e.key === 'Escape') {{
                closeModal();
            }}
        }});
        
        // ç®€å•çš„ Markdown æ¸²æŸ“
        function renderMarkdown(text) {{
            if (!text) return '';
            
            return text
                // ä»£ç å—
                .replace(/```(\\w*)\\n([\\s\\S]*?)```/g, '<pre><code class="language-$1">$2</code></pre>')
                // è¡Œå†…ä»£ç 
                .replace(/`([^`]+)`/g, '<code>$1</code>')
                // æ ‡é¢˜
                .replace(/^### (.*$)/gim, '<h3>$1</h3>')
                .replace(/^## (.*$)/gim, '<h2>$1</h2>')
                .replace(/^# (.*$)/gim, '<h1>$1</h1>')
                // ç²—ä½“
                .replace(/\\*\\*([^*]+)\\*\\*/g, '<strong>$1</strong>')
                // æ–œä½“
                .replace(/\\*([^*]+)\\*/g, '<em>$1</em>')
                // é“¾æ¥
                .replace(/\\[([^\\]]+)\\]\\(([^)]+)\\)/g, '<a href="$2" target="_blank">$1</a>')
                // å›¾ç‰‡
                .replace(/!\\[([^\\]]*?)\\]\\(([^)]+)\\)/g, '<img src="$2" alt="$1" />')
                // å¼•ç”¨
                .replace(/^> (.*$)/gim, '<blockquote>$1</blockquote>')
                // æ— åºåˆ—è¡¨
                .replace(/^- (.*$)/gim, '<li>$1</li>')
                // æ®µè½
                .replace(/\\n\\n/g, '</p><p>')
                // æ¢è¡Œ
                .replace(/\\n/g, '<br>');
        }}
    </script>
</body>
</html>
'''
    
    # ä¿å­˜ HTML æŠ¥å‘Š
    filename = f"articles_{now.strftime('%Y-%m-%d')}.html"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(html_template)
    
    print(f"âœ… HTMLå¡ç‰‡æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def main():
    parser = argparse.ArgumentParser(description='AIèµ„è®¯è¿½è¸ªå®˜ - æ–‡ç« çˆ¬è™«')
    parser.add_argument('--days', type=int, default=7, help='è·å–æœ€è¿‘Nå¤©çš„æ–‡ç« ï¼ˆé»˜è®¤7å¤©ï¼‰')
    parser.add_argument('--category', type=str, help='æŒ‰åˆ†ç±»ç­›é€‰')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶å…¨é‡æ›´æ–°ï¼Œå¿½ç•¥ç¼“å­˜')
    parser.add_argument('--report', choices=['weekly', 'daily', 'html', 'cards', 'briefing', 'all'], 
                        help='ç”ŸæˆæŠ¥å‘Šç±»å‹')
    parser.add_argument('--push', action='store_true', help='æ¨é€ç®€æŠ¥åˆ°ä¿¡æ¯æµï¼ˆéœ€é…ç½® Supabaseï¼‰')
    parser.add_argument('--list-only', action='store_true', help='åªè·å–åˆ—è¡¨ï¼Œä¸æŠ“å–è¯¦æƒ…')
    
    args = parser.parse_args()
    
    # åŠ è½½ç´¢å¼•
    index = load_index()
    
    # å¦‚æœåªæ˜¯ç”ŸæˆæŠ¥å‘Šï¼ˆä»ç´¢å¼•ç”Ÿæˆï¼‰
    if args.report == 'weekly':
        generate_weekly_report(index)
        return
    
    # ä»ç¼“å­˜ç”ŸæˆæŠ¥å‘Š
    if args.report in ['html', 'cards', 'briefing', 'all']:
        cached_articles = list(index.get("articles", {}).values())
        if cached_articles:
            print(f"ğŸ“Š ä½¿ç”¨ç¼“å­˜æ•°æ®ç”ŸæˆæŠ¥å‘Šï¼ˆ{len(cached_articles)} ç¯‡æ–‡ç« ï¼‰")
            
            if args.report in ['html', 'cards']:
                generate_html_cards_report(cached_articles, index)
            elif args.report == 'briefing':
                briefing = generate_briefing_for_feed(cached_articles)
                if args.push and briefing.get('should_push'):
                    push_briefing_to_feed(briefing)
                elif args.push:
                    print("â„¹ï¸ ç®€æŠ¥ä»·å€¼ä¸è¶³ï¼Œè·³è¿‡æ¨é€")
            elif args.report == 'all':
                generate_briefing_for_feed(cached_articles)
                generate_html_cards_report(cached_articles, index)
            
            return
        else:
            print("âš ï¸ ç¼“å­˜ä¸­æ²¡æœ‰æ–‡ç« ï¼Œå°è¯•åœ¨çº¿è·å–...")
            # ç»§ç»­æ‰§è¡Œåœ¨çº¿è·å–é€»è¾‘
    
    # é…ç½®ä»£ç†
    proxy_config = None
    if HTTPS_PROXY or HTTP_PROXY:
        proxy_url = HTTPS_PROXY or HTTP_PROXY
        proxy_config = proxy_url
        print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_url}")
    
    # åˆ›å»º HTTP å®¢æˆ·ç«¯
    with httpx.Client(
        headers={"User-Agent": USER_AGENT},
        timeout=30.0,
        follow_redirects=True,
        proxy=proxy_config
    ) as client:
        
        # è·å–æ–‡ç« åˆ—è¡¨
        articles = fetch_article_list(client, args.days, args.category)
        
        if not articles:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ", file=sys.stderr)
            sys.exit(1)
        
        # ç­›é€‰æ–°æ–‡ç« 
        new_articles = []
        for article in articles:
            url_hash = get_url_hash(article["url"])
            if args.force or url_hash not in index["articles"]:
                new_articles.append(article)
        
        print(f"ğŸ“Š æ–°æ–‡ç« : {len(new_articles)} / æ€»è®¡: {len(articles)}")
        
        if args.list_only:
            # åªè¾“å‡ºåˆ—è¡¨
            print("\nğŸ“° æ–‡ç« åˆ—è¡¨:")
            for article in articles:
                print(f"  - [{article['title']}] {article['url']}")
            return
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.report in ['html', 'cards']:
            generate_html_cards_report(articles, index)
            return
        
        if args.report in ['briefing', 'all']:
            briefing = generate_briefing_for_feed(articles)
            
            # å¦‚æœæŒ‡å®šäº† --push ä¸”ç®€æŠ¥å€¼å¾—æ¨é€
            if args.push and briefing.get('should_push'):
                push_briefing_to_feed(briefing)
            elif args.push and not briefing.get('should_push'):
                print("â„¹ï¸ ç®€æŠ¥ä»·å€¼ä¸è¶³ï¼Œè·³è¿‡æ¨é€ï¼ˆå¯æ‰‹åŠ¨æ¨é€ï¼‰")
            
            if args.report == 'all':
                # åŒæ—¶ç”Ÿæˆ HTML æŠ¥å‘Š
                generate_html_cards_report(articles, index)
            
            return
        
        # æŠ“å–æ–°æ–‡ç« è¯¦æƒ…
        for i, article in enumerate(new_articles, 1):
            print(f"ğŸ“¥ [{i}/{len(new_articles)}] æ­£åœ¨æŠ“å–: {article['title'][:40]}...")
            
            content = fetch_article_detail(article["url"], client)
            if content:
                file_path = save_article(article, content)
                
                # æ›´æ–°ç´¢å¼•
                url_hash = get_url_hash(article["url"])
                index["articles"][url_hash] = {
                    **article,
                    "crawled_at": datetime.now().isoformat(),
                    "file_path": file_path
                }
            
            time.sleep(REQUEST_DELAY)
        
        # ä¿å­˜ç´¢å¼•
        save_index(index)
        
        print(f"\nâœ… å®Œæˆ! å·²ä¿å­˜ {len(new_articles)} ç¯‡æ–°æ–‡ç« ")
        print(f"ğŸ“ æ–‡ç« ç›®å½•: {ARTICLES_DIR}")
        print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {INDEX_FILE}")


if __name__ == "__main__":
    main()


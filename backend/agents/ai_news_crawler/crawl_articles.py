#!/usr/bin/env python3
"""
AIèµ„è®¯è¿½è¸ªå®˜ - æ–‡ç« çˆ¬è™«è„šæœ¬

çˆ¬å– bestblogs.dev çš„ AI å‰æ²¿èµ„è®¯æ–‡ç« 
æ”¯æŒå¢é‡æ›´æ–°ã€æ ¼å¼ä¿ç•™ã€æŠ¥å‘Šç”Ÿæˆ
"""

import argparse
import hashlib
import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup

# ä»£ç†é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
HTTP_PROXY = os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
HTTPS_PROXY = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy")

# é…ç½®
BASE_URL = "https://www.bestblogs.dev"
ARTICLES_URL = f"{BASE_URL}/articles"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
REQUEST_DELAY = 1.5  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

# è·¯å¾„é…ç½®
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
ARTICLES_DIR = DATA_DIR / "articles"
REPORTS_DIR = SCRIPT_DIR / "reports"
INDEX_FILE = DATA_DIR / "index.json"


def get_url_hash(url: str) -> str:
    """ç”Ÿæˆ URL çš„çŸ­å“ˆå¸Œå€¼"""
    return hashlib.md5(url.encode()).hexdigest()[:12]


def slugify(text: str, max_length: int = 50) -> str:
    """å°†æ ‡é¢˜è½¬æ¢ä¸ºæ–‡ä»¶åå®‰å…¨çš„ slug"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—
    text = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', text)
    # æ›¿æ¢ç©ºæ ¼ä¸ºè¿å­—ç¬¦
    text = re.sub(r'\s+', '-', text)
    # æˆªæ–­
    return text[:max_length].strip('-')


def load_index() -> dict:
    """åŠ è½½æ–‡ç« ç´¢å¼•"""
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "last_updated": None,
        "source": "bestblogs.dev",
        "articles": {}
    }


def save_index(index: dict):
    """ä¿å­˜æ–‡ç« ç´¢å¼•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    index["last_updated"] = datetime.now().isoformat()
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def fetch_page(url: str, client: httpx.Client) -> Optional[str]:
    """è·å–é¡µé¢ HTML"""
    try:
        response = client.get(url, follow_redirects=True)
        response.raise_for_status()
        return response.text
    except httpx.HTTPError as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {url} - {e}", file=sys.stderr)
        return None


def parse_article_card(card) -> Optional[dict]:
    """è§£ææ–‡ç« å¡ç‰‡ï¼Œæå–å…ƒæ•°æ®"""
    try:
        # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
        title_elem = card.select_one('h2, h3, [class*="title"]')
        if not title_elem:
            return None
        
        title = title_elem.get_text(strip=True)
        
        # æŸ¥æ‰¾é“¾æ¥
        link_elem = card.select_one('a[href*="/article"]') or card.select_one('a')
        if not link_elem:
            return None
        
        url = link_elem.get('href', '')
        if not url.startswith('http'):
            url = urljoin(BASE_URL, url)
        
        # æå–å…¶ä»–å…ƒæ•°æ®
        article = {
            "title": title,
            "url": url,
            "source": "",
            "date": "",
            "word_count": 0,
            "read_time": "",
            "score": 0,
            "category": "",
            "summary": ""
        }
        
        # å°è¯•æå–æ¥æº
        source_elem = card.select_one('[class*="source"], [class*="author"]')
        if source_elem:
            article["source"] = source_elem.get_text(strip=True)
        
        # å°è¯•æå–æ—¥æœŸ
        date_elem = card.select_one('time, [class*="date"]')
        if date_elem:
            date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
            article["date"] = date_text
        
        # å°è¯•æå–è¯„åˆ†
        score_elem = card.select_one('[class*="score"], [class*="rating"]')
        if score_elem:
            score_text = score_elem.get_text(strip=True)
            score_match = re.search(r'\d+', score_text)
            if score_match:
                article["score"] = int(score_match.group())
        
        # å°è¯•æå–æ‘˜è¦
        summary_elem = card.select_one('[class*="summary"], [class*="desc"], p')
        if summary_elem:
            article["summary"] = summary_elem.get_text(strip=True)[:500]
        
        # å°è¯•æå–åˆ†ç±»
        category_elem = card.select_one('[class*="category"], [class*="tag"]')
        if category_elem:
            article["category"] = category_elem.get_text(strip=True)
        
        # å°è¯•æå–å­—æ•°å’Œé˜…è¯»æ—¶é—´
        text = card.get_text()
        word_match = re.search(r'(\d+)\s*å­—', text)
        if word_match:
            article["word_count"] = int(word_match.group(1))
        
        time_match = re.search(r'çº¦?\s*(\d+)\s*åˆ†é’Ÿ', text)
        if time_match:
            article["read_time"] = f"çº¦ {time_match.group(1)} åˆ†é’Ÿ"
        
        return article
        
    except Exception as e:
        print(f"âš ï¸ è§£ææ–‡ç« å¡ç‰‡å¤±è´¥: {e}", file=sys.stderr)
        return None


def fetch_article_list(client: httpx.Client, days: int = 7, category: Optional[str] = None) -> list[dict]:
    """è·å–æ–‡ç« åˆ—è¡¨"""
    print(f"ğŸ“¡ æ­£åœ¨è·å–æœ€è¿‘ {days} å¤©çš„æ–‡ç« åˆ—è¡¨...")
    
    articles = []
    page = 1
    max_pages = 10  # é™åˆ¶æœ€å¤§é¡µæ•°
    cutoff_date = datetime.now() - timedelta(days=days)
    
    while page <= max_pages:
        # æ„å»º URLï¼ˆæ ¹æ®ç½‘ç«™å®é™… API è°ƒæ•´ï¼‰
        url = f"{ARTICLES_URL}?page={page}"
        
        html = fetch_page(url, client)
        if not html:
            break
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« å¡ç‰‡ï¼ˆæ ¹æ®ç½‘ç«™å®é™…ç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        cards = soup.select('article, [class*="article-card"], [class*="post-item"], .card')
        
        if not cards:
            # å°è¯•å…¶ä»–é€‰æ‹©å™¨
            cards = soup.select('div[class*="article"], div[class*="post"], div[class*="item"]')
        
        if not cards:
            print(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« å¡ç‰‡", file=sys.stderr)
            break
        
        found_old = False
        for card in cards:
            article = parse_article_card(card)
            if article:
                # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨èŒƒå›´å†…
                if article["date"]:
                    try:
                        article_date = datetime.fromisoformat(article["date"].replace('Z', '+00:00'))
                        if article_date.replace(tzinfo=None) < cutoff_date:
                            found_old = True
                            continue
                    except ValueError:
                        pass  # æ—¥æœŸæ ¼å¼ä¸æ ‡å‡†ï¼Œä¿ç•™æ–‡ç« 
                
                # åˆ†ç±»ç­›é€‰
                if category and article["category"] and category not in article["category"]:
                    continue
                
                articles.append(article)
        
        print(f"  ğŸ“„ ç¬¬ {page} é¡µ: è·å– {len(cards)} ç¯‡æ–‡ç« ")
        
        if found_old:
            print("  â° å·²åˆ°è¾¾æ—¶é—´èŒƒå›´è¾¹ç•Œ")
            break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        next_link = soup.select_one('a[class*="next"], [aria-label="ä¸‹ä¸€é¡µ"], .pagination a:last-child')
        if not next_link:
            break
        
        page += 1
        time.sleep(REQUEST_DELAY)
    
    print(f"âœ… å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def html_to_markdown(html_content: str, base_url: str = "") -> str:
    """å°† HTML è½¬æ¢ä¸º Markdownï¼Œä¿ç•™æ ¼å¼"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
    for tag in soup.select('script, style, nav, footer, header, aside'):
        tag.decompose()
    
    lines = []
    
    def process_element(elem, depth=0):
        if isinstance(elem, str):
            text = elem.strip()
            if text:
                lines.append(text)
            return
        
        tag_name = elem.name if hasattr(elem, 'name') else None
        
        if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            level = int(tag_name[1])
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"\n{'#' * level} {text}\n")
        
        elif tag_name == 'p':
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"\n{text}\n")
        
        elif tag_name in ['pre', 'code']:
            code = elem.get_text()
            lang = ''
            if elem.get('class'):
                for cls in elem.get('class', []):
                    if cls.startswith('language-'):
                        lang = cls.replace('language-', '')
                        break
            if tag_name == 'pre' or '\n' in code:
                lines.append(f"\n```{lang}\n{code}\n```\n")
            else:
                lines.append(f"`{code}`")
        
        elif tag_name == 'img':
            src = elem.get('src', '')
            alt = elem.get('alt', 'å›¾ç‰‡')
            if src:
                if not src.startswith('http'):
                    src = urljoin(base_url, src)
                lines.append(f"\n![{alt}]({src})\n")
        
        elif tag_name == 'a':
            href = elem.get('href', '')
            text = elem.get_text(strip=True)
            if href and text:
                if not href.startswith('http'):
                    href = urljoin(base_url, href)
                lines.append(f"[{text}]({href})")
        
        elif tag_name in ['ul', 'ol']:
            lines.append("")
            for i, li in enumerate(elem.find_all('li', recursive=False)):
                prefix = f"{i+1}. " if tag_name == 'ol' else "- "
                text = li.get_text(strip=True)
                if text:
                    lines.append(f"{prefix}{text}")
            lines.append("")
        
        elif tag_name == 'blockquote':
            text = elem.get_text(strip=True)
            if text:
                quoted = '\n'.join(f"> {line}" for line in text.split('\n'))
                lines.append(f"\n{quoted}\n")
        
        elif tag_name in ['strong', 'b']:
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"**{text}**")
        
        elif tag_name in ['em', 'i']:
            text = elem.get_text(strip=True)
            if text:
                lines.append(f"*{text}*")
        
        elif tag_name in ['div', 'section', 'article', 'main']:
            for child in elem.children:
                process_element(child, depth + 1)
        
        elif hasattr(elem, 'children'):
            for child in elem.children:
                process_element(child, depth)
    
    # æŸ¥æ‰¾æ–‡ç« ä¸»ä½“
    main_content = soup.select_one('article, main, [class*="content"], [class*="post-body"]')
    if main_content:
        process_element(main_content)
    else:
        process_element(soup.body if soup.body else soup)
    
    # æ¸…ç†è¾“å‡º
    result = '\n'.join(lines)
    result = re.sub(r'\n{3,}', '\n\n', result)  # å‹ç¼©å¤šä½™ç©ºè¡Œ
    return result.strip()


def fetch_article_detail(url: str, client: httpx.Client) -> Optional[str]:
    """è·å–æ–‡ç« è¯¦æƒ…å¹¶è½¬æ¢ä¸º Markdown"""
    html = fetch_page(url, client)
    if not html:
        return None
    
    return html_to_markdown(html, url)


def save_article(article: dict, content: str) -> str:
    """ä¿å­˜æ–‡ç« ä¸º Markdown æ–‡ä»¶"""
    ARTICLES_DIR.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    date_str = article.get("date", datetime.now().strftime("%Y-%m-%d"))
    if isinstance(date_str, str) and len(date_str) >= 10:
        date_str = date_str[:10]
    else:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    slug = slugify(article["title"])
    url_hash = get_url_hash(article["url"])
    filename = f"{date_str}-{slug}-{url_hash}.md"
    filepath = ARTICLES_DIR / filename
    
    # æ„å»º frontmatter
    frontmatter = f"""---
title: "{article['title']}"
source: "{article.get('source', '')}"
url: "{article['url']}"
date: "{article.get('date', '')}"
category: "{article.get('category', '')}"
score: {article.get('score', 0)}
word_count: {article.get('word_count', 0)}
crawled_at: "{datetime.now().isoformat()}"
---

"""
    
    full_content = frontmatter + content
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    return f"articles/{filename}"


def generate_weekly_report(index: dict) -> str:
    """ç”Ÿæˆå‘¨æŠ¥"""
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    articles = list(index.get("articles", {}).values())
    
    # æŒ‰è¯„åˆ†æ’åº
    articles_by_score = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for article in articles:
        cat = article.get("category", "å…¶ä»–") or "å…¶ä»–"
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(article)
    
    # ç”ŸæˆæŠ¥å‘Š
    now = datetime.now()
    week_num = now.isocalendar()[1]
    
    report = f"""# AIèµ„è®¯å‘¨æŠ¥ - {now.year}å¹´ç¬¬{week_num}å‘¨

> æœ¬å‘¨æ”¶å½• **{len(articles)}** ç¯‡ AI å‰æ²¿æ–‡ç« 
> æ•°æ®æ¥æº: bestblogs.dev
> ç”Ÿæˆæ—¶é—´: {now.strftime("%Y-%m-%d %H:%M")}

## ğŸ”¥ çƒ­é—¨æ–‡ç«  TOP 5

"""
    
    for i, article in enumerate(articles_by_score[:5], 1):
        report += f"""{i}. **[{article['title']}]({article['url']})** - {article.get('source', 'æœªçŸ¥')} | â­ {article.get('score', 0)}
   > {article.get('summary', '')[:150]}...

"""
    
    report += "\n## ğŸ“‚ æŒ‰åˆ†ç±»æµè§ˆ\n\n"
    
    for cat, cat_articles in sorted(by_category.items(), key=lambda x: len(x[1]), reverse=True):
        report += f"### {cat} ({len(cat_articles)}ç¯‡)\n\n"
        for article in cat_articles[:10]:  # æ¯åˆ†ç±»æœ€å¤šæ˜¾ç¤º10ç¯‡
            report += f"- [{article['title']}]({article['url']}) - {article.get('source', '')}\n"
        if len(cat_articles) > 10:
            report += f"- *...è¿˜æœ‰ {len(cat_articles) - 10} ç¯‡*\n"
        report += "\n"
    
    report += """---
*ç”± AIèµ„è®¯è¿½è¸ªå®˜ è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    # ä¿å­˜æŠ¥å‘Š
    filename = f"weekly_{now.strftime('%Y-%m-%d')}.md"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… å‘¨æŠ¥å·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def main():
    parser = argparse.ArgumentParser(description='AIèµ„è®¯è¿½è¸ªå®˜ - æ–‡ç« çˆ¬è™«')
    parser.add_argument('--days', type=int, default=7, help='è·å–æœ€è¿‘Nå¤©çš„æ–‡ç« ï¼ˆé»˜è®¤7å¤©ï¼‰')
    parser.add_argument('--category', type=str, help='æŒ‰åˆ†ç±»ç­›é€‰')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶å…¨é‡æ›´æ–°ï¼Œå¿½ç•¥ç¼“å­˜')
    parser.add_argument('--report', choices=['weekly', 'daily'], help='ç”ŸæˆæŠ¥å‘Š')
    parser.add_argument('--list-only', action='store_true', help='åªè·å–åˆ—è¡¨ï¼Œä¸æŠ“å–è¯¦æƒ…')
    
    args = parser.parse_args()
    
    # åŠ è½½ç´¢å¼•
    index = load_index()
    
    # å¦‚æœåªæ˜¯ç”ŸæˆæŠ¥å‘Š
    if args.report:
        if args.report == 'weekly':
            generate_weekly_report(index)
        return
    
    # é…ç½®ä»£ç†
    proxy_config = None
    if HTTPS_PROXY or HTTP_PROXY:
        proxy_url = HTTPS_PROXY or HTTP_PROXY
        proxy_config = proxy_url
        print(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy_url}")
    
    # åˆ›å»º HTTP å®¢æˆ·ç«¯
    with httpx.Client(
        headers={"User-Agent": USER_AGENT},
        timeout=30.0,
        follow_redirects=True,
        proxy=proxy_config
    ) as client:
        
        # è·å–æ–‡ç« åˆ—è¡¨
        articles = fetch_article_list(client, args.days, args.category)
        
        if not articles:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ", file=sys.stderr)
            sys.exit(1)
        
        # ç­›é€‰æ–°æ–‡ç« 
        new_articles = []
        for article in articles:
            url_hash = get_url_hash(article["url"])
            if args.force or url_hash not in index["articles"]:
                new_articles.append(article)
        
        print(f"ğŸ“Š æ–°æ–‡ç« : {len(new_articles)} / æ€»è®¡: {len(articles)}")
        
        if args.list_only:
            # åªè¾“å‡ºåˆ—è¡¨
            print("\nğŸ“° æ–‡ç« åˆ—è¡¨:")
            for article in articles:
                print(f"  - [{article['title']}] {article['url']}")
            return
        
        # æŠ“å–æ–°æ–‡ç« è¯¦æƒ…
        for i, article in enumerate(new_articles, 1):
            print(f"ğŸ“¥ [{i}/{len(new_articles)}] æ­£åœ¨æŠ“å–: {article['title'][:40]}...")
            
            content = fetch_article_detail(article["url"], client)
            if content:
                file_path = save_article(article, content)
                
                # æ›´æ–°ç´¢å¼•
                url_hash = get_url_hash(article["url"])
                index["articles"][url_hash] = {
                    **article,
                    "crawled_at": datetime.now().isoformat(),
                    "file_path": file_path
                }
            
            time.sleep(REQUEST_DELAY)
        
        # ä¿å­˜ç´¢å¼•
        save_index(index)
        
        print(f"\nâœ… å®Œæˆ! å·²ä¿å­˜ {len(new_articles)} ç¯‡æ–°æ–‡ç« ")
        print(f"ğŸ“ æ–‡ç« ç›®å½•: {ARTICLES_DIR}")
        print(f"ğŸ“‹ ç´¢å¼•æ–‡ä»¶: {INDEX_FILE}")


if __name__ == "__main__":
    main()


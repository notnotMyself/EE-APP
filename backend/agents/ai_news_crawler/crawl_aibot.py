#!/usr/bin/env python3
"""
AIèµ„è®¯è¿½è¸ªå®˜ - AIå·¥å…·é›†(ai-bot.cn)çˆ¬è™«è„šæœ¬

çˆ¬å– ai-bot.cn/daily-ai-news/ çš„æ¯æ—¥ AI èµ„è®¯
ç”Ÿæˆç²¾ç¾æ—¥æŠ¥ï¼ˆå‚è€ƒ SACC-AI Native å®éªŒå®¤é£æ ¼ï¼‰
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import httpx
from bs4 import BeautifulSoup

# é…ç½®
BASE_URL = "https://ai-bot.cn"
NEWS_URL = f"{BASE_URL}/daily-ai-news/"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# è·¯å¾„é…ç½®
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
REPORTS_DIR = SCRIPT_DIR / "reports"
AIBOT_INDEX = DATA_DIR / "aibot_index.json"

# åˆ†ç±»æ˜ å°„ï¼ˆæ ¹æ®å…³é”®è¯è‡ªåŠ¨åˆ†ç±»ï¼‰
CATEGORY_KEYWORDS = {
    "äº§ä¸šé‡ç£…": ["èèµ„", "æŠ•èµ„", "æ”¶è´­", "ä¸Šå¸‚", "ä¼°å€¼", "äº¿å…ƒ", "äº¿ç¾å…ƒ", "åˆä½œ"],
    "å‰æ²¿æŠ€æœ¯": ["å¼€æº", "æ¨¡å‹", "æ¡†æ¶", "ç®—æ³•", "è®ºæ–‡", "ç ”ç©¶", "å‘å¸ƒ", "æ¨å‡º"],
    "å·¥å…·å‘å¸ƒ": ["å·¥å…·", "å¹³å°", "åº”ç”¨", "åŠŸèƒ½", "ä¸Šçº¿", "æ›´æ–°", "ç‰ˆæœ¬"],
    "å®‰å…¨åˆè§„": ["å®‰å…¨", "éšç§", "åˆè§„", "ç›‘ç®¡", "æŠ¤æ "],
}


def categorize_news(title: str, summary: str) -> tuple[str, str]:
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»ï¼Œè¿”å› (ç±»åˆ«, æ ‡ç­¾)"""
    text = title + summary
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                # ç¡®å®šæ ‡ç­¾
                if "èèµ„" in text or "æŠ•èµ„" in text:
                    tag = "é‡ç£…"
                elif "å¼€æº" in text:
                    tag = "å¼€æº"
                elif "è®ºæ–‡" in text or "ç ”ç©¶" in text:
                    tag = "å­¦æœ¯"
                elif "å·¥å…·" in text or "å¹³å°" in text:
                    tag = "å·¥å…·"
                elif "å®‰å…¨" in text:
                    tag = "å®‰å…¨"
                else:
                    tag = ""
                return category, tag
    
    return "å‰æ²¿æŠ€æœ¯", ""


def load_index() -> dict:
    """åŠ è½½ç´¢å¼•"""
    if AIBOT_INDEX.exists():
        with open(AIBOT_INDEX, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "last_updated": None,
        "source": "ai-bot.cn",
        "news_by_date": {}
    }


def save_index(index: dict):
    """ä¿å­˜ç´¢å¼•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    index["last_updated"] = datetime.now().isoformat()
    with open(AIBOT_INDEX, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def fetch_news(days: int = 3) -> dict[str, list[dict]]:
    """
    è·å–æœ€è¿‘ N å¤©çš„æ–°é—»
    è¿”å›æ ¼å¼: {"1æœˆ6Â·å‘¨äºŒ": [news1, news2, ...], ...}
    """
    print(f"ğŸ“¡ æ­£åœ¨è·å– ai-bot.cn æœ€è¿‘ {days} å¤©çš„ AI èµ„è®¯...")
    
    try:
        with httpx.Client(
            headers={"User-Agent": USER_AGENT},
            timeout=30.0,
            follow_redirects=True
        ) as client:
            response = client.get(NEWS_URL)
            response.raise_for_status()
            html = response.text
    except httpx.HTTPError as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return {}
    
    soup = BeautifulSoup(html, 'html.parser')
    
    news_by_date = {}
    current_date = None
    collected_dates = 0
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¥æœŸå’Œæ–°é—»é¡¹
    for elem in soup.select('.news-date, .news-item'):
        if 'news-date' in elem.get('class', []):
            # æ–°çš„æ—¥æœŸ
            current_date = elem.get_text(strip=True)
            if current_date not in news_by_date:
                news_by_date[current_date] = []
                collected_dates += 1
                if collected_dates > days:
                    break
        
        elif 'news-item' in elem.get('class', []) and current_date:
            # æ–°é—»æ¡ç›®
            content = elem.select_one('.news-content')
            if not content:
                continue
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = content.select_one('h2 a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            url = title_elem.get('href', '')
            
            # æå–æ‘˜è¦
            summary_elem = content.select_one('p')
            summary = ""
            source = ""
            if summary_elem:
                # ç§»é™¤æ¥æºæ ‡ç­¾åçš„æ–‡æœ¬ä½œä¸ºæ‘˜è¦
                summary_text = summary_elem.get_text(strip=True)
                # æå–æ¥æº
                source_elem = summary_elem.select_one('.news-time')
                if source_elem:
                    source = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
                    summary = summary_text.replace(source_elem.get_text(), '').strip()
                else:
                    summary = summary_text
            
            # è‡ªåŠ¨åˆ†ç±»
            category, tag = categorize_news(title, summary)
            
            news_item = {
                "title": title,
                "url": url,
                "summary": summary,
                "source": source,
                "category": category,
                "tag": tag
            }
            
            news_by_date[current_date].append(news_item)
    
    # ç»Ÿè®¡
    total_news = sum(len(items) for items in news_by_date.values())
    print(f"âœ… è·å– {len(news_by_date)} å¤©å…± {total_news} æ¡èµ„è®¯")
    
    return news_by_date


def generate_daily_report(news_by_date: dict, date_key: str = None) -> str:
    """
    ç”Ÿæˆæ—¥æŠ¥ï¼ˆå‚è€ƒ SACC-AI Native å®éªŒå®¤é£æ ¼ï¼‰
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°çš„ä¸€å¤©
    if date_key is None:
        date_key = list(news_by_date.keys())[0] if news_by_date else None
    
    if not date_key or date_key not in news_by_date:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ—¥æœŸçš„æ–°é—»", file=sys.stderr)
        return ""
    
    news_items = news_by_date[date_key]
    
    # è§£ææ—¥æœŸ
    now = datetime.now()
    # ä» "1æœˆ6Â·å‘¨äºŒ" æ ¼å¼è§£æ
    date_match = re.match(r'(\d+)æœˆ(\d+)', date_key)
    if date_match:
        month, day = int(date_match.group(1)), int(date_match.group(2))
        report_date = f"{now.year}.{month:02d}.{day:02d}"
    else:
        report_date = now.strftime("%Y.%m.%d")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for item in news_items:
        cat = item.get("category", "å‰æ²¿æŠ€æœ¯")
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    # ç”Ÿæˆæ—¥æŠ¥å¤´éƒ¨æ‘˜è¦
    highlights = []
    for item in news_items[:3]:  # å–å‰3æ¡ä½œä¸ºå¤´æ¡
        title_short = item["title"][:15] + "..." if len(item["title"]) > 15 else item["title"]
        highlights.append(title_short)
    headline = "ï¼Œ".join(highlights) + "ã€‚"
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# AI Daily Report

**DAILY NEWS** {report_date}

*AIèµ„è®¯è¿½è¸ªå®˜*

{headline}

---

"""
    
    # æŒ‰åˆ†ç±»è¾“å‡º
    category_icons = {
        "äº§ä¸šé‡ç£…": "ğŸ“ˆ",
        "å‰æ²¿æŠ€æœ¯": "ğŸ”¬",
        "å·¥å…·å‘å¸ƒ": "ğŸ› ï¸",
        "å®‰å…¨åˆè§„": "ğŸ”’",
    }
    
    for category in ["äº§ä¸šé‡ç£…", "å‰æ²¿æŠ€æœ¯", "å·¥å…·å‘å¸ƒ", "å®‰å…¨åˆè§„"]:
        if category not in by_category:
            continue
        
        icon = category_icons.get(category, "ğŸ“Œ")
        report += f"## {icon} {category}\n\n"
        
        for item in by_category[category]:
            tag = f"**{item['tag']}** " if item.get('tag') else ""
            source = f" ({item['source']})" if item.get('source') else ""
            
            report += f"### {tag}[{item['title']}]({item['url']}){source}\n\n"
            report += f"{item['summary']}\n\n"
        
        report += "---\n\n"
    
    report += f"""
> âš¡ æ¯æ—¥æ›´æ–° Â· æ´å¯Ÿæœªæ¥
> 
> *ç”± AIèµ„è®¯è¿½è¸ªå®˜ è‡ªåŠ¨ç”Ÿæˆ - {datetime.now().strftime("%Y-%m-%d %H:%M")}*
> 
> æ•°æ®æ¥æº: [AIå·¥å…·é›†](https://ai-bot.cn/daily-ai-news/)
"""
    
    # ä¿å­˜æŠ¥å‘Š
    filename = f"daily_{report_date.replace('.', '-')}.md"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æ—¥æŠ¥å·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def generate_json_report(news_by_date: dict, date_key: str = None) -> str:
    """ç”Ÿæˆ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼ˆä¾¿äºå‰ç«¯æ¸²æŸ“ï¼‰"""
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    if date_key is None:
        date_key = list(news_by_date.keys())[0] if news_by_date else None
    
    if not date_key or date_key not in news_by_date:
        return ""
    
    news_items = news_by_date[date_key]
    
    # è§£ææ—¥æœŸ
    now = datetime.now()
    date_match = re.match(r'(\d+)æœˆ(\d+)', date_key)
    if date_match:
        month, day = int(date_match.group(1)), int(date_match.group(2))
        report_date = f"{now.year}-{month:02d}-{day:02d}"
    else:
        report_date = now.strftime("%Y-%m-%d")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for item in news_items:
        cat = item.get("category", "å‰æ²¿æŠ€æœ¯")
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    report_data = {
        "date": report_date,
        "date_display": date_key,
        "generated_at": datetime.now().isoformat(),
        "total_news": len(news_items),
        "source": "ai-bot.cn",
        "categories": by_category,
        "highlights": [item["title"] for item in news_items[:3]]
    }
    
    filename = f"daily_{report_date}.json"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSON æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def print_news_summary(news_by_date: dict):
    """åœ¨ç»ˆç«¯æ‰“å°æ–°é—»æ‘˜è¦"""
    for date_key, items in news_by_date.items():
        print(f"\nğŸ“… {date_key} ({len(items)} æ¡)")
        print("-" * 50)
        for i, item in enumerate(items, 1):
            tag = f"[{item['tag']}] " if item.get('tag') else ""
            print(f"  {i}. {tag}{item['title']}")
            if item.get('source'):
                print(f"     â””â”€ æ¥æº: {item['source']}")


def main():
    parser = argparse.ArgumentParser(description='AIèµ„è®¯è¿½è¸ªå®˜ - AIå·¥å…·é›†çˆ¬è™«')
    parser.add_argument('--days', type=int, default=3, help='è·å–æœ€è¿‘Nå¤©çš„èµ„è®¯ï¼ˆé»˜è®¤3å¤©ï¼‰')
    parser.add_argument('--report', choices=['daily', 'json', 'both'], help='ç”ŸæˆæŠ¥å‘Šç±»å‹')
    parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸç”ŸæˆæŠ¥å‘Šï¼ˆå¦‚ "1æœˆ6Â·å‘¨äºŒ"ï¼‰')
    parser.add_argument('--list', action='store_true', help='åªåˆ—å‡ºèµ„è®¯ï¼Œä¸ç”ŸæˆæŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # è·å–æ–°é—»
    news_by_date = fetch_news(args.days)
    
    if not news_by_date:
        print("âŒ æœªè·å–åˆ°ä»»ä½•èµ„è®¯", file=sys.stderr)
        sys.exit(1)
    
    # ä¿å­˜ç´¢å¼•
    index = load_index()
    index["news_by_date"].update(news_by_date)
    save_index(index)
    
    # åˆ—å‡ºæ–°é—»
    if args.list:
        print_news_summary(news_by_date)
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        date_key = args.date or list(news_by_date.keys())[0]
        
        if args.report in ['daily', 'both']:
            generate_daily_report(news_by_date, date_key)
        
        if args.report in ['json', 'both']:
            generate_json_report(news_by_date, date_key)
    else:
        # é»˜è®¤æ‰“å°æ‘˜è¦
        print_news_summary(news_by_date)
        print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ --report daily ç”Ÿæˆæ—¥æŠ¥")


if __name__ == "__main__":
    main()


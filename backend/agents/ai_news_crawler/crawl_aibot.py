#!/usr/bin/env python3
"""
AIèµ„è®¯è¿½è¸ªå®˜ - AIå·¥å…·é›†(ai-bot.cn)çˆ¬è™«è„šæœ¬

çˆ¬å– ai-bot.cn/daily-ai-news/ çš„æ¯æ—¥ AI èµ„è®¯
ç”Ÿæˆç²¾ç¾æ—¥æŠ¥ï¼ˆå‚è€ƒ SACC-AI Native å®éªŒå®¤é£æ ¼ï¼‰
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import httpx
from bs4 import BeautifulSoup

# é…ç½®
BASE_URL = "https://ai-bot.cn"
NEWS_URL = f"{BASE_URL}/daily-ai-news/"
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# è·¯å¾„é…ç½®
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
REPORTS_DIR = SCRIPT_DIR / "reports"
AIBOT_INDEX = DATA_DIR / "aibot_index.json"

# åˆ†ç±»æ˜ å°„ï¼ˆæ ¹æ®å…³é”®è¯è‡ªåŠ¨åˆ†ç±»ï¼‰
CATEGORY_KEYWORDS = {
    "äº§ä¸šé‡ç£…": ["èèµ„", "æŠ•èµ„", "æ”¶è´­", "ä¸Šå¸‚", "ä¼°å€¼", "äº¿å…ƒ", "äº¿ç¾å…ƒ", "åˆä½œ"],
    "å‰æ²¿æŠ€æœ¯": ["å¼€æº", "æ¨¡å‹", "æ¡†æ¶", "ç®—æ³•", "è®ºæ–‡", "ç ”ç©¶", "å‘å¸ƒ", "æ¨å‡º"],
    "å·¥å…·å‘å¸ƒ": ["å·¥å…·", "å¹³å°", "åº”ç”¨", "åŠŸèƒ½", "ä¸Šçº¿", "æ›´æ–°", "ç‰ˆæœ¬"],
    "å®‰å…¨åˆè§„": ["å®‰å…¨", "éšç§", "åˆè§„", "ç›‘ç®¡", "æŠ¤æ "],
}


def categorize_news(title: str, summary: str) -> tuple[str, str]:
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»ï¼Œè¿”å› (ç±»åˆ«, æ ‡ç­¾)"""
    text = title + summary
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                # ç¡®å®šæ ‡ç­¾
                if "èèµ„" in text or "æŠ•èµ„" in text:
                    tag = "é‡ç£…"
                elif "å¼€æº" in text:
                    tag = "å¼€æº"
                elif "è®ºæ–‡" in text or "ç ”ç©¶" in text:
                    tag = "å­¦æœ¯"
                elif "å·¥å…·" in text or "å¹³å°" in text:
                    tag = "å·¥å…·"
                elif "å®‰å…¨" in text:
                    tag = "å®‰å…¨"
                else:
                    tag = ""
                return category, tag
    
    return "å‰æ²¿æŠ€æœ¯", ""


def load_index() -> dict:
    """åŠ è½½ç´¢å¼•"""
    if AIBOT_INDEX.exists():
        with open(AIBOT_INDEX, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "last_updated": None,
        "source": "ai-bot.cn",
        "news_by_date": {}
    }


def save_index(index: dict):
    """ä¿å­˜ç´¢å¼•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    index["last_updated"] = datetime.now().isoformat()
    with open(AIBOT_INDEX, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def fetch_news(days: int = 3) -> dict[str, list[dict]]:
    """
    è·å–æœ€è¿‘ N å¤©çš„æ–°é—»
    è¿”å›æ ¼å¼: {"1æœˆ6Â·å‘¨äºŒ": [news1, news2, ...], ...}
    """
    print(f"ğŸ“¡ æ­£åœ¨è·å– ai-bot.cn æœ€è¿‘ {days} å¤©çš„ AI èµ„è®¯...")
    
    try:
        with httpx.Client(
            headers={"User-Agent": USER_AGENT},
            timeout=30.0,
            follow_redirects=True
        ) as client:
            response = client.get(NEWS_URL)
            response.raise_for_status()
            html = response.text
    except httpx.HTTPError as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return {}
    
    soup = BeautifulSoup(html, 'html.parser')
    
    news_by_date = {}
    current_date = None
    collected_dates = 0
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¥æœŸå’Œæ–°é—»é¡¹
    for elem in soup.select('.news-date, .news-item'):
        if 'news-date' in elem.get('class', []):
            # æ–°çš„æ—¥æœŸ
            current_date = elem.get_text(strip=True)
            if current_date not in news_by_date:
                news_by_date[current_date] = []
                collected_dates += 1
                if collected_dates > days:
                    break
        
        elif 'news-item' in elem.get('class', []) and current_date:
            # æ–°é—»æ¡ç›®
            content = elem.select_one('.news-content')
            if not content:
                continue
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = content.select_one('h2 a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            url = title_elem.get('href', '')
            
            # æå–æ‘˜è¦
            summary_elem = content.select_one('p')
            summary = ""
            source = ""
            if summary_elem:
                # ç§»é™¤æ¥æºæ ‡ç­¾åçš„æ–‡æœ¬ä½œä¸ºæ‘˜è¦
                summary_text = summary_elem.get_text(strip=True)
                # æå–æ¥æº
                source_elem = summary_elem.select_one('.news-time')
                if source_elem:
                    source = source_elem.get_text(strip=True).replace('æ¥æºï¼š', '')
                    summary = summary_text.replace(source_elem.get_text(), '').strip()
                else:
                    summary = summary_text
            
            # è‡ªåŠ¨åˆ†ç±»
            category, tag = categorize_news(title, summary)
            
            news_item = {
                "title": title,
                "url": url,
                "summary": summary,
                "source": source,
                "category": category,
                "tag": tag
            }
            
            news_by_date[current_date].append(news_item)
    
    # ç»Ÿè®¡
    total_news = sum(len(items) for items in news_by_date.values())
    print(f"âœ… è·å– {len(news_by_date)} å¤©å…± {total_news} æ¡èµ„è®¯")
    
    return news_by_date


def generate_daily_report(news_by_date: dict, date_key: str = None) -> str:
    """
    ç”Ÿæˆæ—¥æŠ¥ï¼ˆå‚è€ƒ SACC-AI Native å®éªŒå®¤é£æ ¼ï¼‰
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°çš„ä¸€å¤©
    if date_key is None:
        date_key = list(news_by_date.keys())[0] if news_by_date else None
    
    if not date_key or date_key not in news_by_date:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡å®šæ—¥æœŸçš„æ–°é—»", file=sys.stderr)
        return ""
    
    news_items = news_by_date[date_key]
    
    # è§£ææ—¥æœŸ
    now = datetime.now()
    # ä» "1æœˆ6Â·å‘¨äºŒ" æ ¼å¼è§£æ
    date_match = re.match(r'(\d+)æœˆ(\d+)', date_key)
    if date_match:
        month, day = int(date_match.group(1)), int(date_match.group(2))
        report_date = f"{now.year}.{month:02d}.{day:02d}"
    else:
        report_date = now.strftime("%Y.%m.%d")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for item in news_items:
        cat = item.get("category", "å‰æ²¿æŠ€æœ¯")
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    # ç”Ÿæˆæ—¥æŠ¥å¤´éƒ¨æ‘˜è¦
    highlights = []
    for item in news_items[:3]:  # å–å‰3æ¡ä½œä¸ºå¤´æ¡
        title_short = item["title"][:15] + "..." if len(item["title"]) > 15 else item["title"]
        highlights.append(title_short)
    headline = "ï¼Œ".join(highlights) + "ã€‚"
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# AI Daily Report

**DAILY NEWS** {report_date}

*AIèµ„è®¯è¿½è¸ªå®˜*

{headline}

---

"""
    
    # æŒ‰åˆ†ç±»è¾“å‡º
    category_icons = {
        "äº§ä¸šé‡ç£…": "ğŸ“ˆ",
        "å‰æ²¿æŠ€æœ¯": "ğŸ”¬",
        "å·¥å…·å‘å¸ƒ": "ğŸ› ï¸",
        "å®‰å…¨åˆè§„": "ğŸ”’",
    }
    
    for category in ["äº§ä¸šé‡ç£…", "å‰æ²¿æŠ€æœ¯", "å·¥å…·å‘å¸ƒ", "å®‰å…¨åˆè§„"]:
        if category not in by_category:
            continue
        
        icon = category_icons.get(category, "ğŸ“Œ")
        report += f"## {icon} {category}\n\n"
        
        for item in by_category[category]:
            tag = f"**{item['tag']}** " if item.get('tag') else ""
            source = f" ({item['source']})" if item.get('source') else ""
            
            report += f"### {tag}[{item['title']}]({item['url']}){source}\n\n"
            report += f"{item['summary']}\n\n"
        
        report += "---\n\n"
    
    report += f"""
> âš¡ æ¯æ—¥æ›´æ–° Â· æ´å¯Ÿæœªæ¥
> 
> *ç”± AIèµ„è®¯è¿½è¸ªå®˜ è‡ªåŠ¨ç”Ÿæˆ - {datetime.now().strftime("%Y-%m-%d %H:%M")}*
> 
> æ•°æ®æ¥æº: [AIå·¥å…·é›†](https://ai-bot.cn/daily-ai-news/)
"""
    
    # ä¿å­˜æŠ¥å‘Š
    filename = f"daily_{report_date.replace('.', '-')}.md"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æ—¥æŠ¥å·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def generate_json_report(news_by_date: dict, date_key: str = None) -> str:
    """ç”Ÿæˆ JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ï¼ˆä¾¿äºå‰ç«¯æ¸²æŸ“ï¼‰"""
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    if date_key is None:
        date_key = list(news_by_date.keys())[0] if news_by_date else None
    
    if not date_key or date_key not in news_by_date:
        return ""
    
    news_items = news_by_date[date_key]
    
    # è§£ææ—¥æœŸ
    now = datetime.now()
    date_match = re.match(r'(\d+)æœˆ(\d+)', date_key)
    if date_match:
        month, day = int(date_match.group(1)), int(date_match.group(2))
        report_date = f"{now.year}-{month:02d}-{day:02d}"
    else:
        report_date = now.strftime("%Y-%m-%d")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for item in news_items:
        cat = item.get("category", "å‰æ²¿æŠ€æœ¯")
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    report_data = {
        "date": report_date,
        "date_display": date_key,
        "generated_at": datetime.now().isoformat(),
        "total_news": len(news_items),
        "source": "ai-bot.cn",
        "categories": by_category,
        "highlights": [item["title"] for item in news_items[:3]]
    }
    
    filename = f"daily_{report_date}.json"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSON æŠ¥å‘Šå·²ç”Ÿæˆ: {filepath}")
    return str(filepath)


def generate_briefing(news_by_date: dict, date_key: str = None) -> dict:
    """
    ç”Ÿæˆç®€æŠ¥æ ¼å¼ - ç”¨äºä¿¡æ¯æµæ¨é€
    
    ç®€æŠ¥è®¾è®¡åŸåˆ™ï¼š
    1. æ ‡é¢˜åŠ¨è¯å¼€å¤´ï¼Œè¯´æ¸…æ ¸å¿ƒå‘ç°
    2. æ‘˜è¦ç®€æ´æœ‰åŠ›
    3. åªåœ¨æœ‰ä»·å€¼æ—¶æ¨é€
    
    Returns:
        ç®€æŠ¥æ•°æ®ï¼ŒåŒ…å« should_push åˆ¤æ–­
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    if date_key is None:
        date_key = list(news_by_date.keys())[0] if news_by_date else None
    
    if not date_key or date_key not in news_by_date:
        return {
            "error": "No news found",
            "should_push": False
        }
    
    news_items = news_by_date[date_key]
    
    # è§£ææ—¥æœŸ
    now = datetime.now()
    date_match = re.match(r'(\d+)æœˆ(\d+)', date_key)
    if date_match:
        month, day = int(date_match.group(1)), int(date_match.group(2))
        report_date = f"{now.year}-{month:02d}-{day:02d}"
    else:
        report_date = now.strftime("%Y-%m-%d")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for item in news_items:
        cat = item.get("category", "å‰æ²¿æŠ€æœ¯")
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(item)
    
    # åˆ¤æ–­æ˜¯å¦å€¼å¾—æ¨é€
    should_push = False
    priority = "P2"
    
    # äº§ä¸šé‡ç£…æ–°é—»æ•°é‡
    major_news_count = len(by_category.get("äº§ä¸šé‡ç£…", []))
    # é‡ç£…æ ‡ç­¾æ•°é‡
    hot_news_count = sum(1 for item in news_items if item.get("tag") == "é‡ç£…")
    
    # æ¨é€åˆ¤æ–­é€»è¾‘
    if major_news_count >= 2 or hot_news_count >= 2:
        should_push = True
        priority = "P1"  # é‡è¦
    elif len(news_items) >= 3:
        should_push = True
        priority = "P2"  # æ™®é€š
    elif major_news_count >= 1:
        should_push = True
        priority = "P2"
    
    # ç”Ÿæˆæ ‡é¢˜ï¼ˆåŠ¨è¯å¼€å¤´ï¼Œè¯´æ¸…æ ¸å¿ƒå‘ç°ï¼‰
    if news_items:
        top_news = news_items[0]
        # æå–å…³é”®ä¿¡æ¯
        title_text = top_news["title"]
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯é‡å¤§æ–°é—»
        if "ä¸Šå¸‚" in title_text:
            title = f"ä»Šæ—¥AIå¤´æ¡ï¼š{title_text[:20]}..."
        elif "èèµ„" in title_text or "æŠ•èµ„" in title_text:
            title = f"AIäº§ä¸šåŠ¨æ€ï¼š{title_text[:20]}..."
        elif "å‘å¸ƒ" in title_text or "æ¨å‡º" in title_text:
            title = f"æ–°å“é€Ÿé€’ï¼š{title_text[:20]}..."
        elif "å¼€æº" in title_text:
            title = f"å¼€æºåŠ¨æ€ï¼š{title_text[:20]}..."
        else:
            title = f"ä»Šæ—¥AIèµ„è®¯ï¼š{len(news_items)}æ¡é‡è¦åŠ¨æ€"
    else:
        title = "ä»Šæ—¥æš‚æ— é‡è¦AIèµ„è®¯"
        should_push = False
    
    # ç”Ÿæˆæ‘˜è¦ï¼ˆç®€æ´æœ‰åŠ›ï¼‰
    summary_parts = []
    for item in news_items[:3]:
        # æå–æ ¸å¿ƒå†…å®¹
        item_summary = item.get("summary", "")[:50]
        source = item.get("source", "")
        if source:
            summary_parts.append(f"{item['title'][:15]}...({source})")
        else:
            summary_parts.append(f"{item['title'][:20]}...")
    
    summary = "ï¼›".join(summary_parts) + "ã€‚"
    
    # æŒ‰é‡è¦æ€§æ’åºçš„å…³é”®æ–°é—»
    key_news = []
    # å…ˆæ·»åŠ äº§ä¸šé‡ç£…
    for item in by_category.get("äº§ä¸šé‡ç£…", [])[:2]:
        key_news.append({
            "title": item["title"],
            "source": item.get("source", ""),
            "category": "äº§ä¸šé‡ç£…",
            "url": item.get("url", "")
        })
    # å†æ·»åŠ å…¶ä»–åˆ†ç±»
    for cat in ["å‰æ²¿æŠ€æœ¯", "å·¥å…·å‘å¸ƒ", "å®‰å…¨åˆè§„"]:
        for item in by_category.get(cat, [])[:1]:
            if len(key_news) < 5:
                key_news.append({
                    "title": item["title"],
                    "source": item.get("source", ""),
                    "category": cat,
                    "url": item.get("url", "")
                })
    
    # ç”Ÿæˆç»“æ„åŒ–æ–°é—»åˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯å¡ç‰‡å±•ç¤ºï¼‰
    summary_structured = []
    for idx, item in enumerate(news_items[:5], 1):
        summary_structured.append({
            "index": idx,
            "title": item["title"][:40] + ("..." if len(item["title"]) > 40 else ""),
            "full_title": item["title"],
            "source": item.get("source", ""),
            "category": item.get("category", "å‰æ²¿æŠ€æœ¯"),
            "tag": item.get("tag", ""),
            "url": item.get("url", "")
        })

    briefing = {
        "briefing_type": "ai_news",
        "generated_at": datetime.now().isoformat(),
        "date": report_date,
        "date_display": date_key,
        "should_push": should_push,
        "priority": priority,
        "title": title,
        "summary": summary,
        "summary_structured": summary_structured,  # ç»“æ„åŒ–æ–°é—»åˆ—è¡¨
        "cover_style": "news_list",  # æç¤ºUIä½¿ç”¨ç´§å‡‘æ ·å¼
        "metrics": {
            "total_news": len(news_items),
            "major_news": major_news_count,
            "hot_news": hot_news_count,
            "categories": list(by_category.keys())
        },
        "key_news": key_news,
        "source": "ai-bot.cn"
    }
    
    # ä¿å­˜ç®€æŠ¥
    filename = f"briefing_{report_date}.json"
    filepath = REPORTS_DIR / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(briefing, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç®€æŠ¥å·²ç”Ÿæˆ: {filepath}")
    
    return briefing


def print_news_summary(news_by_date: dict):
    """åœ¨ç»ˆç«¯æ‰“å°æ–°é—»æ‘˜è¦"""
    for date_key, items in news_by_date.items():
        print(f"\nğŸ“… {date_key} ({len(items)} æ¡)")
        print("-" * 50)
        for i, item in enumerate(items, 1):
            tag = f"[{item['tag']}] " if item.get('tag') else ""
            print(f"  {i}. {tag}{item['title']}")
            if item.get('source'):
                print(f"     â””â”€ æ¥æº: {item['source']}")


def main():
    parser = argparse.ArgumentParser(description='AIèµ„è®¯è¿½è¸ªå®˜ - AIå·¥å…·é›†çˆ¬è™«')
    parser.add_argument('--days', type=int, default=3, help='è·å–æœ€è¿‘Nå¤©çš„èµ„è®¯ï¼ˆé»˜è®¤3å¤©ï¼‰')
    parser.add_argument('--report', choices=['daily', 'json', 'briefing', 'both', 'all'], 
                        help='ç”ŸæˆæŠ¥å‘Šç±»å‹: daily(æ—¥æŠ¥), json(ç»“æ„åŒ–æ•°æ®), briefing(ç®€æŠ¥), both(æ—¥æŠ¥+JSON), all(å…¨éƒ¨)')
    parser.add_argument('--date', type=str, help='æŒ‡å®šæ—¥æœŸç”ŸæˆæŠ¥å‘Šï¼ˆå¦‚ "1æœˆ6Â·å‘¨äºŒ"ï¼‰')
    parser.add_argument('--list', action='store_true', help='åªåˆ—å‡ºèµ„è®¯ï¼Œä¸ç”ŸæˆæŠ¥å‘Š')
    parser.add_argument('--output-json', action='store_true', help='è¾“å‡ºJSONåˆ°stdoutï¼ˆç”¨äºç®¡é“ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–æ–°é—»
    news_by_date = fetch_news(args.days)
    
    if not news_by_date:
        print("âŒ æœªè·å–åˆ°ä»»ä½•èµ„è®¯", file=sys.stderr)
        sys.exit(1)
    
    # ä¿å­˜ç´¢å¼•
    index = load_index()
    index["news_by_date"].update(news_by_date)
    save_index(index)
    
    # åˆ—å‡ºæ–°é—»
    if args.list:
        print_news_summary(news_by_date)
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        date_key = args.date or list(news_by_date.keys())[0]
        
        if args.report in ['daily', 'both', 'all']:
            generate_daily_report(news_by_date, date_key)
        
        if args.report in ['json', 'both', 'all']:
            generate_json_report(news_by_date, date_key)
        
        if args.report in ['briefing', 'all']:
            briefing = generate_briefing(news_by_date, date_key)
            if args.output_json:
                # è¾“å‡ºåˆ°stdoutï¼ˆç”¨äºç®¡é“ï¼‰
                print(json.dumps(briefing, ensure_ascii=False, indent=2))
    else:
        # é»˜è®¤æ‰“å°æ‘˜è¦
        print_news_summary(news_by_date)
        print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ --report daily ç”Ÿæˆæ—¥æŠ¥, --report briefing ç”Ÿæˆç®€æŠ¥")


if __name__ == "__main__":
    main()


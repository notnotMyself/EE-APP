#!/usr/bin/env python3
"""
Chris è®¾è®¡èµ„è®¯çˆ¬è™« - Best Designs on X

çˆ¬å– bestdesignsonx.com ä¸Šç²¾é€‰çš„ X (Twitter) è®¾è®¡å¸–å­
é€šè¿‡ç‚¹å‡»å¡ç‰‡è·å–å®Œæ•´å¸–å­å†…å®¹ï¼ˆä½œè€…ã€æ–‡å­—ã€åª’ä½“ï¼‰

ä¾èµ–å®‰è£…ï¼š
    pip install playwright
    playwright install chromium
"""

import argparse
import asyncio
import hashlib
import json
import re
import sys
from datetime import datetime
from pathlib import Path

# è·¯å¾„é…ç½®
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data" / "bestdesignsonx"
REPORTS_DIR = SCRIPT_DIR / "reports"
INDEX_FILE = DATA_DIR / "index.json"

# é…ç½®
BASE_URL = "https://bestdesignsonx.com/"


def get_post_hash(text: str) -> str:
    """ç”Ÿæˆå†…å®¹çš„çŸ­å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode()).hexdigest()[:12]


def load_index() -> dict:
    """åŠ è½½å¸–å­ç´¢å¼•"""
    if INDEX_FILE.exists():
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "last_updated": None,
        "source": "bestdesignsonx.com",
        "posts": {}
    }


def save_index(index: dict):
    """ä¿å­˜å¸–å­ç´¢å¼•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    index["last_updated"] = datetime.now().isoformat()
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


async def fetch_posts_playwright(max_posts: int = 20) -> list[dict]:
    """
    ä½¿ç”¨ Playwright çˆ¬å– bestdesignsonx.com çš„è®¾è®¡å¸–å­
    """
    try:
        from playwright.async_api import async_playwright
    except ImportError:
        print("âŒ æœªå®‰è£… Playwrightï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install chromium", file=sys.stderr)
        return []
    
    print(f"ğŸ“¡ æ­£åœ¨è·å– bestdesignsonx.com çš„è®¾è®¡å¸–å­...")
    
    posts = []
    seen_urls = set()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page(viewport={'width': 1920, 'height': 1080})
        
        try:
            await page.goto(BASE_URL, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(6000)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            for _ in range(2):
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await page.wait_for_timeout(1500)
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # æŸ¥æ‰¾åª’ä½“å›¾ç‰‡å¡ç‰‡
            imgs = await page.query_selector_all('img[src*="cdn.bestdesignsonx.com/media"]')
            print(f"   æ‰¾åˆ° {len(imgs)} ä¸ªè®¾è®¡å¡ç‰‡")
            
            # éå†å¡ç‰‡
            for i, img in enumerate(imgs):
                if len(posts) >= max_posts:
                    break
                
                try:
                    # æ»šåŠ¨åˆ°å›¾ç‰‡ä½ç½®
                    await img.scroll_into_view_if_needed()
                    await page.wait_for_timeout(300)
                    
                    # ç‚¹å‡»æ‰“å¼€å¼¹çª—
                    await img.click()
                    await page.wait_for_timeout(1500)
                    
                    # æå–å¼¹çª—å†…å®¹
                    post_data = await page.evaluate('''() => {
                        // æŸ¥æ‰¾å¼¹çª—å®¹å™¨
                        const modals = document.querySelectorAll('[class*="bg-background"]');
                        for (const modal of modals) {
                            if (!modal.innerText.includes('View on X')) continue;
                            
                            // æå–ä½œè€…ï¼ˆfont-semiboldï¼‰
                            const authorEl = modal.querySelector('.font-semibold');
                            const author = authorEl ? authorEl.innerText.trim() : '';
                            
                            // æå–ç”¨æˆ·åï¼ˆtext-gray-500 åŒ…å«@ï¼‰
                            const usernameEls = modal.querySelectorAll('.text-gray-500, [class*="gray"]');
                            let username = '';
                            for (const el of usernameEls) {
                                const text = el.innerText.trim();
                                if (text.startsWith('@')) {
                                    username = text.substring(1);
                                    break;
                                }
                            }
                            
                            // æå–å†…å®¹ï¼ˆç¬¬äºŒä¸ª text-gray-500ï¼Œä¸åŒ…å«@ï¼‰
                            let content = '';
                            for (const el of usernameEls) {
                                const text = el.innerText.trim();
                                if (!text.startsWith('@') && text.length > 1) {
                                    content = text;
                                    break;
                                }
                            }
                            
                            // æå– X é“¾æ¥
                            const linkEl = modal.querySelector('a[href*="x.com/"][href*="/status/"]');
                            const x_url = linkEl ? linkEl.href : '';
                            
                            // æå–åª’ä½“å›¾ç‰‡
                            const mediaImgs = modal.querySelectorAll('img[src*="cdn.bestdesignsonx.com/media"]');
                            const media_urls = Array.from(mediaImgs).map(img => img.src);
                            
                            // æå–è§†é¢‘
                            const videos = modal.querySelectorAll('video');
                            const video_urls = [];
                            videos.forEach(v => {
                                const src = v.src || v.querySelector('source')?.src;
                                if (src && src.includes('cdn.bestdesignsonx.com')) {
                                    video_urls.push({
                                        src: src,
                                        poster: v.poster || ''
                                    });
                                }
                            });
                            
                            // æå–å¤´åƒ
                            const avatarEl = modal.querySelector('img[src*="twimg.com/profile_images"]');
                            const avatar_url = avatarEl ? avatarEl.src : '';
                            
                            if (x_url) {
                                return {
                                    author: author,
                                    username: username,
                                    content: content,
                                    x_url: x_url,
                                    media_urls: media_urls,
                                    video_urls: video_urls,
                                    avatar_url: avatar_url,
                                    has_video: video_urls.length > 0
                                };
                            }
                        }
                        return null;
                    }''')
                    
                    if post_data and post_data.get('x_url') and post_data['x_url'] not in seen_urls:
                        seen_urls.add(post_data['x_url'])
                        
                        post = {
                            "id": get_post_hash(post_data['x_url']),
                            "author": post_data.get('author', ''),
                            "username": post_data.get('username', ''),
                            "content": post_data.get('content', ''),
                            "x_url": post_data['x_url'],
                            "media_urls": post_data.get('media_urls', []),
                            "video_urls": post_data.get('video_urls', []),
                            "has_video": post_data.get('has_video', False),
                            "avatar_url": post_data.get('avatar_url', ''),
                            "source": "bestdesignsonx",
                            "source_name": "Best Designs on X",
                            "category": "è®¾è®¡çµæ„Ÿ",
                            "tags": ["X/Twitter", "è®¾è®¡"],
                            "fetched_at": datetime.now().isoformat()
                        }
                        
                        posts.append(post)
                        media_type = "ğŸ¬ è§†é¢‘" if post['has_video'] else f"ğŸ“· {len(post['media_urls'])}å›¾"
                        print(f"   [{len(posts)}/{max_posts}] @{post['username']}: {post['content'][:35]}... [{media_type}]")
                    
                    # å…³é—­å¼¹çª—
                    await page.keyboard.press('Escape')
                    await page.wait_for_timeout(500)
                    
                except Exception as e:
                    await page.keyboard.press('Escape')
                    await page.wait_for_timeout(300)
                    continue
            
            print(f"âœ… ä» Best Designs on X è·å– {len(posts)} ä¸ªè®¾è®¡å¸–å­")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}", file=sys.stderr)
        
        finally:
            await browser.close()
    
    return posts


def fetch_posts_sync(max_posts: int = 20) -> list[dict]:
    """åŒæ­¥ç‰ˆæœ¬"""
    return asyncio.run(fetch_posts_playwright(max_posts))


def generate_briefing(posts: list[dict]) -> dict:
    """ç”Ÿæˆç®€æŠ¥"""
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    
    if not posts:
        return {"error": "No posts found", "should_push": False}
    
    report_date = datetime.now().strftime("%Y-%m-%d")
    
    should_push = len(posts) >= 5
    priority = "P1" if len(posts) >= 10 else "P2"
    
    # ç”Ÿæˆæ ‡é¢˜
    if posts:
        author = posts[0].get("author") or posts[0].get("username", "")
        title = f"X è®¾è®¡çµæ„Ÿï¼š@{author} ç­‰ {len(posts)} ä¸ªç²¾é€‰å¸–å­"
    else:
        title = "æœ¬å‘¨æš‚æ— ç²¾é€‰è®¾è®¡å¸–å­"
        should_push = False
    
    # ç”Ÿæˆæ‘˜è¦
    summary_parts = []
    for post in posts[:3]:
        content = post.get("content", "")[:30]
        username = post.get("username", "")
        if content:
            summary_parts.append(f"@{username}: {content}")
    summary = " | ".join(summary_parts) if summary_parts else "ç²¾é€‰ X ä¸Šçš„è®¾è®¡çµæ„Ÿã€‚"
    
    # ç»“æ„åŒ–åˆ—è¡¨
    summary_structured = []
    for idx, post in enumerate(posts[:10], 1):
        summary_structured.append({
            "index": idx,
            "author": post.get("author", ""),
            "username": post.get("username", ""),
            "content": post.get("content", ""),
            "x_url": post.get("x_url", ""),
            "media_urls": post.get("media_urls", [])[:3],
            "avatar_url": post.get("avatar_url", "")
        })
    
    briefing = {
        "briefing_type": "x_design_inspiration",
        "generated_at": datetime.now().isoformat(),
        "date": report_date,
        "should_push": should_push,
        "priority": priority,
        "title": title,
        "summary": summary,
        "summary_structured": summary_structured,
        "cover_style": "social_cards",
        "metrics": {
            "total_posts": len(posts),
            "unique_authors": len(set(p.get("username", "") for p in posts)),
            "with_media": len([p for p in posts if p.get("media_urls")])
        },
        "posts": posts,
        "source": "bestdesignsonx.com"
    }
    
    # ä¿å­˜
    filename = f"x_design_briefing_{report_date}.json"
    filepath = REPORTS_DIR / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(briefing, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç®€æŠ¥å·²ç”Ÿæˆ: {filepath}")
    return briefing


def print_posts_summary(posts: list[dict]):
    """æ‰“å°æ‘˜è¦"""
    video_count = len([p for p in posts if p.get('has_video')])
    image_count = len(posts) - video_count
    
    print(f"\nğŸ¦ Best Designs on X ({len(posts)} ä¸ªå¸–å­: {image_count} å›¾ç‰‡, {video_count} è§†é¢‘)")
    print("-" * 70)
    
    for i, post in enumerate(posts, 1):
        author = post.get("author", "")
        username = post.get("username", "unknown")
        content = post.get("content", "")[:50]
        media_count = len(post.get("media_urls", []))
        has_video = post.get("has_video", False)
        
        display_name = f"{author} @{username}" if author else f"@{username}"
        print(f"  {i}. {display_name}")
        if content:
            print(f"     â””â”€ {content}...")
        if has_video:
            video_urls = post.get("video_urls", [])
            print(f"     â””â”€ ğŸ¬ è§†é¢‘ ({len(video_urls)} ä¸ª)")
        elif media_count:
            print(f"     â””â”€ ğŸ“· {media_count} å¼ å›¾ç‰‡")
        print(f"     â””â”€ {post.get('x_url', '')}")
        print()


def main():
    parser = argparse.ArgumentParser(description='Chris è®¾è®¡èµ„è®¯çˆ¬è™« - Best Designs on X')
    parser.add_argument('--max', type=int, default=20, help='æœ€å¤§å¸–å­æ•°ï¼ˆé»˜è®¤20ï¼‰')
    parser.add_argument('--briefing', action='store_true', help='ç”Ÿæˆç®€æŠ¥')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºå¸–å­')
    parser.add_argument('--output-json', action='store_true', help='è¾“å‡ºJSON')
    
    args = parser.parse_args()
    
    posts = fetch_posts_sync(args.max)
    
    if not posts:
        print("âŒ æœªè·å–åˆ°å¸–å­", file=sys.stderr)
        sys.exit(1)
    
    # ä¿å­˜ç´¢å¼•
    index = load_index()
    for post in posts:
        index["posts"][post["id"]] = post
    save_index(index)
    
    if args.list:
        print_posts_summary(posts)
        return
    
    if args.briefing:
        briefing = generate_briefing(posts)
        if args.output_json:
            print(json.dumps(briefing, ensure_ascii=False, indent=2))
    else:
        print_posts_summary(posts)
        print(f"\nğŸ’¡ ä½¿ç”¨ --briefing ç”Ÿæˆç®€æŠ¥")


if __name__ == "__main__":
    main()

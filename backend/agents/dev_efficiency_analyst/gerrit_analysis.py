#!/usr/bin/env python3
"""
Gerrit Code Review Data Analysis
åˆ†æä»£ç å®¡æŸ¥æ•ˆç‡æŒ‡æ ‡
"""

import json
from datetime import datetime, timedelta
from statistics import median, mean
import sys

def parse_datetime(dt_string):
    """è§£æISOæ ¼å¼æ—¶é—´"""
    return datetime.fromisoformat(dt_string.replace('Z', '+00:00'))

def calculate_review_time(change):
    """è®¡ç®—å•ä¸ªchangeçš„å®¡æŸ¥æ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
    created = parse_datetime(change['created'])
    updated = parse_datetime(change['updated'])
    return (updated - created).total_seconds() / 3600

def calculate_rework_rate(change):
    """è®¡ç®—è¿”å·¥ç‡ï¼ˆrevisionæ•°é‡ > 1è¡¨ç¤ºæœ‰è¿”å·¥ï¼‰"""
    return len(change.get('revisions', {})) > 1

def analyze_gerrit_data(data):
    """åˆ†æGerritæ•°æ®"""
    changes = data.get('changes', [])
    
    if not changes:
        return {
            'error': 'æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„å˜æ›´è®°å½•'
        }
    
    # è®¡ç®—å®¡æŸ¥æ—¶é—´
    review_times = []
    for change in changes:
        review_time = calculate_review_time(change)
        review_times.append(review_time)
    
    # è®¡ç®—è¿”å·¥ç‡
    rework_count = sum(1 for change in changes if calculate_rework_rate(change))
    rework_rate = (rework_count / len(changes)) * 100
    
    # è®¡ç®—P95
    sorted_times = sorted(review_times)
    p95_index = int(len(sorted_times) * 0.95)
    p95_time = sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1]
    
    # é¡¹ç›®ç»´åº¦åˆ†æ
    project_stats = {}
    for change in changes:
        project = change['project']
        if project not in project_stats:
            project_stats[project] = {
                'count': 0,
                'review_times': [],
                'rework_count': 0
            }
        project_stats[project]['count'] += 1
        project_stats[project]['review_times'].append(calculate_review_time(change))
        if calculate_rework_rate(change):
            project_stats[project]['rework_count'] += 1
    
    # æ•´ç†é¡¹ç›®ç»Ÿè®¡
    project_summary = {}
    for project, stats in project_stats.items():
        project_summary[project] = {
            'change_count': stats['count'],
            'avg_review_time_hours': round(mean(stats['review_times']), 2),
            'median_review_time_hours': round(median(stats['review_times']), 2),
            'rework_rate_percent': round((stats['rework_count'] / stats['count']) * 100, 2)
        }
    
    return {
        'summary': {
            'total_changes': len(changes),
            'avg_review_time_hours': round(mean(review_times), 2),
            'median_review_time_hours': round(median(review_times), 2),
            'p95_review_time_hours': round(p95_time, 2),
            'rework_rate_percent': round(rework_rate, 2),
            'merged_count': sum(1 for c in changes if c['status'] == 'MERGED')
        },
        'project_breakdown': project_summary,
        'details': [
            {
                'id': change['id'],
                'project': change['project'],
                'subject': change['subject'],
                'review_time_hours': round(calculate_review_time(change), 2),
                'revision_count': len(change.get('revisions', {})),
                'status': change['status']
            }
            for change in changes
        ]
    }

def detect_anomalies(analysis_result):
    """æ£€æµ‹å¼‚å¸¸æŒ‡æ ‡"""
    anomalies = []
    summary = analysis_result['summary']
    
    # é˜ˆå€¼é…ç½®
    MEDIAN_THRESHOLD = 24  # ä¸­ä½æ•° > 24å°æ—¶
    P95_THRESHOLD = 72     # P95 > 72å°æ—¶
    REWORK_THRESHOLD = 15  # è¿”å·¥ç‡ > 15%
    
    if summary['median_review_time_hours'] > MEDIAN_THRESHOLD:
        anomalies.append({
            'type': 'REVIEW_TIME_HIGH',
            'severity': 'WARNING',
            'metric': 'Reviewä¸­ä½è€—æ—¶',
            'current_value': f"{summary['median_review_time_hours']}å°æ—¶",
            'threshold': f"< {MEDIAN_THRESHOLD}å°æ—¶",
            'message': f"Reviewä¸­ä½è€—æ—¶({summary['median_review_time_hours']}å°æ—¶)è¶…è¿‡é˜ˆå€¼({MEDIAN_THRESHOLD}å°æ—¶)"
        })
    
    if summary['p95_review_time_hours'] > P95_THRESHOLD:
        anomalies.append({
            'type': 'P95_TIME_HIGH',
            'severity': 'WARNING',
            'metric': 'Review P95è€—æ—¶',
            'current_value': f"{summary['p95_review_time_hours']}å°æ—¶",
            'threshold': f"< {P95_THRESHOLD}å°æ—¶",
            'message': f"Review P95è€—æ—¶({summary['p95_review_time_hours']}å°æ—¶)è¶…è¿‡é˜ˆå€¼({P95_THRESHOLD}å°æ—¶)"
        })
    
    if summary['rework_rate_percent'] > REWORK_THRESHOLD:
        anomalies.append({
            'type': 'REWORK_RATE_HIGH',
            'severity': 'WARNING',
            'metric': 'è¿”å·¥ç‡',
            'current_value': f"{summary['rework_rate_percent']}%",
            'threshold': f"< {REWORK_THRESHOLD}%",
            'message': f"è¿”å·¥ç‡({summary['rework_rate_percent']}%)è¶…è¿‡é˜ˆå€¼({REWORK_THRESHOLD}%)"
        })
    
    return anomalies

def main():
    # è¯»å–æ•°æ®
    try:
        with open('data/mock_gerrit_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ data/mock_gerrit_data.json")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSONè§£æå¤±è´¥ - {e}")
        sys.exit(1)
    
    # åˆ†ææ•°æ®
    analysis_result = analyze_gerrit_data(data)
    
    if 'error' in analysis_result:
        print(f"âŒ åˆ†æå¤±è´¥: {analysis_result['error']}")
        sys.exit(1)
    
    # æ£€æµ‹å¼‚å¸¸
    anomalies = detect_anomalies(analysis_result)
    
    # è¾“å‡ºç»“æœ
    result = {
        'analysis': analysis_result,
        'anomalies': anomalies
    }
    
    # ä¿å­˜ç»“æœ
    with open('analysis_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print("âœ… åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° analysis_result.json")
    
    # è¾“å‡ºç®€è¦æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š ä»£ç å®¡æŸ¥æ•ˆç‡åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    summary = analysis_result['summary']
    print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
    print(f"  â€¢ æ€»å˜æ›´æ•°: {summary['total_changes']} ä¸ª")
    print(f"  â€¢ å·²åˆå¹¶: {summary['merged_count']} ä¸ª")
    print(f"  â€¢ Reviewå¹³å‡è€—æ—¶: {summary['avg_review_time_hours']} å°æ—¶")
    print(f"  â€¢ Reviewä¸­ä½è€—æ—¶: {summary['median_review_time_hours']} å°æ—¶")
    print(f"  â€¢ Review P95è€—æ—¶: {summary['p95_review_time_hours']} å°æ—¶")
    print(f"  â€¢ è¿”å·¥ç‡: {summary['rework_rate_percent']}%")
    
    print(f"\nğŸ“¦ é¡¹ç›®ç»´åº¦:")
    for project, stats in analysis_result['project_breakdown'].items():
        print(f"  â€¢ {project}:")
        print(f"    - å˜æ›´æ•°: {stats['change_count']}")
        print(f"    - ä¸­ä½è€—æ—¶: {stats['median_review_time_hours']} å°æ—¶")
        print(f"    - è¿”å·¥ç‡: {stats['rework_rate_percent']}%")
    
    if anomalies:
        print(f"\nğŸš¨ å‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸:")
        for anomaly in anomalies:
            print(f"  â€¢ [{anomaly['severity']}] {anomaly['message']}")
    else:
        print("\nâœ… æœªå‘ç°å¼‚å¸¸ï¼Œå„é¡¹æŒ‡æ ‡æ­£å¸¸")
    
    print("\n" + "="*60)

if __name__ == '__main__':
    main()

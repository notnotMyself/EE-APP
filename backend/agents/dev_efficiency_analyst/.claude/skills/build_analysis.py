#!/usr/bin/env python3
"""
Build Analysis Skill
é—¨ç¦æ„å»ºæ•°æ®åˆ†æèƒ½åŠ› - ä»MySQLæ•°æ®åº“è·å–æ„å»ºæ•°æ®å¹¶åˆ†æ

è®¾è®¡ç†å¿µï¼šé—®é¢˜å¯¼å‘ï¼Œä¸åªæ‘†æŒ‡æ ‡ï¼Œè€Œæ˜¯å‘ˆç°é—®é¢˜å’Œè§£å†³æ€è·¯

åŠŸèƒ½ï¼š
1. é—®é¢˜å‘ç°ï¼šå“ªäº›å¹³å°/ç»„ä»¶/æµç¨‹è½åï¼Œè½åå¤šå°‘ï¼Œè¶‹åŠ¿å¦‚ä½•
2. P95è½ååˆ†æï¼šæ‰¾å‡ºä½äºæ•´ä½“P95çš„å¹³å°ï¼Œé‡åŒ–å·®è·
3. äººå‘˜ç»´åº¦ï¼šä»ä¸ªäººè§’åº¦çœ‹æ„å»ºä¼˜åŒ–ç‚¹
4. ç»„ä»¶åˆ†æï¼šä¸åŒç¼–è¯‘ç»„ä»¶çš„è€—æ—¶å æ¯”å’Œç“¶é¢ˆ
5. è¶‹åŠ¿æ´å¯Ÿï¼šå“ªäº›åœ¨æ¶åŒ–ï¼Œå“ªäº›åœ¨æ”¹å–„ï¼Œæä¾›è§£å†³æ€è·¯

æ³¨æ„ï¼šæ­¤æŠ€èƒ½ä»…æ‰§è¡Œåªè¯»æŸ¥è¯¢ï¼Œä¸ä¼šä¿®æ”¹ä»»ä½•æ•°æ®
"""

import json
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict

# æ•°æ®åº“é…ç½® - é—¨ç¦æ„å»ºæ•°æ®åº“ï¼ˆåªè¯»ï¼‰
BUILD_DB_CONFIG = {
    "dialect": "mysql",
    "host": "rn-test-mysql.mysql.oppo.test",
    "port": 33066,
    "username": "rn_ddl",
    "password": "LXK0Cva89SWDj47x9QbRWJfgETp7JiRP",
    "database": "rn_test"
}

# èŠ¯ç‰‡å¹³å°ä¸­æ–‡åç§°æ˜ å°„ï¼ˆSM=é«˜é€šSnapdragon, MT=è”å‘ç§‘MediaTekï¼‰
PLATFORM_VENDOR_MAP = {
    "SM": "é«˜é€š",
    "MT": "è”å‘ç§‘",
}

# å¸¸è§èŠ¯ç‰‡å‹å·çš„ä¸­æ–‡è¯´æ˜
CHIP_INFO = {
    # é«˜é€šæ——èˆ°
    "SM8750": "é«˜é€šéªé¾™8 Elite (æ——èˆ°)",
    "SM8850": "é«˜é€šéªé¾™8 Elite+ (æ——èˆ°)",
    "SM8650": "é«˜é€šéªé¾™8 Gen3 (æ——èˆ°)",
    "SM8550": "é«˜é€šéªé¾™8 Gen2 (æ——èˆ°)",
    "SM8450": "é«˜é€šéªé¾™8 Gen1 (æ——èˆ°)",
    # é«˜é€šä¸­ç«¯
    "SM7750": "é«˜é€šéªé¾™7+ Gen3 (ä¸­é«˜ç«¯)",
    "SM7675": "é«˜é€šéªé¾™7s Gen3 (ä¸­ç«¯)",
    "SM7550": "é«˜é€šéªé¾™7 Gen3 (ä¸­ç«¯)",
    "SM7475": "é«˜é€šéªé¾™7+ Gen2 (ä¸­ç«¯)",
    "SM7435": "é«˜é€šéªé¾™7s Gen2 (ä¸­ç«¯)",
    "SM7325": "é«˜é€šéªé¾™778G (ä¸­ç«¯)",
    # é«˜é€šå…¥é—¨
    "SM6650": "é«˜é€šéªé¾™6 Gen3 (å…¥é—¨)",
    "SM6450": "é«˜é€šéªé¾™6 Gen1 (å…¥é—¨)",
    "SM6375": "é«˜é€šéªé¾™695 (å…¥é—¨)",
    "SM6225": "é«˜é€šéªé¾™680 (å…¥é—¨)",
    # è”å‘ç§‘æ——èˆ°
    "MT6991": "è”å‘ç§‘å¤©ç‘9400 (æ——èˆ°)",
    "MT6993": "è”å‘ç§‘å¤©ç‘9300+ (æ——èˆ°)",
    "MT6989": "è”å‘ç§‘å¤©ç‘9300 (æ——èˆ°)",
    "MT6985": "è”å‘ç§‘å¤©ç‘9200+ (æ——èˆ°)",
    # è”å‘ç§‘ä¸­ç«¯
    "MT6899": "è”å‘ç§‘å¤©ç‘8400 (ä¸­é«˜ç«¯)",
    "MT6897": "è”å‘ç§‘å¤©ç‘8300 (ä¸­é«˜ç«¯)",
    "MT6896": "è”å‘ç§‘å¤©ç‘8200 (ä¸­ç«¯)",
    "MT6895": "è”å‘ç§‘å¤©ç‘8100 (ä¸­ç«¯)",
    "MT6878": "è”å‘ç§‘å¤©ç‘7300 (ä¸­ç«¯)",
    "MT6877": "è”å‘ç§‘å¤©ç‘1080 (ä¸­ç«¯)",
    # è”å‘ç§‘å…¥é—¨
    "MT6835": "è”å‘ç§‘å¤©ç‘6300 (å…¥é—¨)",
    "MT6789": "è”å‘ç§‘Helio G99 (å…¥é—¨)",
    "MT6769": "è”å‘ç§‘Helio G85 (å…¥é—¨)",
}


def get_platform_display_name(baseline_name: str) -> str:
    """
    è·å–å¹³å°çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰
    
    Args:
        baseline_name: å¹³å°ä»£å·ï¼Œå¦‚ SM8750_16
        
    Returns:
        ä¸­æ–‡æ˜¾ç¤ºåç§°
    """
    if not baseline_name:
        return "æœªçŸ¥å¹³å°"
    
    # æå–èŠ¯ç‰‡å‹å·ï¼ˆå»æ‰Androidç‰ˆæœ¬åç¼€ï¼‰
    parts = baseline_name.split("_")
    chip = parts[0] if parts else baseline_name
    
    # æŸ¥æ‰¾ä¸­æ–‡åç§°
    if chip in CHIP_INFO:
        return f"{baseline_name} ({CHIP_INFO[chip]})"
    
    # è¯†åˆ«å‚å•†
    for prefix, vendor in PLATFORM_VENDOR_MAP.items():
        if chip.startswith(prefix):
            return f"{baseline_name} ({vendor})"
    
    return baseline_name


def get_vendor_name(baseline_name: str) -> str:
    """è·å–èŠ¯ç‰‡å‚å•†åç§°"""
    if not baseline_name:
        return "æœªçŸ¥"
    for prefix, vendor in PLATFORM_VENDOR_MAP.items():
        if baseline_name.startswith(prefix):
            return vendor
    return "å…¶ä»–"


def get_db_connection():
    """
    è·å–MySQLæ•°æ®åº“è¿æ¥ï¼ˆåªè¯»ï¼‰
    
    Returns:
        æ•°æ®åº“è¿æ¥å¯¹è±¡
    """
    try:
        import pymysql
        connection = pymysql.connect(
            host=BUILD_DB_CONFIG["host"],
            port=BUILD_DB_CONFIG["port"],
            user=BUILD_DB_CONFIG["username"],
            password=BUILD_DB_CONFIG["password"],
            database=BUILD_DB_CONFIG["database"],
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor,
            read_timeout=30,
            write_timeout=30
        )
        return connection
    except ImportError:
        raise ImportError("pymysql is required. Install it with: pip install pymysql")
    except Exception as e:
        raise ConnectionError(f"Failed to connect to database: {e}")


def calculate_percentiles(values: List[float], percentiles: List[int] = [50, 75, 90, 95, 99]) -> Dict[str, float]:
    """
    è®¡ç®—åˆ†ä½æ•°
    
    Args:
        values: æ•°å€¼åˆ—è¡¨
        percentiles: è¦è®¡ç®—çš„åˆ†ä½æ•°åˆ—è¡¨
        
    Returns:
        åˆ†ä½æ•°å­—å…¸ï¼Œå¦‚ {"p50": 100, "p95": 200}
    """
    if not values:
        return {f"p{p}": 0 for p in percentiles}
    
    sorted_values = sorted(values)
    n = len(sorted_values)
    result = {}
    
    for p in percentiles:
        index = int(n * p / 100)
        index = min(index, n - 1)
        result[f"p{p}"] = round(sorted_values[index], 2)
    
    result["avg"] = round(sum(values) / n, 2)
    result["min"] = round(min(values), 2)
    result["max"] = round(max(values), 2)
    result["count"] = n
    
    return result


def fetch_build_data(
    days: int = 7,
    baseline_name: Optional[str] = None,
    android_version: Optional[str] = None,
    compile_component: Optional[str] = None,
    created_by: Optional[str] = None,
    limit: int = 10000
) -> List[Dict[str, Any]]:
    """
    ä»æ•°æ®åº“è·å–æ„å»ºæ•°æ®ï¼ˆåªè¯»æŸ¥è¯¢ï¼‰
    
    Args:
        days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        baseline_name: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šå¹³å°ï¼ˆå¦‚ SM8750, MT6991ï¼‰
        android_version: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šAndroidç‰ˆæœ¬
        compile_component: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šç¼–è¯‘ç»„ä»¶
        created_by: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šåˆ›å»ºäºº
        limit: æœ€å¤§è¿”å›è®°å½•æ•°
        
    Returns:
        æ„å»ºæ•°æ®åˆ—è¡¨
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            # æ„å»ºåªè¯»æŸ¥è¯¢SQL
            sql = """
                SELECT 
                    task_num,
                    created_name,
                    baseline_name,
                    android_version,
                    compile_component,
                    CAST(build_time AS SIGNED) as build_time_sec,
                    CAST(download_time AS SIGNED) as download_time_sec,
                    CAST(copy_time AS SIGNED) as copy_time_sec,
                    CAST(ofp_time AS SIGNED) as ofp_time_sec,
                    CAST(pipeline_time AS SIGNED) as pipeline_time_sec,
                    task_create_time,
                    build_trigger_time,
                    build_start_time,
                    build_end_time,
                    created_by
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND build_end_time IS NOT NULL
                AND build_start_time IS NOT NULL
            """
            params = [days]
            
            if baseline_name:
                sql += " AND baseline_name = %s"
                params.append(baseline_name)
            
            if android_version:
                sql += " AND android_version LIKE %s"
                params.append(f"%{android_version}%")
            
            if compile_component:
                sql += " AND compile_component LIKE %s"
                params.append(f"%{compile_component}%")
            
            if created_by:
                sql += " AND created_by = %s"
                params.append(created_by)
            
            sql += " ORDER BY build_end_time DESC LIMIT %s"
            params.append(limit)
            
            cursor.execute(sql, params)
            results = cursor.fetchall()
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            for row in results:
                for key in ['task_create_time', 'build_trigger_time', 'build_start_time', 'build_end_time']:
                    if row.get(key) and isinstance(row[key], datetime):
                        row[key] = row[key].isoformat()
            
            return results
    finally:
        connection.close()


def analyze_build_metrics(builds: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ†ææ„å»ºæŒ‡æ ‡
    
    Args:
        builds: æ„å»ºæ•°æ®åˆ—è¡¨
        
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    if not builds:
        return {
            "error": "No data to analyze",
            "metrics": {}
        }
    
    # æå–å„é˜¶æ®µè€—æ—¶
    build_times = []
    download_times = []
    copy_times = []
    ofp_times = []
    pipeline_times = []
    total_durations = []
    
    for build in builds:
        # å„é˜¶æ®µè€—æ—¶ï¼ˆç§’è½¬åˆ†é’Ÿï¼‰
        if build.get('build_time_sec'):
            build_times.append(build['build_time_sec'] / 60)
        if build.get('download_time_sec'):
            download_times.append(build['download_time_sec'] / 60)
        if build.get('copy_time_sec'):
            copy_times.append(build['copy_time_sec'] / 60)
        if build.get('ofp_time_sec'):
            ofp_times.append(build['ofp_time_sec'] / 60)
        if build.get('pipeline_time_sec'):
            pipeline_times.append(build['pipeline_time_sec'] / 60)
        
        # è®¡ç®—ç«¯åˆ°ç«¯è€—æ—¶
        start = build.get('build_start_time')
        end = build.get('build_end_time')
        if start and end:
            if isinstance(start, str):
                start = datetime.fromisoformat(start)
            if isinstance(end, str):
                end = datetime.fromisoformat(end)
            duration_min = (end - start).total_seconds() / 60
            if duration_min > 0:
                total_durations.append(duration_min)
    
    # è®¡ç®—åˆ†ä½æ•°
    metrics = {
        "total_builds": len(builds),
        "total_duration_minutes": calculate_percentiles(total_durations),
        "build_time_minutes": calculate_percentiles(build_times),
        "download_time_minutes": calculate_percentiles(download_times),
        "copy_time_minutes": calculate_percentiles(copy_times),
        "ofp_time_minutes": calculate_percentiles(ofp_times),
        "pipeline_time_minutes": calculate_percentiles(pipeline_times)
    }
    
    # è®¡ç®—å„é˜¶æ®µå¹³å‡å æ¯”
    if pipeline_times:
        avg_pipeline = sum(pipeline_times) / len(pipeline_times)
        avg_build = sum(build_times) / len(build_times) if build_times else 0
        avg_download = sum(download_times) / len(download_times) if download_times else 0
        avg_copy = sum(copy_times) / len(copy_times) if copy_times else 0
        avg_ofp = sum(ofp_times) / len(ofp_times) if ofp_times else 0
        
        if avg_pipeline > 0:
            metrics["stage_ratio_percent"] = {
                "build": round(avg_build / avg_pipeline * 100, 1),
                "download": round(avg_download / avg_pipeline * 100, 1),
                "copy": round(avg_copy / avg_pipeline * 100, 1),
                "ofp": round(avg_ofp / avg_pipeline * 100, 1)
            }
    
    return metrics


def analyze_by_dimension(
    days: int = 7,
    dimension: str = "baseline_name",
    top_n: int = 10
) -> Dict[str, Any]:
    """
    æŒ‰ç»´åº¦åˆ†ç»„åˆ†ææ„å»ºè€—æ—¶
    
    Args:
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        dimension: åˆ†ç»„ç»´åº¦ (baseline_name, android_version, compile_component)
        top_n: è¿”å›å‰Nä¸ªç»“æœ
        
    Returns:
        åˆ†ç»„åˆ†æç»“æœ
    """
    valid_dimensions = ["baseline_name", "android_version", "compile_component", "created_by"]
    if dimension not in valid_dimensions:
        return {"error": f"Invalid dimension. Must be one of: {valid_dimensions}"}
    
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            sql = f"""
                SELECT 
                    {dimension} as dimension_value,
                    COUNT(*) as build_count,
                    AVG(CAST(pipeline_time AS SIGNED)) / 60 as avg_pipeline_min,
                    AVG(CAST(build_time AS SIGNED)) / 60 as avg_build_min
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND build_end_time IS NOT NULL
                AND {dimension} IS NOT NULL
                AND {dimension} != ''
                GROUP BY {dimension}
                ORDER BY avg_pipeline_min DESC
                LIMIT %s
            """
            cursor.execute(sql, [days, top_n])
            results = cursor.fetchall()
            
            return {
                "dimension": dimension,
                "days": days,
                "results": [
                    {
                        "name": row["dimension_value"],
                        "build_count": row["build_count"],
                        "avg_pipeline_minutes": round(float(row["avg_pipeline_min"] or 0), 1),
                        "avg_build_minutes": round(float(row["avg_build_min"] or 0), 1)
                    }
                    for row in results
                ]
            }
    finally:
        connection.close()


def analyze_percentile_by_platform(
    days: int = 7,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    æŒ‰å¹³å°åˆ†ææ„å»ºè€—æ—¶åˆ†ä½æ•°ï¼ˆP50/P95/P99ï¼‰
    
    Args:
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        top_n: è¿”å›å‰Nä¸ªå¹³å°
        
    Returns:
        å„å¹³å°çš„åˆ†ä½æ•°åˆ†æç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            # å…ˆè·å–æ„å»ºé‡æœ€å¤šçš„å¹³å°
            sql = """
                SELECT baseline_name, COUNT(*) as cnt
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND baseline_name IS NOT NULL AND baseline_name != ''
                GROUP BY baseline_name
                ORDER BY cnt DESC
                LIMIT %s
            """
            cursor.execute(sql, [days, top_n])
            platforms = [row["baseline_name"] for row in cursor.fetchall()]
            
            results = []
            for platform in platforms:
                sql = """
                    SELECT CAST(pipeline_time AS SIGNED) / 60 as duration_min
                    FROM personal_build
                    WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                    AND baseline_name = %s
                    AND pipeline_time IS NOT NULL AND pipeline_time != ''
                    ORDER BY CAST(pipeline_time AS SIGNED)
                """
                cursor.execute(sql, [days, platform])
                durations = [row["duration_min"] for row in cursor.fetchall() if row["duration_min"]]
                
                if durations:
                    percentiles = calculate_percentiles(durations)
                    results.append({
                        "platform": platform,
                        **percentiles
                    })
            
            return {
                "days": days,
                "platforms": results
            }
    finally:
        connection.close()


def analyze_trend(
    days: int = 30,
    granularity: str = "day",
    baseline_name: Optional[str] = None
) -> Dict[str, Any]:
    """
    åˆ†ææ„å»ºè€—æ—¶è¶‹åŠ¿
    
    Args:
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        granularity: ç²’åº¦ (day, week)
        baseline_name: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šå¹³å°
        
    Returns:
        è¶‹åŠ¿åˆ†æç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            if granularity == "week":
                date_format = "%Y-%u"
                date_label = "YEARWEEK(build_end_time)"
            else:
                date_format = "%Y-%m-%d"
                date_label = "DATE(build_end_time)"
            
            sql = f"""
                SELECT 
                    {date_label} as period,
                    COUNT(*) as build_count,
                    AVG(CAST(pipeline_time AS SIGNED)) / 60 as avg_pipeline_min,
                    AVG(CAST(build_time AS SIGNED)) / 60 as avg_build_min
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND build_end_time IS NOT NULL
            """
            params = [days]
            
            if baseline_name:
                sql += " AND baseline_name = %s"
                params.append(baseline_name)
            
            sql += f" GROUP BY {date_label} ORDER BY period"
            
            cursor.execute(sql, params)
            results = cursor.fetchall()
            
            trend_data = []
            prev_avg = None
            for row in results:
                current_avg = float(row["avg_pipeline_min"] or 0)
                change_percent = None
                if prev_avg and prev_avg > 0:
                    change_percent = round((current_avg - prev_avg) / prev_avg * 100, 1)
                
                trend_data.append({
                    "period": str(row["period"]),
                    "build_count": row["build_count"],
                    "avg_pipeline_minutes": round(current_avg, 1),
                    "avg_build_minutes": round(float(row["avg_build_min"] or 0), 1),
                    "change_percent": change_percent
                })
                prev_avg = current_avg
            
            # åˆ¤æ–­æ•´ä½“è¶‹åŠ¿
            if len(trend_data) >= 2:
                first_half = trend_data[:len(trend_data)//2]
                second_half = trend_data[len(trend_data)//2:]
                first_avg = sum(t["avg_pipeline_minutes"] for t in first_half) / len(first_half)
                second_avg = sum(t["avg_pipeline_minutes"] for t in second_half) / len(second_half)
                
                if second_avg > first_avg * 1.1:
                    overall_trend = "worsening"
                elif second_avg < first_avg * 0.9:
                    overall_trend = "improving"
                else:
                    overall_trend = "stable"
            else:
                overall_trend = "insufficient_data"
            
            return {
                "days": days,
                "granularity": granularity,
                "baseline_name": baseline_name,
                "overall_trend": overall_trend,
                "data": trend_data
            }
    finally:
        connection.close()


def detect_anomalies(
    days: int = 7,
    p95_threshold_minutes: float = None
) -> Dict[str, Any]:
    """
    æ£€æµ‹å¼‚å¸¸æ„å»º
    
    Args:
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        p95_threshold_minutes: P95é˜ˆå€¼ï¼ˆåˆ†é’Ÿï¼‰ï¼Œè¶…è¿‡æ­¤å€¼çš„æ„å»ºè¢«æ ‡è®°ä¸ºå¼‚å¸¸
        
    Returns:
        å¼‚å¸¸æ£€æµ‹ç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            # å…ˆè®¡ç®—P95é˜ˆå€¼
            if p95_threshold_minutes is None:
                sql = """
                    SELECT CAST(pipeline_time AS SIGNED) / 60 as duration_min
                    FROM personal_build
                    WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                    AND pipeline_time IS NOT NULL AND pipeline_time != ''
                    ORDER BY CAST(pipeline_time AS SIGNED)
                """
                cursor.execute(sql, [days])
                durations = [row["duration_min"] for row in cursor.fetchall() if row["duration_min"]]
                
                if durations:
                    n = len(durations)
                    p95_threshold_minutes = durations[int(n * 0.95)]
                else:
                    p95_threshold_minutes = 120  # é»˜è®¤2å°æ—¶
            
            # æŸ¥æ‰¾è¶…è¿‡P95çš„æ„å»º
            sql = """
                SELECT 
                    task_num,
                    baseline_name,
                    android_version,
                    compile_component,
                    CAST(pipeline_time AS SIGNED) / 60 as pipeline_minutes,
                    CAST(build_time AS SIGNED) / 60 as build_minutes,
                    build_end_time,
                    created_by
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND CAST(pipeline_time AS SIGNED) / 60 > %s
                ORDER BY CAST(pipeline_time AS SIGNED) DESC
                LIMIT 20
            """
            cursor.execute(sql, [days, p95_threshold_minutes])
            slow_builds = cursor.fetchall()
            
            # æ£€æµ‹æ¶åŒ–çš„å¹³å°
            sql = """
                SELECT 
                    baseline_name,
                    AVG(CASE WHEN build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY) 
                        THEN CAST(pipeline_time AS SIGNED) / 60 END) as recent_avg,
                    AVG(CASE WHEN build_end_time < DATE_SUB(NOW(), INTERVAL %s DAY) 
                             AND build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                        THEN CAST(pipeline_time AS SIGNED) / 60 END) as prev_avg
                FROM personal_build
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND baseline_name IS NOT NULL AND baseline_name != ''
                GROUP BY baseline_name
                HAVING recent_avg IS NOT NULL AND prev_avg IS NOT NULL
            """
            half_days = days // 2
            cursor.execute(sql, [half_days, half_days, days, days])
            platform_changes = cursor.fetchall()
            
            worsening_platforms = []
            for p in platform_changes:
                recent = float(p["recent_avg"] or 0)
                prev = float(p["prev_avg"] or 0)
                if prev > 0 and recent > prev * 1.2:  # æ¶åŒ–è¶…è¿‡20%
                    worsening_platforms.append({
                        "platform": p["baseline_name"],
                        "recent_avg_minutes": round(recent, 1),
                        "prev_avg_minutes": round(prev, 1),
                        "change_percent": round((recent - prev) / prev * 100, 1)
                    })
            
            anomalies = []
            
            if slow_builds:
                anomalies.append({
                    "type": "slow_builds",
                    "severity": "warning",
                    "message": f"å‘ç° {len(slow_builds)} ä¸ªæ„å»ºè¶…è¿‡ P95 é˜ˆå€¼ï¼ˆ{p95_threshold_minutes:.0f}åˆ†é’Ÿï¼‰",
                    "details": [
                        {
                            "task_num": b["task_num"],
                            "platform": b["baseline_name"],
                            "duration_minutes": round(float(b["pipeline_minutes"] or 0), 1)
                        }
                        for b in slow_builds[:5]  # åªè¿”å›å‰5ä¸ª
                    ]
                })
            
            if worsening_platforms:
                anomalies.append({
                    "type": "worsening_platforms",
                    "severity": "critical",
                    "message": f"å‘ç° {len(worsening_platforms)} ä¸ªå¹³å°æ„å»ºè€—æ—¶æ¶åŒ–è¶…è¿‡20%",
                    "details": worsening_platforms
                })
            
            return {
                "days": days,
                "p95_threshold_minutes": round(p95_threshold_minutes, 1),
                "anomalies": anomalies,
                "slow_builds_count": len(slow_builds),
                "worsening_platforms_count": len(worsening_platforms)
            }
    finally:
        connection.close()


# ============ é—®é¢˜å¯¼å‘åˆ†æå‡½æ•° ============


def analyze_lagging_platforms(days: int = 7, min_builds: int = 50) -> Dict[str, Any]:
    """
    åˆ†æP95è½åçš„å¹³å° - æ‰¾å‡ºå“ªäº›å¹³å°ä½äºæ•´ä½“æ°´å¹³
    
    é—®é¢˜å¯¼å‘ï¼šä¸åªçœ‹æŒ‡æ ‡ï¼Œè€Œæ˜¯æ‰¾å‡ºé—®é¢˜æ‰€åœ¨
    
    Args:
        days: åˆ†æå¤©æ•°
        min_builds: æœ€å°æ„å»ºæ•°ï¼ˆé¿å…æ ·æœ¬å¤ªå°ï¼‰
        
    Returns:
        è½åå¹³å°åˆ†æç»“æœï¼ŒåŒ…å«é—®é¢˜æè¿°å’Œè§£å†³å»ºè®®
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            # è®¡ç®—æ•´ä½“P50å’ŒP95
            cursor.execute('''
                SELECT CAST(pipeline_time AS SIGNED)/60 as duration_min
                FROM personal_build 
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND pipeline_time IS NOT NULL AND pipeline_time != ''
                ORDER BY CAST(pipeline_time AS SIGNED)
            ''', [days])
            all_durations = [row["duration_min"] for row in cursor.fetchall() if row["duration_min"]]
            
            if not all_durations:
                return {"error": "No data available"}
            
            n = len(all_durations)
            overall_p50 = all_durations[n // 2]
            overall_p95 = all_durations[int(n * 0.95)]
            
            # è·å–å„å¹³å°æ•°æ®
            cursor.execute('''
                SELECT baseline_name, COUNT(*) as cnt
                FROM personal_build 
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND baseline_name IS NOT NULL AND baseline_name != ''
                GROUP BY baseline_name
                HAVING cnt >= %s
                ORDER BY cnt DESC
            ''', [days, min_builds])
            platforms = [(row["baseline_name"], row["cnt"]) for row in cursor.fetchall()]
            
            lagging_platforms = []
            healthy_platforms = []
            
            for platform, cnt in platforms:
                cursor.execute('''
                    SELECT CAST(pipeline_time AS SIGNED)/60 as duration_min
                    FROM personal_build 
                    WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                    AND baseline_name = %s
                    AND pipeline_time IS NOT NULL AND pipeline_time != ''
                    ORDER BY CAST(pipeline_time AS SIGNED)
                ''', [days, platform])
                durations = [row["duration_min"] for row in cursor.fetchall() if row["duration_min"]]
                
                if durations:
                    pn = len(durations)
                    p50 = durations[pn // 2]
                    p95 = durations[int(pn * 0.95)]
                    
                    platform_info = {
                        "platform": platform,
                        "display_name": get_platform_display_name(platform),
                        "vendor": get_vendor_name(platform),
                        "build_count": cnt,
                        "p50_minutes": round(p50, 1),
                        "p95_minutes": round(p95, 1),
                    }
                    
                    if p95 > overall_p95:
                        gap_percent = round((p95 - overall_p95) / overall_p95 * 100, 1)
                        platform_info["gap_percent"] = gap_percent
                        platform_info["gap_minutes"] = round(p95 - overall_p95, 1)
                        platform_info["status"] = "lagging"
                        lagging_platforms.append(platform_info)
                    else:
                        platform_info["status"] = "healthy"
                        healthy_platforms.append(platform_info)
            
            # æŒ‰å·®è·æ’åº
            lagging_platforms.sort(key=lambda x: x.get("gap_percent", 0), reverse=True)
            
            # ç”Ÿæˆé—®é¢˜æè¿°å’Œå»ºè®®
            problems = []
            suggestions = []
            
            if lagging_platforms:
                worst = lagging_platforms[0]
                problems.append(f"{worst['display_name']} çš„P95è€—æ—¶é«˜äºæ•´ä½“æ°´å¹³ {worst['gap_percent']}%ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨")
                
                # ç»Ÿè®¡å‚å•†æƒ…å†µ
                vendor_lag = {}
                for p in lagging_platforms:
                    v = p["vendor"]
                    if v not in vendor_lag:
                        vendor_lag[v] = 0
                    vendor_lag[v] += 1
                
                for v, count in vendor_lag.items():
                    if count > 2:
                        problems.append(f"{v}å¹³å°æœ‰ {count} ä¸ªå‹å·è½åï¼Œå¯èƒ½æ˜¯è¯¥å‚å•†çš„æ„å»ºæµç¨‹éœ€è¦ä¼˜åŒ–")
                
                suggestions.append("æ’æŸ¥è½åå¹³å°çš„æ„å»ºæ—¥å¿—ï¼Œåˆ†æè€—æ—¶ä¸»è¦é›†ä¸­åœ¨å“ªä¸ªé˜¶æ®µ")
                suggestions.append("å¯¹æ¯”å¥åº·å¹³å°å’Œè½åå¹³å°çš„æ„å»ºé…ç½®å·®å¼‚")
                suggestions.append("è€ƒè™‘ä¸ºè½åå¹³å°åˆ†é…æ›´å¤šæ„å»ºèµ„æºæˆ–ä¼˜åŒ–ç¼–è¯‘å‚æ•°")
            
            return {
                "days": days,
                "overall_p50_minutes": round(overall_p50, 1),
                "overall_p95_minutes": round(overall_p95, 1),
                "total_platforms": len(platforms),
                "lagging_count": len(lagging_platforms),
                "healthy_count": len(healthy_platforms),
                "lagging_platforms": lagging_platforms[:15],  # æœ€å¤šè¿”å›15ä¸ª
                "healthy_platforms": healthy_platforms[:5],   # æœ€å¤šè¿”å›5ä¸ªå¥åº·çš„ä½œä¸ºå¯¹æ¯”
                "problems": problems,
                "suggestions": suggestions
            }
    finally:
        connection.close()


def analyze_component_bottlenecks(days: int = 7) -> Dict[str, Any]:
    """
    åˆ†æç»„ä»¶æ„å»ºç“¶é¢ˆ - å“ªäº›ç¼–è¯‘ç»„ä»¶æœ€æ…¢
    
    Args:
        days: åˆ†æå¤©æ•°
        
    Returns:
        ç»„ä»¶ç“¶é¢ˆåˆ†æç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            cursor.execute('''
                SELECT compile_component, COUNT(*) as cnt
                FROM personal_build 
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND compile_component IS NOT NULL AND compile_component != ''
                GROUP BY compile_component
                ORDER BY cnt DESC
                LIMIT 20
            ''', [days])
            components = [(row["compile_component"], row["cnt"]) for row in cursor.fetchall()]
            
            results = []
            for comp, cnt in components:
                cursor.execute('''
                    SELECT CAST(pipeline_time AS SIGNED)/60 as duration_min
                    FROM personal_build 
                    WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                    AND compile_component = %s
                    AND pipeline_time IS NOT NULL AND pipeline_time != ''
                    ORDER BY CAST(pipeline_time AS SIGNED)
                ''', [days, comp])
                durations = [row["duration_min"] for row in cursor.fetchall() if row["duration_min"]]
                
                if durations:
                    n = len(durations)
                    p50 = float(durations[n // 2])
                    p95 = float(durations[int(n * 0.95)])
                    
                    # åˆ†æç»„ä»¶å¤æ‚åº¦ï¼ˆåŒ…å«å¤šå°‘ä¸ªå­ç»„ä»¶ï¼‰
                    sub_components = comp.split(";") if ";" in comp else [comp]
                    
                    results.append({
                        "component": comp,
                        "sub_component_count": len(sub_components),
                        "build_count": cnt,
                        "p50_minutes": round(p50, 1),
                        "p95_minutes": round(p95, 1),
                        "is_complex": len(sub_components) >= 3
                    })
            
            # æŒ‰P95æ’åº
            results.sort(key=lambda x: x["p95_minutes"], reverse=True)
            
            # ç”Ÿæˆæ´å¯Ÿ
            insights = []
            if results:
                slowest = results[0]
                insights.append(f"æœ€æ…¢ç»„ä»¶: {slowest['component']}ï¼ŒP95è€—æ—¶ {slowest['p95_minutes']} åˆ†é’Ÿ")
                
                # å¤æ‚ç»„ä»¶åˆ†æ
                complex_comps = [r for r in results if r["is_complex"]]
                if complex_comps:
                    avg_complex = sum(r["p95_minutes"] for r in complex_comps) / len(complex_comps)
                    simple_comps = [r for r in results if not r["is_complex"]]
                    if simple_comps:
                        avg_simple = sum(r["p95_minutes"] for r in simple_comps) / len(simple_comps)
                        if avg_complex > avg_simple * 1.5:
                            insights.append(f"å¤šç»„ä»¶æ„å»ºå¹³å‡è€—æ—¶ {avg_complex:.0f} åˆ†é’Ÿï¼Œæ˜¯å•ç»„ä»¶çš„ {avg_complex/avg_simple:.1f} å€")
            
            return {
                "days": days,
                "total_components": len(results),
                "components": results,
                "insights": insights,
                "suggestions": [
                    "è€ƒè™‘å°†å¤æ‚çš„å¤šç»„ä»¶æ„å»ºæ‹†åˆ†ä¸ºç‹¬ç«‹æ„å»ºä»»åŠ¡",
                    "å¯¹P95æœ€é«˜çš„ç»„ä»¶è¿›è¡Œç¼–è¯‘ä¼˜åŒ–åˆ†æ",
                    "è¯„ä¼°æ˜¯å¦å¯ä»¥ä½¿ç”¨å¢é‡ç¼–è¯‘å‡å°‘é‡å¤å·¥ä½œ"
                ]
            }
    finally:
        connection.close()


def analyze_user_builds(days: int = 7, min_builds: int = 10) -> Dict[str, Any]:
    """
    åˆ†æäººå‘˜ç»´åº¦çš„æ„å»ºæƒ…å†µ - ä»ä¸ªäººè§’åº¦çœ‹ä¼˜åŒ–ç‚¹
    
    Args:
        days: åˆ†æå¤©æ•°
        min_builds: æœ€å°æ„å»ºæ•°
        
    Returns:
        äººå‘˜æ„å»ºåˆ†æç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            # æŒ‰äººå‘˜ç»Ÿè®¡
            cursor.execute('''
                SELECT 
                    created_by,
                    COUNT(*) as cnt, 
                    AVG(CAST(pipeline_time AS SIGNED))/60 as avg_min,
                    SUM(CAST(pipeline_time AS SIGNED))/60 as total_min
                FROM personal_build 
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND created_by IS NOT NULL AND created_by != ''
                GROUP BY created_by
                HAVING cnt >= %s
                ORDER BY avg_min DESC
            ''', [days, min_builds])
            
            users = []
            for row in cursor.fetchall():
                users.append({
                    "user_id": row["created_by"],
                    "build_count": row["cnt"],
                    "avg_minutes": round(float(row["avg_min"] or 0), 1),
                    "total_hours": round(float(row["total_min"] or 0) / 60, 1)
                })
            
            # ç»Ÿè®¡
            if users:
                avg_all = sum(u["avg_minutes"] for u in users) / len(users)
                slow_users = [u for u in users if u["avg_minutes"] > avg_all * 1.3]  # æ…¢30%ä»¥ä¸Š
                
                # æŒ‰æ€»è€—æ—¶æ’åºæ‰¾å‡ºæ„å»ºæ—¶é—´æ¶ˆè€—æœ€å¤šçš„
                users_by_total = sorted(users, key=lambda x: x["total_hours"], reverse=True)
                
                insights = []
                if slow_users:
                    insights.append(f"æœ‰ {len(slow_users)} ä½ç”¨æˆ·çš„å¹³å‡æ„å»ºè€—æ—¶é«˜äºæ•´ä½“30%ä»¥ä¸Š")
                
                if users_by_total:
                    top_user = users_by_total[0]
                    insights.append(f"æ„å»ºæ—¶é—´æ¶ˆè€—æœ€å¤š: {top_user['user_id']}ï¼Œå…± {top_user['total_hours']} å°æ—¶")
                
                return {
                    "days": days,
                    "total_users": len(users),
                    "avg_build_time_minutes": round(avg_all, 1),
                    "slowest_users": users[:10],  # å¹³å‡æœ€æ…¢çš„10äºº
                    "most_active_users": users_by_total[:10],  # æ„å»ºæœ€å¤šçš„10äºº
                    "users_need_attention": len(slow_users),
                    "insights": insights,
                    "suggestions": [
                        "å»ºè®®æ„å»ºè€—æ—¶è¾ƒé«˜çš„ç”¨æˆ·æ£€æŸ¥æäº¤çš„ä»£ç å˜æ›´èŒƒå›´æ˜¯å¦è¿‡å¤§",
                        "è€ƒè™‘æ˜¯å¦å¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„ç¼–è¯‘ç»„ä»¶ç»„åˆ",
                        "è¯„ä¼°æ˜¯å¦éœ€è¦ä¸ºé«˜é¢‘æ„å»ºç”¨æˆ·åˆ†é…ä¼˜å…ˆèµ„æº"
                    ]
                }
            
            return {
                "days": days,
                "total_users": 0,
                "insights": ["æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œäººå‘˜åˆ†æ"],
                "suggestions": []
            }
    finally:
        connection.close()


def analyze_trend_changes(days: int = 7) -> Dict[str, Any]:
    """
    åˆ†æè¶‹åŠ¿å˜åŒ– - å“ªäº›å¹³å°åœ¨æ¶åŒ–ï¼Œå“ªäº›åœ¨æ”¹å–„
    
    Args:
        days: åˆ†æå¤©æ•°
        
    Returns:
        è¶‹åŠ¿å˜åŒ–åˆ†æç»“æœ
    """
    connection = get_db_connection()
    try:
        with connection.cursor() as cursor:
            half = days // 2
            
            cursor.execute('''
                SELECT 
                    baseline_name,
                    AVG(CASE WHEN build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY) 
                        THEN CAST(pipeline_time AS SIGNED)/60 END) as recent_avg,
                    AVG(CASE WHEN build_end_time < DATE_SUB(NOW(), INTERVAL %s DAY) 
                        THEN CAST(pipeline_time AS SIGNED)/60 END) as prev_avg,
                    COUNT(CASE WHEN build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY) THEN 1 END) as recent_cnt,
                    COUNT(CASE WHEN build_end_time < DATE_SUB(NOW(), INTERVAL %s DAY) THEN 1 END) as prev_cnt
                FROM personal_build 
                WHERE build_end_time >= DATE_SUB(NOW(), INTERVAL %s DAY)
                AND baseline_name IS NOT NULL AND baseline_name != ''
                GROUP BY baseline_name
                HAVING recent_cnt >= 30 AND prev_cnt >= 30
            ''', [half, half, half, half, days])
            
            worsening = []
            improving = []
            stable = []
            
            for row in cursor.fetchall():
                platform = row["baseline_name"]
                recent = float(row["recent_avg"] or 0)
                prev = float(row["prev_avg"] or 0)
                
                if prev > 0:
                    change = (recent - prev) / prev * 100
                    info = {
                        "platform": platform,
                        "display_name": get_platform_display_name(platform),
                        "recent_avg_minutes": round(recent, 1),
                        "prev_avg_minutes": round(prev, 1),
                        "change_percent": round(change, 1),
                        "recent_count": row["recent_cnt"],
                        "prev_count": row["prev_cnt"]
                    }
                    
                    if change > 10:
                        info["status"] = "worsening"
                        worsening.append(info)
                    elif change < -10:
                        info["status"] = "improving"
                        improving.append(info)
                    else:
                        info["status"] = "stable"
                        stable.append(info)
            
            worsening.sort(key=lambda x: x["change_percent"], reverse=True)
            improving.sort(key=lambda x: x["change_percent"])
            
            # ç”Ÿæˆæ´å¯Ÿ
            problems = []
            good_news = []
            
            if worsening:
                worst = worsening[0]
                problems.append(f"âš ï¸ {worst['display_name']} è€—æ—¶æ¶åŒ–æœ€ä¸¥é‡ï¼Œå¢åŠ äº† {worst['change_percent']}%")
            
            if improving:
                best = improving[0]
                good_news.append(f"âœ… {best['display_name']} ä¼˜åŒ–æ•ˆæœæœ€æ˜æ˜¾ï¼Œå‡å°‘äº† {abs(best['change_percent'])}%")
            
            return {
                "days": days,
                "comparison_period": f"æœ€è¿‘{half}å¤© vs ä¹‹å‰{half}å¤©",
                "worsening_count": len(worsening),
                "improving_count": len(improving),
                "stable_count": len(stable),
                "worsening_platforms": worsening[:10],
                "improving_platforms": improving[:10],
                "problems": problems,
                "good_news": good_news,
                "suggestions": [
                    "å¯¹æ¶åŒ–å¹³å°è¿›è¡Œæ ¹å› åˆ†æï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å˜æ›´æˆ–é…ç½®è°ƒæ•´",
                    "å­¦ä¹ æ”¹å–„å¹³å°çš„ä¼˜åŒ–ç»éªŒï¼Œæ¨å¹¿åˆ°å…¶ä»–å¹³å°",
                    "è®¾ç½®è¶‹åŠ¿ç›‘æ§å‘Šè­¦ï¼ŒåŠæ—¶å‘ç°æ¶åŒ–è¶‹åŠ¿"
                ] if worsening else ["å½“å‰æ‰€æœ‰å¹³å°è¶‹åŠ¿ç¨³å®šï¼Œç»§ç»­ä¿æŒ"]
            }
    finally:
        connection.close()


def generate_problem_report(days: int = 7) -> Dict[str, Any]:
    """
    ç”Ÿæˆé—®é¢˜å¯¼å‘çš„åˆ†ææŠ¥å‘Š - é‡ç‚¹å‘ˆç°é—®é¢˜å’Œè§£å†³æ€è·¯
    
    Args:
        days: åˆ†æå¤©æ•°
        
    Returns:
        é—®é¢˜å¯¼å‘çš„å®Œæ•´åˆ†ææŠ¥å‘Š
    """
    try:
        # æ”¶é›†å„ç»´åº¦åˆ†æ
        lagging = analyze_lagging_platforms(days=days)
        components = analyze_component_bottlenecks(days=days)
        trends = analyze_trend_changes(days=days)
        users = analyze_user_builds(days=days)
        
        # æ±‡æ€»æ‰€æœ‰é—®é¢˜
        all_problems = []
        all_suggestions = []
        
        # ä»å„åˆ†æä¸­æå–é—®é¢˜
        if lagging.get("problems"):
            all_problems.extend(lagging["problems"])
        if lagging.get("suggestions"):
            all_suggestions.extend(lagging["suggestions"])
            
        if trends.get("problems"):
            all_problems.extend(trends["problems"])
        if trends.get("suggestions"):
            all_suggestions.extend(trends["suggestions"])
            
        if components.get("insights"):
            all_problems.extend([f"ğŸ“Š {i}" for i in components["insights"]])
        if components.get("suggestions"):
            all_suggestions.extend(components["suggestions"])
            
        if users.get("insights"):
            all_problems.extend([f"ğŸ‘¤ {i}" for i in users["insights"]])
        if users.get("suggestions"):
            all_suggestions.extend(users["suggestions"])
        
        # å»é‡å»ºè®®
        all_suggestions = list(dict.fromkeys(all_suggestions))
        
        # è®¡ç®—é—®é¢˜ä¸¥é‡ç¨‹åº¦
        severity = "low"
        if lagging.get("lagging_count", 0) > 5 or trends.get("worsening_count", 0) > 3:
            severity = "high"
        elif lagging.get("lagging_count", 0) > 2 or trends.get("worsening_count", 0) > 1:
            severity = "medium"
        
        return {
            "report_type": "problem_analysis",
            "generated_at": datetime.now().isoformat(),
            "analysis_period_days": days,
            "severity": severity,
            "summary": {
                "total_problems": len(all_problems),
                "lagging_platforms": lagging.get("lagging_count", 0),
                "worsening_platforms": trends.get("worsening_count", 0),
                "improving_platforms": trends.get("improving_count", 0),
                "overall_p95_minutes": lagging.get("overall_p95_minutes", 0)
            },
            "problems": all_problems,
            "suggestions": all_suggestions[:10],  # æœ€å¤š10æ¡å»ºè®®
            "details": {
                "lagging_analysis": lagging,
                "component_analysis": components,
                "trend_analysis": trends,
                "user_analysis": users
            }
        }
    except Exception as e:
        return {
            "error": str(e),
            "report_type": "problem_analysis",
            "generated_at": datetime.now().isoformat()
        }


def generate_briefing(days: int = 7) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ„å»ºæ•ˆç‡ç®€æŠ¥ - ç”¨äºä¿¡æ¯æµæ¨é€
    
    ç®€æŠ¥è®¾è®¡åŸåˆ™ï¼ˆå‚è€ƒ CLAUDE.md ç®€æŠ¥ç”ŸæˆæŒ‡å—ï¼‰:
    1. æ ‡é¢˜åŠ¨è¯å¼€å¤´ï¼Œè¯´æ¸…æ ¸å¿ƒå‘ç°
    2. æ‘˜è¦åŒ…å«ä¸‰è¦ç´ ï¼šé—®é¢˜ + å½±å“ + è¡ŒåŠ¨
    3. åªåœ¨çœŸæ­£æœ‰ä»·å€¼æ—¶æ‰æ¨é€
    
    Args:
        days: åˆ†æå¤©æ•°
        
    Returns:
        ç®€æŠ¥æ•°æ®ï¼ŒåŒ…å« should_push å­—æ®µåˆ¤æ–­æ˜¯å¦å€¼å¾—æ¨é€
    """
    try:
        # è·å–é—®é¢˜åˆ†ææ•°æ®
        data = generate_problem_report(days=days)
        
        if data.get("error"):
            return data
        
        severity = data.get("severity", "low")
        summary = data.get("summary", {})
        problems = data.get("problems", [])
        suggestions = data.get("suggestions", [])
        details = data.get("details", {})
        
        # åˆ¤æ–­æ˜¯å¦å€¼å¾—æ¨é€
        should_push = False
        priority = "P2"  # é»˜è®¤æ™®é€š
        
        lagging_count = summary.get("lagging_platforms", 0)
        worsening_count = summary.get("worsening_platforms", 0)
        
        if severity == "high":
            should_push = True
            priority = "P1"  # é‡è¦
        elif severity == "medium":
            should_push = True
            priority = "P2"  # æ™®é€š
        elif worsening_count > 0:
            should_push = True
            priority = "P2"
        # å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œä¸æ¨é€
        
        # ç”Ÿæˆæ ‡é¢˜ï¼ˆåŠ¨è¯å¼€å¤´ï¼Œè¯´æ¸…æ ¸å¿ƒå‘ç°ï¼‰
        if severity == "high":
            if lagging_count > 5:
                title = f"é—¨ç¦æ„å»ºæ•ˆç‡å‘Šè­¦ï¼š{lagging_count}ä¸ªå¹³å°P95è½åï¼Œéœ€é‡ç‚¹å…³æ³¨"
            else:
                title = f"æ„å»ºæ•ˆç‡å¼‚å¸¸ï¼šå‘ç°{len(problems)}ä¸ªé—®é¢˜éœ€è¦å¤„ç†"
        elif worsening_count > 0:
            # æ‰¾å‡ºæ¶åŒ–æœ€ä¸¥é‡çš„å¹³å°
            worsening = details.get("trend_analysis", {}).get("worsening_platforms", [])
            if worsening:
                worst = worsening[0]
                title = f"{worst.get('display_name', worst.get('platform', ''))}æ„å»ºè€—æ—¶æ¶åŒ–{worst.get('change_percent', 0)}%"
            else:
                title = f"å‘ç°{worsening_count}ä¸ªå¹³å°æ„å»ºè¶‹åŠ¿æ¶åŒ–"
        elif lagging_count > 0:
            # æ‰¾å‡ºè½åæœ€ä¸¥é‡çš„å¹³å°
            lagging = details.get("lagging_analysis", {}).get("lagging_platforms", [])
            if lagging:
                worst = lagging[0]
                title = f"{worst.get('display_name', worst.get('platform', ''))} P95è¶…å‡ºæ•´ä½“{worst.get('gap_percent', 0)}%"
            else:
                title = f"{lagging_count}ä¸ªå¹³å°P95è½åäºæ•´ä½“æ°´å¹³"
        else:
            title = "æ„å»ºæ•ˆç‡ä¿æŒç¨³å®šï¼Œæ— æ˜¾è‘—é—®é¢˜"
            should_push = False  # æ­£å¸¸æƒ…å†µä¸æ¨é€
        
        # ç”Ÿæˆæ‘˜è¦ï¼ˆé—®é¢˜ + å½±å“ + è¡ŒåŠ¨ï¼‰
        summary_lines = []
        
        # é—®é¢˜
        if problems:
            summary_lines.append(problems[0])  # æœ€é‡è¦çš„é—®é¢˜
        
        # å½±å“
        overall_p95 = summary.get("overall_p95_minutes", 0)
        if lagging_count > 0:
            summary_lines.append(f"æ•´ä½“P95ä¸º{overall_p95}åˆ†é’Ÿï¼Œ{lagging_count}ä¸ªå¹³å°é«˜äºæ­¤åŸºå‡†ã€‚")
        
        # è¡ŒåŠ¨
        if suggestions:
            summary_lines.append(f"å»ºè®®ï¼š{suggestions[0]}")
        
        summary_text = "\n".join(summary_lines)
        
        # æ ¸å¿ƒæŒ‡æ ‡
        metrics = {
            "lagging_platforms": lagging_count,
            "worsening_platforms": worsening_count,
            "improving_platforms": summary.get("improving_platforms", 0),
            "overall_p95_minutes": overall_p95,
            "total_problems": len(problems)
        }
        
        # å…³é”®é—®é¢˜ï¼ˆæœ€å¤š3ä¸ªï¼‰
        key_problems = problems[:3] if problems else []
        
        # å»ºè®®è¡ŒåŠ¨ï¼ˆæœ€å¤š2ä¸ªï¼‰
        key_suggestions = suggestions[:2] if suggestions else []
        
        return {
            "briefing_type": "build_efficiency",
            "generated_at": datetime.now().isoformat(),
            "analysis_period_days": days,
            "should_push": should_push,
            "priority": priority,
            "severity": severity,
            "title": title,
            "summary": summary_text,
            "metrics": metrics,
            "key_problems": key_problems,
            "key_suggestions": key_suggestions,
            # å¦‚æœéœ€è¦è¯¦ç»†æ•°æ®ï¼Œå¯ä»¥ä»è¿™é‡Œè·å–
            "details_available": True
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "briefing_type": "build_efficiency",
            "should_push": False
        }


def generate_summary_report(days: int = 7) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ„å»ºåˆ†ææ‘˜è¦æŠ¥å‘Š
    
    Args:
        days: åˆ†ææœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
        
    Returns:
        å®Œæ•´çš„åˆ†ææŠ¥å‘Š
    """
    try:
        # è·å–åŸºç¡€æ•°æ®
        builds = fetch_build_data(days=days, limit=50000)
        
        # åŸºç¡€æŒ‡æ ‡åˆ†æ
        basic_metrics = analyze_build_metrics(builds)
        
        # æŒ‰å¹³å°åˆ†æåˆ†ä½æ•°
        platform_percentiles = analyze_percentile_by_platform(days=days, top_n=10)
        
        # è¶‹åŠ¿åˆ†æ
        trend = analyze_trend(days=days, granularity="day")
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = detect_anomalies(days=days)
        
        # æŒ‰å¹³å°åˆ†ç»„
        by_platform = analyze_by_dimension(days=days, dimension="baseline_name", top_n=10)
        
        # æŒ‰Androidç‰ˆæœ¬åˆ†ç»„
        by_android = analyze_by_dimension(days=days, dimension="android_version", top_n=5)
        
        # æŒ‰ç¼–è¯‘ç»„ä»¶åˆ†ç»„
        by_component = analyze_by_dimension(days=days, dimension="compile_component", top_n=10)
        
        # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        summary_text = generate_text_summary(
            basic_metrics, platform_percentiles, trend, anomalies
        )
        
        return {
            "report_type": "build_analysis",
            "generated_at": datetime.now().isoformat(),
            "analysis_period_days": days,
            "summary": summary_text,
            "metrics": basic_metrics,
            "platform_percentiles": platform_percentiles,
            "trend": trend,
            "anomalies": anomalies,
            "breakdown": {
                "by_platform": by_platform,
                "by_android_version": by_android,
                "by_compile_component": by_component
            }
        }
    except Exception as e:
        return {
            "error": str(e),
            "report_type": "build_analysis",
            "generated_at": datetime.now().isoformat()
        }


def generate_text_summary(
    metrics: Dict, 
    platform_percentiles: Dict, 
    trend: Dict, 
    anomalies: Dict
) -> str:
    """
    ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„åˆ†ææ‘˜è¦
    """
    lines = []
    lines.append("## é—¨ç¦æ„å»ºåˆ†ææŠ¥å‘Š\n")
    
    # åŸºç¡€æŒ‡æ ‡
    if metrics.get("total_builds"):
        total = metrics["total_builds"]
        duration = metrics.get("total_duration_minutes", {})
        lines.append(f"### ğŸ“Š åŸºç¡€æŒ‡æ ‡")
        lines.append(f"- åˆ†ææ„å»ºæ•°: **{total:,}** æ¬¡")
        lines.append(f"- å¹³å‡è€—æ—¶: **{duration.get('avg', 0):.1f}** åˆ†é’Ÿ")
        lines.append(f"- P50è€—æ—¶: **{duration.get('p50', 0):.1f}** åˆ†é’Ÿ")
        lines.append(f"- P95è€—æ—¶: **{duration.get('p95', 0):.1f}** åˆ†é’Ÿ")
        lines.append(f"- P99è€—æ—¶: **{duration.get('p99', 0):.1f}** åˆ†é’Ÿ")
        lines.append("")
    
    # è¶‹åŠ¿
    if trend.get("overall_trend"):
        trend_map = {
            "worsening": "ğŸ“ˆ æ¶åŒ–ä¸­",
            "improving": "ğŸ“‰ æ”¹å–„ä¸­", 
            "stable": "â¡ï¸ ä¿æŒç¨³å®š",
            "insufficient_data": "âš ï¸ æ•°æ®ä¸è¶³"
        }
        lines.append(f"### ğŸ“ˆ è¶‹åŠ¿")
        lines.append(f"- æ•´ä½“è¶‹åŠ¿: **{trend_map.get(trend['overall_trend'], 'æœªçŸ¥')}**")
        lines.append("")
    
    # å¼‚å¸¸
    if anomalies.get("anomalies"):
        lines.append(f"### âš ï¸ å¼‚å¸¸å‘Šè­¦")
        for a in anomalies["anomalies"]:
            severity_icon = "ğŸ”´" if a["severity"] == "critical" else "ğŸŸ¡"
            lines.append(f"- {severity_icon} {a['message']}")
        lines.append("")
    
    # æœ€æ…¢å¹³å°
    if platform_percentiles.get("platforms"):
        lines.append(f"### ğŸ¢ æ„å»ºæœ€æ…¢çš„å¹³å° (æŒ‰P95)")
        for i, p in enumerate(sorted(
            platform_percentiles["platforms"], 
            key=lambda x: x.get("p95", 0), 
            reverse=True
        )[:5]):
            lines.append(f"{i+1}. **{p['platform']}**: P50={p.get('p50', 0):.0f}åˆ†é’Ÿ, P95={p.get('p95', 0):.0f}åˆ†é’Ÿ")
        lines.append("")
    
    return "\n".join(lines)


def run_analysis(
    action: str = "problems",
    days: int = 7,
    **kwargs
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ„å»ºåˆ†æ
    
    Args:
        action: åˆ†æç±»å‹
            ã€é—®é¢˜å¯¼å‘åˆ†æ - æ¨èä½¿ç”¨ã€‘
            - problems: é—®é¢˜å¯¼å‘åˆ†ææŠ¥å‘Šï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
            - briefing: ç”Ÿæˆç®€æŠ¥ï¼ˆç”¨äºä¿¡æ¯æµæ¨é€ï¼‰
            - lagging: P95è½åå¹³å°åˆ†æ
            - components: ç»„ä»¶ç“¶é¢ˆåˆ†æ
            - trends: è¶‹åŠ¿å˜åŒ–åˆ†æï¼ˆæ¶åŒ–/æ”¹å–„ï¼‰
            - users: äººå‘˜ç»´åº¦åˆ†æ
            
            ã€åŸºç¡€æŒ‡æ ‡åˆ†æã€‘
            - summary: ç”Ÿæˆå®Œæ•´æ‘˜è¦æŠ¥å‘Š
            - metrics: ä»…è¿”å›åŸºç¡€æŒ‡æ ‡
            - trend: è¶‹åŠ¿åˆ†æï¼ˆæŒ‰å¤©/å‘¨ï¼‰
            - anomalies: å¼‚å¸¸æ£€æµ‹
            - by_platform: æŒ‰å¹³å°åˆ†æ
            - by_android: æŒ‰Androidç‰ˆæœ¬åˆ†æ
            - percentiles: åˆ†ä½æ•°åˆ†æ
            
        days: åˆ†æå¤©æ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åˆ†æç»“æœ
    """
    try:
        # é—®é¢˜å¯¼å‘åˆ†æï¼ˆæ¨èï¼‰
        if action == "problems":
            return generate_problem_report(days=days)
        elif action == "briefing":
            return generate_briefing(days=days)
        elif action == "lagging":
            return analyze_lagging_platforms(days=days, **kwargs)
        elif action == "components":
            return analyze_component_bottlenecks(days=days)
        elif action == "trends":
            return analyze_trend_changes(days=days)
        elif action == "users":
            return analyze_user_builds(days=days, **kwargs)
        
        # åŸºç¡€æŒ‡æ ‡åˆ†æ
        elif action == "summary":
            return generate_summary_report(days=days)
        elif action == "metrics":
            builds = fetch_build_data(days=days, limit=50000, **kwargs)
            return analyze_build_metrics(builds)
        elif action == "trend":
            return analyze_trend(days=days, **kwargs)
        elif action == "anomalies":
            return detect_anomalies(days=days, **kwargs)
        elif action == "by_platform":
            return analyze_by_dimension(days=days, dimension="baseline_name", **kwargs)
        elif action == "by_android":
            return analyze_by_dimension(days=days, dimension="android_version", **kwargs)
        elif action == "percentiles":
            return analyze_percentile_by_platform(days=days, **kwargs)
        else:
            return {"error": f"Unknown action: {action}. Available: problems, lagging, components, trends, users, summary, metrics, trend, anomalies, by_platform, by_android, percentiles"}
    except Exception as e:
        return {
            "error": str(e),
            "action": action,
            "data_source": "mysql"
        }


def main():
    """ä¸»å‡½æ•°ï¼šä»stdinè¯»å–å‚æ•°ï¼Œè¾“å‡ºåˆ†æç»“æœ"""
    try:
        # ä»stdinè¯»å–JSONå‚æ•°
        input_data = sys.stdin.read().strip()
        
        if input_data:
            params = json.loads(input_data)
            action = params.pop("action", "summary")
            days = params.pop("days", 7)
            result = run_analysis(action=action, days=days, **params)
        else:
            # é»˜è®¤ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            result = run_analysis(action="summary", days=7)
        
        # è¾“å‡ºç»“æœ
        print(json.dumps(result, indent=2, ensure_ascii=False, default=str))
        return 0
        
    except Exception as e:
        error_result = {
            "error": str(e),
            "data_source": "mysql"
        }
        print(json.dumps(error_result, indent=2, ensure_ascii=False))
        return 1


if __name__ == "__main__":
    sys.exit(main())


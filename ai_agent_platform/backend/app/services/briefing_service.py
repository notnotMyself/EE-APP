"""
Briefing Service - ç®€æŠ¥ç”Ÿæˆæ ¸å¿ƒæœåŠ¡

è´Ÿè´£ï¼š
1. è°ƒç”¨ Agent æ‰§è¡Œåˆ†æä»»åŠ¡
2. ä» Agent åŸå§‹è¾“å‡ºä¸­æå–åˆ†ææŠ¥å‘Šï¼ˆè¿‡æ»¤æ€è€ƒè¿‡ç¨‹ï¼‰
3. è®© AI åˆ¤æ–­æ˜¯å¦å€¼å¾—æ¨é€ç®€æŠ¥
4. ç”Ÿæˆç®€æŠ¥å¹¶å­˜å…¥æ•°æ®åº“
"""
import json
import logging
import re
from typing import Dict, List, Optional, Any, Tuple
from uuid import UUID
from datetime import datetime, date
from decimal import Decimal

from sqlalchemy.ext.asyncio import AsyncSession

from app.services.claude_service import claude_service
from app.services.agent_sdk_client import execute_agent_task
from app.services.cover_image_service import cover_image_service
from app.crud.crud_briefing import briefing as briefing_crud, scheduled_job as scheduled_job_crud
from app.crud.crud_agent import agent as agent_crud
from app.schemas.briefing import (
    BriefingCreate, BriefingType, BriefingPriority, BriefingAction
)
from app.db.supabase import get_supabase_admin_client
from app.core.config import settings

logger = logging.getLogger(__name__)


# =============================================================================
# åˆ†ææŠ¥å‘Šæå–å™¨ - ä» Agent åŸå§‹è¾“å‡ºä¸­æå–æœ‰æ•ˆå†…å®¹
# =============================================================================

class AnalysisReportExtractor:
    """
    ä» Agent SDK çš„åŸå§‹è¾“å‡ºä¸­æå–åˆ†ææŠ¥å‘Š
    
    Agent è¾“å‡ºé€šå¸¸åŒ…å«ï¼š
    1. æ€è€ƒè¿‡ç¨‹ï¼ˆ"æˆ‘å°†æ‰§è¡Œ...", "è®©æˆ‘å…ˆ...", "çœ‹èµ·æ¥..."ï¼‰
    2. å·¥å…·è°ƒç”¨è®°å½•ï¼ˆ[tool_use], bash å‘½ä»¤è¾“å‡ºç­‰ï¼‰
    3. æœ€ç»ˆçš„åˆ†ææŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
    
    æˆ‘ä»¬åªéœ€è¦ç¬¬3éƒ¨åˆ†ã€‚
    """
    
    # æ€è€ƒè¿‡ç¨‹çš„å…¸å‹å¼€å¤´æ¨¡å¼
    THINKING_PATTERNS = [
        r'^æˆ‘å°†',
        r'^è®©æˆ‘',
        r'^é¦–å…ˆ',
        r'^æ¥ä¸‹æ¥',
        r'^ç°åœ¨',
        r'^å¥½çš„',
        r'^çœ‹èµ·æ¥',
        r'^éœ€è¦å…ˆ',
        r'^æˆ‘éœ€è¦',
        r'^æˆ‘æ¥',
        r'^æˆ‘ä¼š',
        r'^æˆ‘è¦',
        r'^æ­£åœ¨',
        r'^å¼€å§‹',
        r'^æ‰§è¡Œ',
        r'^åˆ†æ',
        r'^è·å–',
        r'^æŸ¥è¯¢',
        r'^è¿æ¥',
        r'^å°è¯•',
        r'^æ£€æŸ¥',
    ]
    
    # å·¥å…·è°ƒç”¨ç›¸å…³çš„æ¨¡å¼
    TOOL_PATTERNS = [
        r'\[tool_use\]',
        r'\[tool_result\]',
        r'TextBlock\(',
        r'ToolUseBlock\(',
        r'ToolResultBlock\(',
        r'ContentBlock\(',
        r'bash\s*\(',
        r'echo\s+[\'"]?\{',
        r'python\s+\w+\.py',
        r'cd\s+\.claude',
        r'pip\s+install',
    ]
    
    # æœ‰æ•ˆæŠ¥å‘Šçš„æ ‡å¿—
    REPORT_MARKERS = [
        r'^#+\s+.+',           # Markdown æ ‡é¢˜
        r'^\|.+\|.+\|',        # Markdown è¡¨æ ¼
        r'^-\s+\*\*.+\*\*',    # å¸¦ç²—ä½“çš„åˆ—è¡¨é¡¹
        r'^##\s*ğŸ“Š',           # å¸¦ emoji çš„æ ‡é¢˜
        r'^##\s*ğŸ”',
        r'^##\s*ğŸ’¡',
        r'^##\s*âš ï¸',
        r'^##\s*ğŸš¨',
        r'^##\s*æ ¸å¿ƒæŒ‡æ ‡',
        r'^##\s*å¼‚å¸¸å‘ç°',
        r'^##\s*æ”¹è¿›å»ºè®®',
        r'^##\s*åˆ†æç»“æœ',
        r'ç ”å‘æ•ˆèƒ½',
        r'Review.*è€—æ—¶',
        r'è¿”å·¥ç‡',
        r'ä»£ç å˜æ›´',
    ]
    
    @classmethod
    def extract(cls, raw_output: str) -> Tuple[str, Dict[str, Any]]:
        """
        ä»åŸå§‹è¾“å‡ºä¸­æå–åˆ†ææŠ¥å‘Š
        
        Args:
            raw_output: Agent SDK çš„åŸå§‹è¾“å‡º
            
        Returns:
            Tuple[str, Dict]: (æå–åçš„æŠ¥å‘Š, æå–å…ƒæ•°æ®)
        """
        if not raw_output or not raw_output.strip():
            return "", {"status": "empty_input"}
        
        metadata = {
            "original_length": len(raw_output),
            "extraction_method": None,
            "filtered_lines": 0,
            "kept_lines": 0,
        }
        
        # æ–¹æ³•1: å°è¯•æ‰¾åˆ° Markdown æŠ¥å‘Šå—
        report = cls._extract_markdown_report(raw_output)
        if report and len(report) > 200:
            metadata["extraction_method"] = "markdown_block"
            metadata["extracted_length"] = len(report)
            return report, metadata
        
        # æ–¹æ³•2: æŒ‰è¡Œè¿‡æ»¤ï¼Œç§»é™¤æ€è€ƒè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨
        report, line_stats = cls._filter_by_lines(raw_output)
        metadata.update(line_stats)
        
        if report and len(report) > 100:
            metadata["extraction_method"] = "line_filter"
            metadata["extracted_length"] = len(report)
            return report, metadata
        
        # æ–¹æ³•3: å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„åŸæ–‡
        cleaned = cls._basic_cleanup(raw_output)
        metadata["extraction_method"] = "basic_cleanup"
        metadata["extracted_length"] = len(cleaned)
        
        return cleaned, metadata
    
    @classmethod
    def _extract_markdown_report(cls, text: str) -> Optional[str]:
        """
        å°è¯•æå–å®Œæ•´çš„ Markdown æŠ¥å‘Šå—
        
        æŸ¥æ‰¾ä»¥ # å¼€å¤´çš„æŠ¥å‘Šæ ‡é¢˜ï¼Œä¸€ç›´åˆ°æ–‡æœ«æˆ–ä¸‹ä¸€ä¸ªæ˜æ˜¾çš„åˆ†éš”
        """
        lines = text.split('\n')
        report_start = -1
        report_lines = []
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # æŸ¥æ‰¾æŠ¥å‘Šå¼€å§‹æ ‡å¿—
            if report_start < 0:
                # åŒ¹é… # ç ”å‘æ•ˆèƒ½, # åˆ†ææŠ¥å‘Š, # æ¯æ—¥åˆ†æ ç­‰
                if re.match(r'^#+\s*(ç ”å‘æ•ˆèƒ½|åˆ†ææŠ¥å‘Š|æ•ˆèƒ½åˆ†æ|æ¯æ—¥åˆ†æ|æ—¥æŠ¥|å‘¨æŠ¥)', stripped):
                    report_start = i
                    report_lines.append(line)
                # æˆ–è€…åŒ¹é… --- åˆ†éš”ç¬¦åçš„ # æ ‡é¢˜
                elif stripped == '---' and i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if re.match(r'^#+\s+', next_line):
                        report_start = i
                        report_lines.append(line)
            else:
                # å·²ç»åœ¨æŠ¥å‘Šä¸­ï¼Œæ£€æŸ¥æ˜¯å¦ç»“æŸ
                # é‡åˆ°å·¥å…·è°ƒç”¨æˆ–æ€è€ƒè¿‡ç¨‹åˆ™åœæ­¢
                is_tool_line = any(re.search(p, stripped, re.IGNORECASE) for p in cls.TOOL_PATTERNS)
                is_thinking = any(re.match(p, stripped) for p in cls.THINKING_PATTERNS[:10])
                
                if is_tool_line or (is_thinking and len(report_lines) > 5):
                    break
                
                report_lines.append(line)
        
        if report_lines:
            return '\n'.join(report_lines).strip()
        
        return None
    
    @classmethod
    def _filter_by_lines(cls, text: str) -> Tuple[str, Dict[str, int]]:
        """
        æŒ‰è¡Œè¿‡æ»¤ï¼Œç§»é™¤æ€è€ƒè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨
        """
        lines = text.split('\n')
        kept_lines = []
        filtered_count = 0
        in_code_block = False
        
        for line in lines:
            stripped = line.strip()
            
            # è·Ÿè¸ªä»£ç å—çŠ¶æ€
            if stripped.startswith('```'):
                in_code_block = not in_code_block
                # ä¿ç•™ Markdown ä»£ç å—ï¼ˆä½†ä¸æ˜¯å·¥å…·è¾“å‡ºçš„ä»£ç å—ï¼‰
                if not any(re.search(p, stripped) for p in cls.TOOL_PATTERNS):
                    kept_lines.append(line)
                continue
            
            # åœ¨ä»£ç å—å†…ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è¾“å‡º
            if in_code_block:
                # è·³è¿‡æ˜æ˜¾çš„å·¥å…·è¾“å‡º
                if any(re.search(p, stripped, re.IGNORECASE) for p in cls.TOOL_PATTERNS):
                    filtered_count += 1
                    continue
                kept_lines.append(line)
                continue
            
            # ç©ºè¡Œä¿ç•™ï¼ˆç”¨äºæ ¼å¼ï¼‰
            if not stripped:
                kept_lines.append(line)
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒè¿‡ç¨‹
            is_thinking = any(re.match(p, stripped) for p in cls.THINKING_PATTERNS)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·ç›¸å…³
            is_tool = any(re.search(p, stripped, re.IGNORECASE) for p in cls.TOOL_PATTERNS)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæŠ¥å‘Šå†…å®¹
            is_report = any(re.search(p, stripped, re.IGNORECASE) for p in cls.REPORT_MARKERS)
            
            # å†³å®šæ˜¯å¦ä¿ç•™
            if is_tool:
                filtered_count += 1
            elif is_thinking and not is_report:
                # å¦‚æœæ˜¯æ€è€ƒè¿‡ç¨‹ä½†åŒ…å«æŠ¥å‘Šå…³é”®è¯ï¼Œè¿˜æ˜¯ä¿ç•™
                filtered_count += 1
            else:
                kept_lines.append(line)
        
        # æ¸…ç†è¿ç»­çš„ç©ºè¡Œ
        result = '\n'.join(kept_lines)
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        return result.strip(), {
            "filtered_lines": filtered_count,
            "kept_lines": len(kept_lines)
        }
    
    @classmethod
    def _basic_cleanup(cls, text: str) -> str:
        """
        åŸºæœ¬æ¸…ç†ï¼šç§»é™¤æ˜æ˜¾çš„å™ªéŸ³
        """
        # ç§»é™¤ TextBlock, ToolUseBlock ç­‰ SDK è¾“å‡ºæ ¼å¼
        text = re.sub(r'TextBlock\(text=[\'"]', '', text)
        text = re.sub(r'ToolUseBlock\([^)]+\)', '', text)
        text = re.sub(r'ToolResultBlock\([^)]+\)', '', text)
        text = re.sub(r'ContentBlock\([^)]+\)', '', text)
        text = re.sub(r'[\'"],?\s*type=[\'"]text[\'"]', '', text)
        text = re.sub(r'\)\s*$', '', text, flags=re.MULTILINE)
        
        # æ¸…ç†è¿ç»­ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()


class BriefingService:
    """ç®€æŠ¥ç”ŸæˆæœåŠ¡"""

    # =========================================================================
    # ç®€æŠ¥åˆ¤æ–­ Prompt - æ ¸å¿ƒä¸­çš„æ ¸å¿ƒï¼ˆV2 ä¼˜åŒ–ç‰ˆï¼‰
    # =========================================================================
    BRIEFING_DECISION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªç ”å‘æ•ˆèƒ½åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹åˆ†æç»“æœï¼Œåˆ¤æ–­æ˜¯å¦å€¼å¾—å‘ç”¨æˆ·æ¨é€ç®€æŠ¥ã€‚

## ä¿¡æ¯æµé“å¾‹

1. **ä¸€å¤©æœ€å¤š3æ¡** - ä¸è¦ç”¨æ— ä»·å€¼ä¿¡æ¯æ‰“æ‰°ç”¨æˆ·
2. **å®å¯ä¸å‘** - å¦‚æœä¸ç¡®å®šæ˜¯å¦å€¼å¾—å‘ï¼Œå°±ä¸å‘
3. **èƒ½æ¥ä¸Šå¯¹è¯** - ç”¨æˆ·çœ‹å®Œä¼šæƒ³é—®"ä¸ºä»€ä¹ˆ"æˆ–"æ€ä¹ˆåŠ"

## åˆ¤æ–­æ ‡å‡†

| æ¨é€ | åœºæ™¯ç¤ºä¾‹ |
|------|----------|
| âœ… æ¨é€ | ä¸€æ¬¡æ€§é€šè¿‡ç‡<50%ï¼Œè¿”å·¥æˆæœ¬é«˜ |
| âœ… æ¨é€ | äººå‡æ´»è·ƒåˆ†æ”¯>15ä¸ªï¼Œå·¥ä½œè¿‡äºåˆ†æ•£ |
| âœ… æ¨é€ | å‘ç°å€Ÿå•å¼‚å¸¸ï¼ˆ1ä¸ªStoryå¯¹åº”>10ä¸ªchange_idï¼‰ |
| âœ… æ¨é€ | åŒåˆ†æ”¯å¤šæ¬¡æäº¤å¼‚å¸¸ï¼ˆåå¤æäº¤-æ”¾å¼ƒï¼‰ |
| âœ… æ¨é€ | æ•ˆç‡è¶‹åŠ¿æ¶åŒ–è¶…è¿‡20% |
| âŒ ä¸æ¨é€ | å„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œæ— å¼‚å¸¸å‘ç° |
| âŒ ä¸æ¨é€ | æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ® |
| âŒ ä¸æ¨é€ | çº¯ç²¹çš„æ•°å­—ç½—åˆ—ï¼Œæ²¡æœ‰æ´å¯Ÿ |

## åˆ†æç»“æœ

{analysis_result}

## è¾“å‡ºè¦æ±‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œ**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼**ï¼š

**å¦‚æœå€¼å¾—æ¨é€**ï¼š
```json
{{
  "should_push": true,
  "briefing": {{
    "type": "insight",
    "priority": "P1",
    "title": "ä¸€æ¬¡æ€§é€šè¿‡ç‡ä»…33.9%ï¼Œå›¢é˜Ÿè¿”å·¥æˆæœ¬è¾ƒé«˜",
    "summary": "æœ€è¿‘7å¤©åˆ†ææ˜¾ç¤ºï¼Œä»£ç ä¸€æ¬¡æ€§é€šè¿‡ç‡ä»…33.9%ï¼Œæ€»è¿”å·¥æ¬¡æ•°è¾¾45126æ¬¡ã€‚äººå‡æ´»è·ƒåˆ†æ”¯25.9ä¸ªï¼Œå·¥ä½œåˆ†æ•£åº¦è¾ƒé«˜ã€‚å»ºè®®ï¼š1ï¼‰åŠ å¼ºä»£ç è‡ªæµ‹ï¼Œæé«˜ä¸€æ¬¡æ€§é€šè¿‡ç‡ï¼›2ï¼‰å‡å°‘åˆ†æ”¯åˆ‡æ¢ï¼Œèšç„¦æ ¸å¿ƒä»»åŠ¡ã€‚",
    "impact": "è¿”å·¥å¯¼è‡´çº¦30%çš„å¼€å‘æ—¶é—´æµªè´¹",
    "actions": [
      {{"label": "ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ", "action": "start_conversation", "prompt": "è¯·è¯¦ç»†åˆ†æè¿”å·¥ç‡é«˜çš„åŸå› ï¼Œå“ªäº›æ¨¡å—æˆ–äººå‘˜è¿”å·¥æœ€å¤šï¼Ÿ"}},
      {{"label": "ç»™æˆ‘è¯¦ç»†åˆ†æ", "action": "start_conversation", "prompt": "è¯·ç»™æˆ‘å®Œæ•´çš„æ—¶é—´æ•ˆç‡æŸè€—åˆ†ææŠ¥å‘Š"}},
      {{"label": "å¦‚ä½•æ”¹è¿›ï¼Ÿ", "action": "start_conversation", "prompt": "é’ˆå¯¹å½“å‰çš„æ•ˆç‡é—®é¢˜ï¼Œè¯·ç»™å‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¼˜å…ˆçº§"}}
    ],
    "importance_score": 0.85
  }}
}}
```

**å¦‚æœä¸å€¼å¾—æ¨é€**ï¼š
```json
{{
  "should_push": false,
  "reason": "å„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œæ— éœ€æ¨é€"
}}
```

## æ ‡é¢˜å†™ä½œæŒ‡å—ï¼ˆéå¸¸é‡è¦ï¼ï¼‰

**å¥½çš„æ ‡é¢˜**ï¼ˆè¯´æ¸…æ ¸å¿ƒå‘ç°ï¼Œæœ‰æ•°å­—æ”¯æ’‘ï¼‰ï¼š
- "ä¸€æ¬¡æ€§é€šè¿‡ç‡ä»…33.9%ï¼Œå›¢é˜Ÿè¿”å·¥æˆæœ¬è¾ƒé«˜" âœ…
- "äººå‡æ´»è·ƒåˆ†æ”¯25.9ä¸ªï¼Œå·¥ä½œè¿‡äºåˆ†æ•£" âœ…
- "å‘ç°412ä¸ªç–‘ä¼¼å€Ÿå•Storyï¼Œéœ€è¦å…³æ³¨" âœ…
- "ç³»ç»Ÿå¼€å‘éƒ¨è¿”å·¥ç‡ä¸‹é™15%ï¼Œæ•ˆç‡æå‡" âœ…

**å·®çš„æ ‡é¢˜**ï¼ˆç»å¯¹ä¸è¦è¿™æ ·å†™ï¼‰ï¼š
- "æœ¬å‘¨ç ”å‘æ•ˆèƒ½å‘¨æŠ¥" âŒ
- "æˆ‘å°†æ‰§è¡Œæ¯æ—¥ç ”å‘æ•ˆèƒ½åˆ†æ" âŒ
- "ä»£ç å®¡æŸ¥æ•°æ®åˆ†æç»“æœ" âŒ
- "æœªçŸ¥" âŒ

## Summary å†™ä½œæŒ‡å—ï¼ˆéå¸¸é‡è¦ï¼ï¼‰

Summary å¿…é¡»åŒ…å«ä¸‰è¦ç´ ï¼š**å‘ç° + å½±å“ + å»ºè®®**

**å¥½çš„ Summary ç¤ºä¾‹**ï¼š
"æœ€è¿‘7å¤©åˆ†ææ˜¾ç¤ºï¼Œä»£ç ä¸€æ¬¡æ€§é€šè¿‡ç‡ä»…33.9%ï¼Œæ€»è¿”å·¥æ¬¡æ•°è¾¾45126æ¬¡ã€‚äººå‡æ´»è·ƒåˆ†æ”¯25.9ä¸ªï¼Œå·¥ä½œåˆ†æ•£åº¦è¾ƒé«˜ã€‚å»ºè®®ï¼š1ï¼‰åŠ å¼ºä»£ç è‡ªæµ‹ï¼Œæé«˜ä¸€æ¬¡æ€§é€šè¿‡ç‡ï¼›2ï¼‰å‡å°‘åˆ†æ”¯åˆ‡æ¢ï¼Œèšç„¦æ ¸å¿ƒä»»åŠ¡ã€‚"

**å·®çš„ Summary**ï¼ˆç»å¯¹ä¸è¦è¿™æ ·å†™ï¼‰ï¼š
- "æˆ‘å°†æ‰§è¡Œæ¯æ—¥ç ”å‘æ•ˆèƒ½åˆ†æï¼ŒæŒ‰ç…§æµç¨‹è·å–æ•°æ®å¹¶åˆ†æå…³é”®æŒ‡æ ‡" âŒ
- "çœ‹æ¥éœ€è¦å…ˆäº†è§£æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„è¡¨ç»“æ„" âŒ

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
"""

    async def execute_and_generate_briefing(
        self,
        db: AsyncSession,
        agent_id: UUID,
        task_prompt: str,
        briefing_config: Dict[str, Any],
        target_user_ids: Optional[List[UUID]] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†æå¹¶ç”Ÿæˆç®€æŠ¥

        Args:
            db: æ•°æ®åº“ä¼šè¯
            agent_id: Agent ID
            task_prompt: ä»»åŠ¡æç¤ºè¯
            briefing_config: ç®€æŠ¥é…ç½®
            target_user_ids: ç›®æ ‡ç”¨æˆ·IDåˆ—è¡¨ï¼ˆNoneåˆ™æ¨é€ç»™æ‰€æœ‰è®¢é˜…ç”¨æˆ·ï¼‰

        Returns:
            {
                "analysis_completed": True,
                "briefing_generated": True/False,
                "briefing_count": 0,
                "reason": "...",
                "briefing_ids": [...]
            }
        """
        logger.info(f"Starting briefing generation for agent {agent_id}")

        try:
            # 1. è·å– Agent é…ç½® (ä½¿ç”¨ Supabase å®¢æˆ·ç«¯)
            supabase = get_supabase_admin_client()
            agent_result = supabase.table('agents').select('*').eq('id', str(agent_id)).execute()

            if not agent_result.data:
                return {"error": f"Agent not found: {agent_id}"}

            agent = agent_result.data[0]

            # 2. æ‰§è¡Œ Agent åˆ†æä»»åŠ¡
            raw_analysis_result = await self._execute_agent_analysis(
                agent_name=agent['name'],
                agent_role=agent['role'],
                agent_description=agent.get('description', ''),
                task_prompt=task_prompt
            )

            logger.info(f"Raw analysis completed, length: {len(raw_analysis_result)}")

            # 2.5 ã€å…³é”®æ­¥éª¤ã€‘ä»åŸå§‹è¾“å‡ºä¸­æå–åˆ†ææŠ¥å‘Š
            # Agent SDK è¿”å›çš„å†…å®¹åŒ…å«æ€è€ƒè¿‡ç¨‹ã€å·¥å…·è°ƒç”¨ç­‰å™ªéŸ³
            # è¿™é‡Œæå–å‡ºçœŸæ­£çš„åˆ†ææŠ¥å‘Š
            analysis_result, extraction_meta = AnalysisReportExtractor.extract(raw_analysis_result)
            
            logger.info(
                f"Report extracted: method={extraction_meta.get('extraction_method')}, "
                f"original={extraction_meta.get('original_length')}, "
                f"extracted={extraction_meta.get('extracted_length')}"
            )
            
            # å¦‚æœæå–åçš„å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æå–å¤±è´¥
            if len(analysis_result) < 50:
                logger.warning(f"Extracted report too short ({len(analysis_result)} chars), using raw output")
                analysis_result = raw_analysis_result[:4000]  # é™åˆ¶é•¿åº¦

            # 3. è®© AI åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆç®€æŠ¥
            min_importance = briefing_config.get('min_importance_score', 0.6)
            briefing_decision = await self._decide_briefing(
                analysis_result=analysis_result,
                min_importance_score=min_importance
            )

            if not briefing_decision.get('should_push'):
                logger.info(f"Briefing not needed: {briefing_decision.get('reason')}")
                return {
                    "analysis_completed": True,
                    "briefing_generated": False,
                    "briefing_count": 0,
                    "reason": briefing_decision.get('reason', 'Not important enough'),
                    "analysis_result": analysis_result[:500],  # æå–åçš„åˆ†ææŠ¥å‘Šï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    "extraction_meta": extraction_meta
                }

            # 4. æ£€æŸ¥ä»Šæ—¥ç®€æŠ¥é…é¢ (ä½¿ç”¨ Supabase)
            max_daily = briefing_config.get('max_daily_briefings', 3)
            today = date.today().isoformat()
            count_result = supabase.table('briefings').select('id', count='exact').eq(
                'agent_id', str(agent_id)
            ).gte('created_at', f"{today}T00:00:00").execute()
            today_count = count_result.count or 0

            if today_count >= max_daily:
                logger.warning(f"Daily quota exceeded: {today_count}/{max_daily}")
                return {
                    "analysis_completed": True,
                    "briefing_generated": False,
                    "briefing_count": 0,
                    "reason": f"Daily briefing quota exceeded ({today_count}/{max_daily})"
                }

            # 5. è·å–ç›®æ ‡ç”¨æˆ·
            users = await self._get_target_users(agent_id, target_user_ids)

            if not users:
                logger.warning("No target users found")
                return {
                    "analysis_completed": True,
                    "briefing_generated": False,
                    "briefing_count": 0,
                    "reason": "No subscribed users found"
                }

            # 6. ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç®€æŠ¥ (ä½¿ç”¨ Supabase)
            briefing_data = briefing_decision['briefing']
            briefing_ids = []

            for user in users:
                briefing_id = await self._create_briefing_for_user_supabase(
                    agent_id=agent_id,
                    user_id=UUID(user['user_id']),
                    briefing_data=briefing_data,
                    context_data={
                        'analysis_result': analysis_result,  # æå–åçš„æŠ¥å‘Š
                        'raw_output_preview': raw_analysis_result[:1000] if len(raw_analysis_result) > 1000 else raw_analysis_result,  # åŸå§‹è¾“å‡ºé¢„è§ˆ
                        'extraction_meta': extraction_meta,
                        'task_prompt': task_prompt,
                        'generated_at': datetime.utcnow().isoformat()
                    }
                )
                briefing_ids.append(str(briefing_id))

            logger.info(f"Generated {len(briefing_ids)} briefings")

            return {
                "analysis_completed": True,
                "briefing_generated": True,
                "briefing_count": len(briefing_ids),
                "briefing_ids": briefing_ids,
                "briefing_title": briefing_data.get('title')
            }

        except Exception as e:
            logger.error(f"Error in execute_and_generate_briefing: {e}", exc_info=True)
            return {
                "analysis_completed": False,
                "briefing_generated": False,
                "error": str(e)
            }

    async def _execute_agent_analysis(
        self,
        agent_name: str,
        agent_role: str,
        agent_description: str,
        task_prompt: str
    ) -> str:
        """ä½¿ç”¨ Claude Agent SDK æ‰§è¡Œåˆ†æä»»åŠ¡"""
        try:
            # ä½¿ç”¨ Agent SDK æ‰§è¡Œä»»åŠ¡
            result = await execute_agent_task(
                agent_role=agent_role,
                task_prompt=task_prompt,
                allowed_tools=["Bash", "Read", "Write", "Grep", "Glob"],
                timeout=300
            )

            logger.info(f"Agent {agent_role} analysis completed")
            return result

        except Exception as e:
            logger.error(f"Agent analysis failed: {e}", exc_info=True)
            # é™çº§åˆ°æ—§æ–¹æ³•ï¼ˆå¯é€‰ï¼‰
            logger.warning("Falling back to legacy claude_service")

            system_prompt = claude_service.build_agent_system_prompt(
                agent_name=agent_name,
                agent_role=agent_role,
                agent_description=agent_description
            )

            messages = [{"role": "user", "content": task_prompt}]
            result = await claude_service.chat_completion(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=4096
            )

            return result

    async def _decide_briefing(
        self,
        analysis_result: str,
        min_importance_score: float
    ) -> Dict[str, Any]:
        """è®© AI åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆç®€æŠ¥"""
        prompt = self.BRIEFING_DECISION_PROMPT.format(
            analysis_result=analysis_result
        )

        messages = [{"role": "user", "content": prompt}]

        response = await claude_service.chat_completion(
            messages=messages,
            max_tokens=1024,
            temperature=0.3  # é™ä½éšæœºæ€§ï¼Œè®©åˆ¤æ–­æ›´ç¨³å®š
        )

        # è§£æ JSON å“åº”
        try:
            # æå– JSON éƒ¨åˆ†ï¼ˆå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—ï¼‰
            json_str = response
            if '```json' in response:
                json_str = response.split('```json')[1].split('```')[0]
            elif '```' in response:
                json_str = response.split('```')[1].split('```')[0]
            else:
                # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]

            decision = json.loads(json_str.strip())

            # æ£€æŸ¥é‡è¦æ€§åˆ†æ•°
            if decision.get('should_push'):
                importance = decision.get('briefing', {}).get('importance_score', 0)
                if isinstance(importance, str):
                    importance = float(importance)
                if importance < min_importance_score:
                    return {
                        "should_push": False,
                        "reason": f"Importance score {importance} below threshold {min_importance_score}"
                    }

            return decision

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse briefing decision: {response[:500]}")
            return {
                "should_push": False,
                "reason": f"Failed to parse AI response: {str(e)}"
            }

    async def _get_target_users(
        self,
        agent_id: UUID,
        target_user_ids: Optional[List[UUID]]
    ) -> List[Dict[str, Any]]:
        """è·å–ç›®æ ‡ç”¨æˆ·åˆ—è¡¨"""
        supabase = get_supabase_admin_client()

        if target_user_ids:
            # æŒ‡å®šç”¨æˆ·
            result = supabase.table('user_agent_subscriptions').select(
                'user_id'
            ).in_(
                'user_id', [str(uid) for uid in target_user_ids]
            ).eq('agent_id', str(agent_id)).eq('is_active', True).execute()
        else:
            # æ‰€æœ‰è®¢é˜…ç”¨æˆ·
            result = supabase.table('user_agent_subscriptions').select(
                'user_id'
            ).eq('agent_id', str(agent_id)).eq('is_active', True).execute()

        return result.data if result.data else []

    async def _create_briefing_for_user(
        self,
        db: AsyncSession,
        agent_id: UUID,
        user_id: UUID,
        briefing_data: Dict[str, Any],
        context_data: Dict[str, Any]
    ) -> UUID:
        """ä¸ºç”¨æˆ·åˆ›å»ºç®€æŠ¥"""
        # è§£æç®€æŠ¥ç±»å‹
        type_map = {
            'alert': BriefingType.ALERT,
            'insight': BriefingType.INSIGHT,
            'summary': BriefingType.SUMMARY,
            'action': BriefingType.ACTION
        }

        priority_map = {
            'P0': BriefingPriority.P0,
            'P1': BriefingPriority.P1,
            'P2': BriefingPriority.P2
        }

        # è§£æ actions
        actions = []
        for action_data in briefing_data.get('actions', []):
            actions.append(BriefingAction(
                label=action_data.get('label', 'æŸ¥çœ‹'),
                action=action_data.get('action', 'view_report'),
                data=action_data.get('data'),
                prompt=action_data.get('prompt')
            ))

        # è·å–é‡è¦æ€§åˆ†æ•°
        importance_score = briefing_data.get('importance_score', 0.5)
        if isinstance(importance_score, str):
            importance_score = float(importance_score)

        briefing_create = BriefingCreate(
            agent_id=agent_id,
            user_id=user_id,
            briefing_type=type_map.get(briefing_data.get('type', 'insight'), BriefingType.INSIGHT),
            priority=priority_map.get(briefing_data.get('priority', 'P2'), BriefingPriority.P2),
            title=briefing_data.get('title', 'æ–°ç®€æŠ¥'),
            summary=briefing_data.get('summary', ''),
            impact=briefing_data.get('impact'),
            actions=actions,
            context_data=context_data,
            importance_score=Decimal(str(importance_score))
        )

        created = await briefing_crud.create(db, obj_in=briefing_create)
        return created.id

    async def _create_briefing_for_user_supabase(
        self,
        agent_id: UUID,
        user_id: UUID,
        briefing_data: Dict[str, Any],
        context_data: Dict[str, Any]
    ) -> UUID:
        """ä¸ºç”¨æˆ·åˆ›å»ºç®€æŠ¥ (ä½¿ç”¨ Supabase)"""
        import uuid as uuid_module

        # è·å–é‡è¦æ€§åˆ†æ•°
        importance_score = briefing_data.get('importance_score', 0.5)
        if isinstance(importance_score, str):
            importance_score = float(importance_score)

        # âœ¨ ç”Ÿæˆ AI å°é¢å›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        cover_image_url = None
        enable_cover = getattr(settings, 'ENABLE_AI_COVER_GENERATION', False)
        
        if enable_cover:
            try:
                cover_image_url = await cover_image_service.generate_cover_image(
                    briefing_type=briefing_data.get('type', 'insight'),
                    title=briefing_data.get('title', ''),
                    summary=briefing_data.get('summary', ''),
                    priority=briefing_data.get('priority', 'P2')
                )
                if cover_image_url:
                    logger.info(f"Generated cover image: {cover_image_url[:50]}...")
            except Exception as e:
                logger.warning(f"Failed to generate cover image, using fallback: {e}")
                # é™çº§åˆ°å‰ç«¯æ¸å˜èƒŒæ™¯

        briefing_record = {
            'id': str(uuid_module.uuid4()),
            'agent_id': str(agent_id),
            'user_id': str(user_id),
            'briefing_type': briefing_data.get('type', 'insight'),
            'priority': briefing_data.get('priority', 'P2'),
            'title': briefing_data.get('title', 'æ–°ç®€æŠ¥'),
            'summary': briefing_data.get('summary', ''),
            'impact': briefing_data.get('impact'),
            'actions': briefing_data.get('actions', []),
            'context_data': {
                **context_data,
                'cover_image_url': cover_image_url  # âœ¨ å°é¢å›¾ URL
            },
            'importance_score': importance_score,
            'status': 'new'
        }

        supabase = get_supabase_admin_client()
        result = supabase.table('briefings').insert(briefing_record).execute()

        return UUID(result.data[0]['id'])


# å•ä¾‹å®ä¾‹
briefing_service = BriefingService()
